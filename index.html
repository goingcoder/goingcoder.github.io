<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="离弦的博客" type="application/atom+xml" />






<meta name="description" content="匆忙世间的闲人。">
<meta name="keywords" content="Hexo,next">
<meta property="og:type" content="website">
<meta property="og:title" content="离弦的博客">
<meta property="og:url" content="https://goingcoder.github.io/index.html">
<meta property="og:site_name" content="离弦的博客">
<meta property="og:description" content="匆忙世间的闲人。">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="离弦的博客">
<meta name="twitter:description" content="匆忙世间的闲人。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://goingcoder.github.io/"/>





  <title>离弦的博客</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?your-analytics-id";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband">
		
	</div>
	
	
	
	<a href="https://github.com/goingcoder" class="github-corner" aria-label="View source on Github">
		<svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
			<path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
	
	<header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">离弦的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">我若为王,舍我其谁</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'your-swiftype-key','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/10/linux5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/linux5/" itemprop="url">在linux上 anaconda3和anaconda2共存的解决办法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T19:20:03+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/liunx/" itemprop="url" rel="index">
                    <span itemprop="name">liunx</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/10/linux5/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/10/linux5/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <p>​    因为需要，有时候用到Python2有时候用到Python3, ，本人之前装的是anaconda3，由于合作伙伴用的是Python2 ，为此为了考虑兼容性，经过三个小时的折腾结合网上的资源整理如下，安装环境是linux2。先根据Ubuntukylin16.04.2安装anaconda3，然后将anaconda2安装到anaconda3的envs文件夹下作为虚拟环境用于切换。</p>
<p>​    因为在线安装anaconda是很慢的，这里用离线安装方法。请自行到官网下载安装包。（<a href="https://www.anaconda.com/download/#linux）" target="_blank" rel="noopener">https://www.anaconda.com/download/#linux）</a></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/10/linux5/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/10/ner8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/ner8/" itemprop="url">Transfer learning for sequence tagging with hierarchical recurrent networks笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T16:37:06+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/10/ner8/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/10/ner8/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <p><strong>转载自-李虹磊 基于迁移学习和深度神经网络的序列标注方法</strong> </p>
<p><strong>论文题目：Transfer learning for sequence tagging with hierarchical recurrent networks</strong></p>
<p><strong>论文作者：Zhilin Yang, Ruslan Salakhutdinov, William W. Cohen</strong></p>
<p><strong>School of Computer Science Carnegie Mellon University</strong></p>
<p><strong>论文出处：Published as a conference paper at ICLR 2017</strong></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    本文利用迁移学习的思想和神经网络模型来解决序列标注问题。使用有大量已标注数据集的任务作为源数据领域，来提升只有少量标注数据的目标任务的性能。本文从跨域、跨应用和跨语言三个迁移设置来验证迁移学习用于深度层级循环神经网络中的性能。实验表明，在深度层级神经网络的基础上使用迁移学习的思想可以显著提高序列标注的性能。 </p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/10/ner8/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/10/ner7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/ner7/" itemprop="url">Multi-Task Cross-Lingual Sequence Tagging from Scratch笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T15:27:55+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/10/ner7/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/10/ner7/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    论文提出了一个深度层叠循环神经网络的序列标注模型，这个模型是任务独立的，语言独立的，以及没有特征工程。论文探索了两种类型的联合训练，在多任务联合训练中，同种语言多个任务连个训练——英文的词性标注和实体识别。在跨语言联合训练中，模型训练同一个任务在不同的语言上——英文和西班牙语中的实体识别。论文表明多任务和跨语言的联合训练可以在不同场景提高性能。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/10/ner7/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/08/ner6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/ner6/" itemprop="url">Contextual String Embeddings for Sequence Labeling笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T13:40:45+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/08/ner6/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/08/ner6/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    使用递归神经网络进行语言建模的最新进展表明基于字符的语言建模是可行的。这篇论文提出利用一个预训练的字符语言模型的内部状态，以产生一种新型的字嵌入，称之为上下文字符串向量。我们提出的embedding有明显的优点：（1）在没有任何明确的词汇概念的情况下进行训练，从根本上将单词作为字符序列建模。（2）由其周围语境上下文化，这意味着相同的单词的embedding将根据其不同的语境的不同而不同。我们对先前的embedding进行比较评估，发现我们的embedding对于下游任务非常有用。在四个经典序列标记任务中，我们始终优于当前最优的结果。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/08/ner6/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/08/ner5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/ner5/" itemprop="url">Deep contextualized word representations论文笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T09:22:44+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/08/ner5/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/08/ner5/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    预训练的词向量是很多神经语言理解模型的关键部分，然而学习高质量的表示是很有挑战性的。一个好的词向量应该包含：（1）能够建模词的复杂字符特征（比如语法和语义），（2）能够在不同语境有不同反映（如多义词）。这篇论文中，引入一种新类型的深度上下文词表示，可以应对这些挑战，并且可以容易与当前存在的模型进行整合，有效提升了一系列有挑战的语言理解问题的最优性能。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/08/ner5/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/06/ner4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/06/ner4/" itemprop="url">Semi-supervised sequence tagging with bidirectional language models笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-06T16:23:33+08:00">
                2018-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/06/ner4/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/06/ner4/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    对于NLP任务，从无标签文本中学习预训练的embedding已经是神经网络的一个标准部分。在这篇论文中，提出了将一个双向的语言模型的context embedding引入序列标注任务中，在NER和chunking两个标准数据集上进行了实验，证明了与其他的某些使用迁移学习和使用附加标注数据和特定任务字典的联合学习相比，效果都要好，超过了以往所有的模型。</p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul>
<li><p>主要贡献是表明在LM embedding中捕获的上下文敏感表示在监督序列标记设置中是有用的。</p>
</li>
<li><p>双向的LM embedding比前向的LM embedding效果要好。</p></li></ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/06/ner4/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/17/tf10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/17/tf10/" itemprop="url">CNN卷积神经网络对MNIST数据分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-17T14:31:32+08:00">
                2018-05-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/17/tf10/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/17/tf10/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <p>​    Convolutional Neural Networks (CNN) 是神经网络处理图片信息的一大利器. 因为利用卷积神经网络在图像和语音识别方面能够给出更优预测结果, 这一种技术也被广泛的传播可应用. 卷积神经网络最常被应用的方面是计算机的图像识别, 不过因为不断地创新, 它也被应用在视频分析, 自然语言处理, 药物发现, 等等. 近期最火的 Alpha Go, 让计算机看懂围棋, 同样也是有运用到这门技术。</p>
<p>​    这里将建立一个卷积神经网络，它可以把MNIST手写字符的识别准确率提高到99%，读者可能需要一些卷积神经网络的基础知识才能更好地理解本节的内容。</p>
<p>​    程序的开头依旧是导入Tensorflow：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br></pre></td></tr></table></figure>
<p>​    接下来载入MNIST数据集，并建立占位符。占位符x的含义为训练图像，y_为对应训练图像的标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>​    由于要使用的是卷积神经网络对图像进行分类，所以不能再使用784维的向量表示输入的x，而是将其还原为28*28的图片形式。[-1,28,28,1]中的-1表示形状第一维的大小是根据x自动确定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>​    x_image就是输入的训练图像，接下来。我们堆训练图像进行卷积操作，第一层卷积的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"><span class="comment"># 第一层卷积层</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br></pre></td></tr></table></figure>
<p>​    先定义四个函数，函数weight_variable可以返回一个给定形状的变量并自动以截断正态分布初始化，bias_variable同样返回一个给定形状的变量，初始化时所有值是0.1，可分别用这两个函数创建卷积的核与偏置。h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)是真正进行卷积运算，卷积计算后选用ReLU作为激活函数。h_pool1 = max_pool_2x2(h_conv1)是调用函数max_pool_2x2(h_conv1)进行一次池化操作。卷积、激活函数、池化，可以说是一个卷积层的“标配”,通常一个卷积层都会包含这三个步骤，有时会去掉最后的池化操作。</p>
<p>​    tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。 </p>
<p>​    对第一个卷积操作后产生的h_pool1再做一次卷积计算，使用的代码与上面类似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<p>两层卷积之后是全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">   W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">   b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">   h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">   h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">   <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">   keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">   h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<p>​    在全连接中加入了Dropout，它是防止神经网络过拟合的一种手段。在每一步训练时，以一定的概率“去掉”网络中的某些连接，但这种去除不是永久性的，只是在当前步骤中去除，并且每一步去除的连接都是随机选择的。在这个程序中，选择的Dropout概率是0.5,也就是说训练每一个连接都有50%的概率被去除。在测试时保留所有连接。</p>
<p>​    最后，在加入一层全连接层，把上一步得到的h_fc1_drop转换为10个类别的打分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">   W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">   b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">   y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure>
<p>​    y_conv相当于Softmax模型中的Logit，当然可以使用Softmax函数将其转换为10个类别的概率，在定义交叉熵损失。但其实Tensorflow提供了一个更直接的cross_entropy = tf.reduce_mean(    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))函数，它可以直接对Logit定义交叉熵损失，写法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line"><span class="comment"># 同样定义train_step</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>定义测试的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义测试的准确率</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>​    在验证集上计算模型准确率并输出，方便监控训练的进度，也可以据此调整模型的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>​    训练结束后，打印在全体测试集上的准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>​    得到的准确率结果应该在99%左右。与Softmax回归模型相比，使用两层卷积神经网络模型借助了卷积的威力，准确率非常大的提升。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># print(x_image.shape)</span></span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 第一层卷积层</span></span><br><span class="line">    W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line"></span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">    print(h_pool1.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">    print(h_pool2.shape)</span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">    W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">    <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">    W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">    <span class="comment"># cross_entropy = tf.reduce_mean(</span></span><br><span class="line">    <span class="comment">#     tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同样定义train_step</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试的准确率</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">    print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/v_july_v/article/details/51812459" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/51812459</a></p>
<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
<p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a></p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/14/tf9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/14/tf9/" itemprop="url">tensorflow入门笔记1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-14T15:29:59+08:00">
                2018-05-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/14/tf9/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/14/tf9/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="1、MNIST数据集介绍"><a href="#1、MNIST数据集介绍" class="headerlink" title="1、MNIST数据集介绍"></a>1、MNIST数据集介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍MNIST数据集。如果所示，MNIST数据集主要由一些手写数字的图片和相应的标签组成，图片一共有10类，分别对应0~9，共10个阿拉伯数字。如下图：</p>
<p><img src="/2018/05/14/tf9/image1.png" alt="1"></p>
<p>MNIST 数据集可在 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> 获取, 它包含了四个部分:</p>
<ul>
<li><strong>Training set images</strong>: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</li>
<li><strong>Training set labels</strong>: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</li>
<li><strong>Test set images</strong>: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</li>
<li><strong>Test set labels</strong>: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MNIST 数据集来自美国国家标准与技术研究所, <strong>National Institute of Standards and Technology (NIST)</strong>. 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据。 60,000 个训练样本和 10,000 个测试样本. </p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/14/tf9/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/11/embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/11/embedding/" itemprop="url">embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-11T21:30:51+08:00">
                2018-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/11/embedding/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/11/embedding/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h2 id="搜狐新闻数据-SogouCS"><a href="#搜狐新闻数据-SogouCS" class="headerlink" title="搜狐新闻数据(SogouCS)"></a>搜狐新闻数据(SogouCS)</h2><h5 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h5><p>来自搜狐新闻2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息</p>
<h5 id="格式说明："><a href="#格式说明：" class="headerlink" title="格式说明："></a>格式说明：</h5><p>数据格式为</p>
<doc><br><br><url>页面URL</url><br><br><docno>页面ID</docno><br><br><contenttitle>页面标题</contenttitle><br><br><content>页面内容</content><br><br></doc>

<p>注意：content字段去除了HTML标签，保存的是新闻正文文本</p>
<p>完整版(648MB)：<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.tar.gz" target="_blank" rel="noopener">tar.gz格式</a>，<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.zip" target="_blank" rel="noopener">zip格式</a> </p>
<h2 id="1、构建中文语料库"><a href="#1、构建中文语料库" class="headerlink" title="1、构建中文语料库"></a>1、构建中文语料库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜狐新闻 2.1G</span></span><br><span class="line">tar -zxvf news_sohusite_xml.full.tar.gz </span><br><span class="line">cat news_sohusite_xml.dat | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_sohusite.txt &gt; news_sohusite_cutword.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全网新闻 1.8G</span></span><br><span class="line">tar -zxvf news_tensites_xml.full.tar.gz </span><br><span class="line">cat news_tensites_xml.full.tar.gz | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_tensite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_tensite.txt &gt; news_tensite_cutword.txt</span><br></pre></td></tr></table></figure>
<p>第一条命令解压之后会生成news_sohusite_xml.dat文件</p>
<h2 id="2、利用gensim库进行训练"><a href="#2、利用gensim库进行训练" class="headerlink" title="2、利用gensim库进行训练"></a>2、利用gensim库进行训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment">#  Loading corpus...</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    input_file = <span class="string">"news_sohu_text.txt"</span></span><br><span class="line">    vector_file = <span class="string">"news_sohu_embedding"</span></span><br><span class="line">    <span class="comment"># 保存字典文件</span></span><br><span class="line">    vocab_file = <span class="string">"vocabulary_file"</span></span><br><span class="line">    sentences = word2vec.Text8Corpus(input_file)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Training embedding...</span></span><br><span class="line"><span class="string">    embedding_dim : 100</span></span><br><span class="line"><span class="string">    window_size ：5</span></span><br><span class="line"><span class="string">    minimum_count of word ：5</span></span><br><span class="line"><span class="string">    epoch of model（iter）：10</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 记录各种优先级的东西; 激活日志记录</span></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s: %(levelname)s: %(message)s'</span>, level=logging.INFO)</span><br><span class="line">     <span class="comment"># sg=0 使用cbow训练, sg=1对低频词较为敏感，默认是sg=0</span></span><br><span class="line">    model = word2vec.Word2Vec(sentences, size=<span class="number">100</span>, window=<span class="number">5</span>, min_count=<span class="number">6</span>, iter=<span class="number">10</span>, workers=multiprocessing.cpu_count())</span><br><span class="line">    <span class="comment">#  Saving word_embedding</span></span><br><span class="line">    model.wv.save_word2vec_format(vector_file, fvocab=vocab_file, binary=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>​    input_file是输入文件，如果是训练词向量，必须是分词好的文件，词与词之间空格隔开。如果训练的是字向量，字与字之间同样用空格隔开。一个句子是一个列表，所有的句子是二维列表。</p>
<h2 id="3、加载训练好的embedding并测试"><a href="#3、加载训练好的embedding并测试" class="headerlink" title="3、加载训练好的embedding并测试"></a>3、加载训练好的embedding并测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型，embedding_file是训练好的向量文件</span></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=<span class="keyword">False</span>)</span><br><span class="line">temp1 = model.most_similar(<span class="string">'我'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">temp2 = model.most_similar(<span class="string">'好'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">t = model.similarity(<span class="string">"他"</span>, <span class="string">"她"</span>)  <span class="comment"># 测试相似度</span></span><br><span class="line"></span><br><span class="line">print(temp1)</span><br><span class="line">print(temp2)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<p>我在搜狐新闻训练的向量测试的结果：</p>
<p><code>[(&#39;咱&#39;, 0.8646953105926514), (&#39;他&#39;, 0.857693076133728), (&#39;你&#39;, 0.7627511024475098), (&#39;她&#39;, 0.7525184750556946), (&#39;俺&#39;, 0.7336732149124146), (&#39;它&#39;, 0.7071460485458374), (&#39;牠&#39;, 0.46839216351509094), (&#39;谁&#39;, 0.4390422999858856), (&#39;您&#39;, 0.40126579999923706), (&#39;尤&#39;, 0.3958999514579773)]</code></p>
<p><code>[(&#39;懂&#39;, 0.4896905720233917), (&#39;棒&#39;, 0.4554690718650818), (&#39;是&#39;, 0.4444722533226013), (&#39;佳&#39;, 0.44040754437446594), (&#39;让&#39;, 0.43987756967544556), (&#39;快&#39;, 0.4291210174560547), (&#39;朋&#39;, 0.42199385166168213), (&#39;错&#39;, 0.4218539297580719), (&#39;多&#39;, 0.42170557379722595), (&#39;乖&#39;,</code>0.4164968729019165)]<code>``
</code>0.813737169426`</p>
<h2 id="4、训练注意"><a href="#4、训练注意" class="headerlink" title="4、训练注意"></a>4、训练注意</h2><p>Word2vec 有多个影响训练速度和质量的参数。</p>
<p>其中之一是用来修剪内部字典树的。在一个数以亿计的预料中出现一到两次的单词非常有可能是噪音或不需要被关注的。另外，也没有足够的数据对他们进行有意义的训练。因此，最好的办法就是直接将他们忽略掉。<br> <code>model = Word2Vec(sentences, min_count=10) # default value is 5</code></p>
<ol>
<li>对于设定 <code>min_count</code> 的值，合理的范围是0 - 100，可以根据数据集的规模进行调整。</li>
<li><p>另一个参数是神经网络 NN 层单元数，它也对应了训练算法的自由程度。</p>
<p><code>model = Word2Vec(sentences, size=200) # default value is 100</code><br>更大的 <code>size</code> 值需要更多的训练数据，但也同时可以得到更准确的模型。合理的取值范围是几十到几百。</p>
</li>
</ol>
<p>3.最后一个主要参数是训练并行粒度，用来加速训练。<br> <code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization</code><br> 该参数只有在机器已安装 Cython 情况下才会起到作用。如没有 Cython，则只能单核运行。</p>
<h2 id="5、评估"><a href="#5、评估" class="headerlink" title="5、评估"></a>5、评估</h2><p>Word2vec 训练是一个非监督任务，很难客观地评估结果。评估要<strong>依赖于后续的实际应用场景</strong>。Google 公布了一个包含 20,000 语法语义的测试样例，形式为 “A is to B as C is to D”。</p>
<p>需要注意的是，<strong>如在此测试样例上展示良好性能并不意味着在其它应用场景依然有效，反之亦然</strong>。</p>
<p>参考：</p>
<p>1.<a href="https://www.cnblogs.com/jkmiao/p/7007763.html" target="_blank" rel="noopener">https://www.cnblogs.com/jkmiao/p/7007763.html</a></p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/07/dp1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/07/dp1/" itemprop="url">动态规划</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-07T23:32:16+08:00">
                2018-05-07
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/07/dp1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/07/dp1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <p>​    <strong>动态规划中递推式的求解方法不是动态规划的本质。</strong> 动态规划的本质，是对问题<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 动态规划是通过<strong>拆分问题，</strong>定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。 本题下的其他答案，大多都是在说递推的求解方法，但<strong>如何拆分问题</strong>，才是动态规划的核心。 而<strong>拆分问题</strong>，靠的就是<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 要解决这个问题，我们首先要<strong>定义这个问题</strong>和这个问题的子问题。 有人可能会问了，题目都已经在这了，我们还需定义这个问题吗？需要，原因就是问题在字面上看，不容易找不出子问题，而没有子问题，这个题目就没办法解决 。状态转移方程，就是定义了问题和子问题之间的关系。 可以看出，状态转移方程就是带有条件的递推式。 </p>
<h2 id="一-阶乘-Factorial"><a href="#一-阶乘-Factorial" class="headerlink" title="一.阶乘(Factorial)"></a>一.阶乘(Factorial)</h2><p>$1\times 2\times3\times\dots\times N$,整数1到$N$的连乘积。$N$阶乘$N!$</p>
<p>分析：$N!$源自$(N-1)!$，如此就递回分割问题了。</p>
<p><img src="/2018/05/07/dp1/./dp1/wps63B6.tmp.jpg" alt="img"></p>
<p>阵列的每一格对应每一个问题。设定第一格的答案，再以回圈依序计算其余答案。</p>
<p><img src="/2018/05/07/dp1/D:/hexo\source\_posts/wps3581.jpg" alt="img"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> f[N];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f[i] = f[i<span class="number">-1</span>] * i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> f = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f *= i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h3><p>总共$N$个问题，每个问题花费$O(1)$时间，总共花费$O(N)$时间</p>
<h3 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h3><p>求$1!$到$N!$:总共$N$个问题，用一条$N$格阵列存储全部问题的答案，空间复杂度为$O(N)$</p>
<p>只求$N!$:用一个变量累计成绩，空间复杂度为$O(1)$</p>
<p>Dynamic Programming: recurrence</p>
<p>Dynamic Programming = Divide and Conquer + Memoization</p>
<p>​    动态规划是分治法的延伸。当递推分割出来的问题，一而再、再而三出现，就运用记忆法储存这些问题的答案，避免重复求解，以空间换取时间。动态规划的过程，就是反复地读取数据、计算数据、储存数据。</p>
<p><img src="/2018/05/07/dp1/wpsB825.jpg" alt="img"></p>
<ol>
<li><p>把原问题递归分割成许多更小的问题。（recurrence）</p>
<p>子问题与原问题的求解方式皆类似。（optimal sub-structure）</p>
<p>子问题会一而再、再而三的出现。（overlapping sub-problems）</p>
</li>
<li><p>设计计算过程：</p>
<p> 确认每个问题需要哪些子问题来计算答案。（recurrence）</p>
<p> 确认总共有哪些问题。（state space）</p>
<p>把问题一一对应到表格。（lookup table）</p>
<p>决定问题的计算顺序。（computational sequence）</p>
</li>
</ol>
<p>​       确认初始值、计算范围（initial states / boundary）   </p>
<ol>
<li><p>实作，主要有两种方式：</p>
<p> Top-down</p>
<p> Bottom-up</p>
</li>
</ol>
<h3 id="1-recurrence"><a href="#1-recurrence" class="headerlink" title="1. recurrence"></a>1. recurrence</h3><p>​    递归分割问题时，当子问题与原问题完全相同，只有数值范围不同，我们称此现象为recurrence，再度出现、一而再出现之意。</p>
<p>【注： recursion 和 recurrence ，中文都翻译为“递归”，然而两者意义大不相同，读者切莫混淆】</p>
<p>​    此处以爬楼梯问题当做范例。先前于递归法章节，已经谈过如何求踏法，而此处要谈如何求踏法数目。</p>
<p><img src="/2018/05/07/dp1/wpsB76A.jpg" alt="img"></p>
<p>​    踏上第五阶，只能从第四阶或从第三阶踏过去，因此爬到五阶源自两个子问题：爬到四阶与爬到三阶。</p>
<p><img src="/2018/05/07/dp1/a1.png" alt="a"></p>
<p>​    爬到五阶的踏法数目，就是综合爬到四阶与爬到三阶的踏法数目。写成数学式子是$f(5)=f(4)+f(3)$，其中$f$表示爬到某阶的踏法数目。</p>
<p>​    依样画葫芦，得到<br>$$<br>f(4)=f(3)+f(2)\\<br>f(3)=f(2)+f(1)<br>$$<br>​    爬到两阶与爬到一阶无法再分割，没有子问题，直接得到<br>$$<br>f(2)=2\\<br>f(1)=1<br>$$<br>整理成一道简明扼要的递归公式：<br>$$<br>f(n)= \begin{cases}<br>1&amp;if   n=1 \\<br>2  &amp; if   n =2 \\<br>f(n-1)+f(n-2) &amp; if  n \ge3  and   n\le 5<br>\end{cases}<br>$$<br>爬到任何一阶的踏法数目，都可以借由这道递归公式求得。<img src="/2018/05/07/dp1/D:/hexo\source\_posts/wpsE565.tmp.png" alt="img">带入实际数值，递归计算即可。</p>
<p>​    为什么分割问题之后，就容易计算答案呢？因为分割问题时，同时也分类了这个问题所有可能答案。分类使得答案的规律变得单纯，于是更容易求得答案。</p>
<p><img src="/2018/05/07/dp1/wps2ACB.jpg" alt="img"></p>
<h3 id="2-、state-space"><a href="#2-、state-space" class="headerlink" title="2 、state space"></a>2 、state space</h3><p>想要计算第五阶的踏法数目。</p>
<p>全部的问题是“爬到一节”、“爬到二阶”、“爬到三阶”、“爬到四阶”、“爬到五阶”。</p>
<p><img src="/2018/05/07/dp1/wps2204.jpg" alt="img"></p>
<p>至于爬到零阶、爬到负一阶。爬到负二阶以及爬到六阶、爬到七阶没有必要计算。</p>
<h3 id="3、lookup-table"><a href="#3、lookup-table" class="headerlink" title="3、lookup table"></a>3、lookup table</h3><p>​    建立六格的阵列，存储五个问题的答案。</p>
<p>​    表格的第零格不使用，第一格式爬到一阶的答案，第二格是爬到二阶的答案，以此类推。</p>
<p><img src="/2018/05/07/dp1/wps75F.jpg" alt="img"></p>
<p>​    如果只计算爬完五阶，也可以建立三个变量交替使用。</p>
<h3 id="4、computational-sequence"><a href="#4、computational-sequence" class="headerlink" title="4、computational sequence"></a>4、computational sequence</h3><p>​    因为每个问题都依赖阶数少一阶、阶数少两阶这两个问题，所以必须由阶数小的问题开始计算。</p>
<p>​    计算顺序是爬到一阶、爬到二阶、……、爬到五阶。设定初始值。</p>
<h3 id="5、-initial-states-boundary"><a href="#5、-initial-states-boundary" class="headerlink" title="5、 initial states / boundary"></a>5、 initial states / boundary</h3><p>​    最先计算的问题时爬到一阶与爬到二阶，必须预先将答案填入表格。写入方程式，才能计算其他问题。心算求得爬到一阶的答案是1，爬到二阶的答案是2。最后计算的问题时原问题是原问题爬到五阶。</p>
<p>​    为了让表格更顺畅。为了让程式更漂亮，可以加入爬到零阶的答案，对应到表格的第零格。爬到零阶的答案，可以运用爬到一阶的答案与爬到两阶的答案，刻意逆推而得。</p>
<p><img src="/2018/05/07/dp1/wps589E.jpg" alt="img"></p>
<p>​    最后可以把初始值，尚待计算的部分、不需计算的部分，整理成一道递归公式：<br>$$<br>f(n)= \begin{cases}<br>0 &amp; if  n<0\\ 0="" 1="" &="" if="" \="" n="0\\" 1&if="" \\="" f(n-1)+f(n-2)="" \ge2="" and="" n\le="" 5\\="">5<br>\end{cases}<br>$$</0\\></p>
<h3 id="6、实现"><a href="#6、实现" class="headerlink" title="6、实现"></a>6、实现</h3><p>​    直接用递归实作，而不使用记忆体存储各个问题的答案，是最直接的方式，也是最慢的方式。时间复杂度是$O(f(n))$。问题一而再、再而三的出现，不断呼叫同样的函数求解，效率不彰。刚接触DP的新手常犯这种错误。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​    正确的DP是一边计算，一边将计算出来的数值存入表格，以便不必重算。这里整理了两种实现方式，各有优缺点。</p>
<p>1)Top-down</p>
<p>2)Bottom-up</p>
<p><img src="/2018/05/07/dp1/wps58E0.jpg" alt="img"></p>
<p>1)Top-down</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 表格，储存全部问题的答案。</span></span><br><span class="line"><span class="keyword">bool</span> solve[<span class="number">6</span>];  <span class="comment">// 记录问题是否已经計算完毕</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> table[n] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 如果已經計算過，就直接讀取表格的答案。</span></span><br><span class="line">    <span class="keyword">if</span> (solve[n]) <span class="keyword">return</span> table[n];</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 如果不曾計算過，就計算一遍，儲存答案。</span></span><br><span class="line">    table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>); <span class="comment">// 將答案存入表格</span></span><br><span class="line">    solve[n] = <span class="literal">true</span>;            <span class="comment">// 已經計算完畢</span></span><br><span class="line">    <span class="keyword">return</span> table[n];</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        solve[i] = <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 合并solve跟table，简化代码。</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 用 0 代表该问题还未计算答案</span></span><br><span class="line"><span class="comment">//  if (table[n] != 0) </span></span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line"><span class="keyword">if</span> (table[n]) </span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line">    <span class="keyword">return</span> table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">     table[i] = <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​    这个实现方式的好处是不必斤斤计较计算顺序，因为代码中的递归结构会迫使最小的问题先被计算。这个实现方式的另一个好处是只计算必要的问题，而不必计算所有可能的问题。</p>
<p>​    这个实现的坏处是代码采用递归结构，不断调用函数，执行效率差。这个实现方式的另一个坏处是无法自由地控制计算顺序，因而无法妥善运用记忆体，浪费了可回收再利用的记忆体。</p>
<p>2）Bottom-up</p>
<p>​    指定一个计算顺序，然后由最小问题开始计算。特色是代码通常只有几个递归。这个实现方式的好处与坏处与前一个方式恰好互补。</p>
<p>​    首先建立表格。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">5</span> + <span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<p>心算爬到零阶的答案、爬到一阶的答案，填入报个当中，作为初始值。分别天道表格的第零格、第一格。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">table[0] = 1;</span><br><span class="line">table[1] = 1;</span><br></pre></td></tr></table></figure>
<p>​    尚待计算的部分就是爬到两阶的答案、……、爬到五阶的答案。通常是使用递归，按照计算顺序来计算。</p>
<p>计算过程的实现方式，有两种迥异的风格。一种是往回取值，是常见的实现方式。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">往回取值</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    table[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        table[i] = table[i<span class="number">-1</span>] + table[i<span class="number">-2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另一种是往后补值，是罕见的实现方式。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">往后补值<span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++) table[i] = <span class="number">0</span>;</span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//  table[1] = 1;   // 剛好可以被算到</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span> &lt;= <span class="number">5</span>) table[i+<span class="number">1</span>] += table[i];</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">2</span> &lt;= <span class="number">5</span>) table[i+<span class="number">2</span>] += table[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="7、总结"><a href="#7、总结" class="headerlink" title="7、总结"></a>7、总结</h3><p>​    第一，先找到原问题和其子问题之间的关系，写成递归公式。如此一来，即可利用递归公式，用子子问题的答案，求出子问题的答案；用子问题的答案，求出原问题的答案。</p>
<p>​    第二。确认可能出现的问题全部总共有哪些；用子问题的答案，求出原问题的答案。</p>
<p>​    第三。有了递归公式之后，就必须安排出一套计算顺序。大问题的答案，总是以小问题的答案来求得的，所以，小问题的答案必须是先算的，否则大问题的答案从何而来呢？一个好的安排方式，不但使代码易写，还可重复利用记忆体空间。</p>
<p>​    第四。记得先将最小，最先被计算的问题，心算出答案，储存如表格，内建与代码之中。一道递归公式必须拥有初始值，才有计算其他项。</p>
<p>​    第五。实现DP的代码时，会建立一个表格，表格存入所有大小问题的答案。安排好每个问题的答案再表格的哪个位置，这样计算时才能知道该在哪里取值。</p>
<p>​    切勿存取超出表格的原始，产生溢位情形，导致答案算错。计算过程当中，一旦某个问题的答案出错，就会如骨牌效应般一个影响一个，造成很难除错。</p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4"
                alt="goingcoder" />
            
              <p class="site-author-name" itemprop="name">goingcoder</p>
              <p class="site-description motion-element" itemprop="description">匆忙世间的闲人。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/goingcoder" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
              </a>
            </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.hustyrf.top" title="陈冠希" target="_blank">陈冠希</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">goingcoder</span>

  
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>

-->



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"your-duoshuo-shortname"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


















  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
