<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="离弦的博客" type="application/atom+xml" />






<meta name="description" content="匆忙世间的闲人。">
<meta name="keywords" content="Hexo,next">
<meta property="og:type" content="website">
<meta property="og:title" content="离弦的博客">
<meta property="og:url" content="https://goingcoder.github.io/index.html">
<meta property="og:site_name" content="离弦的博客">
<meta property="og:description" content="匆忙世间的闲人。">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="离弦的博客">
<meta name="twitter:description" content="匆忙世间的闲人。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://goingcoder.github.io/"/>





  <title>离弦的博客</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?your-analytics-id";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband">
		
	</div>
	
	
	
	<a href="https://github.com/goingcoder" class="github-corner" aria-label="View source on Github">
		<svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
			<path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
	
	<header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">离弦的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">我若为王,舍我其谁</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'your-swiftype-key','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/10/ner8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/ner8/" itemprop="url">Transfer learning for sequence tagging with hierarchical recurrent networks笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T16:37:06+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/10/ner8/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/10/ner8/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <p><strong>转载自-李虹磊 基于迁移学习和深度神经网络的序列标注方法</strong> </p>
<p><strong>论文题目：Transfer learning for sequence tagging with hierarchical recurrent networks</strong></p>
<p><strong>论文作者：Zhilin Yang, Ruslan Salakhutdinov, William W. Cohen</strong></p>
<p><strong>School of Computer Science Carnegie Mellon University</strong></p>
<p><strong>论文出处：Published as a conference paper at ICLR 2017</strong></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    本文利用迁移学习的思想和神经网络模型来解决序列标注问题。使用有大量已标注数据集的任务作为源数据领域，来提升只有少量标注数据的目标任务的性能。本文从跨域、跨应用和跨语言三个迁移设置来验证迁移学习用于深度层级循环神经网络中的性能。实验表明，在深度层级神经网络的基础上使用迁移学习的思想可以显著提高序列标注的性能。 </p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/10/ner8/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/10/ner7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/ner7/" itemprop="url">Multi-Task Cross-Lingual Sequence Tagging from Scratch笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T15:27:55+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/10/ner7/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/10/ner7/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    论文提出了一个深度层叠循环神经网络的序列标注模型，这个模型是任务独立的，语言独立的，以及没有特征工程。论文探索了两种类型的联合训练，在多任务联合训练中，同种语言多个任务连个训练——英文的词性标注和实体识别。在跨语言联合训练中，模型训练同一个任务在不同的语言上——英文和西班牙语中的实体识别。论文表明多任务和跨语言的联合训练可以在不同场景提高性能。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/10/ner7/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/08/ner6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/ner6/" itemprop="url">Contextual String Embeddings for Sequence Labeling笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T13:40:45+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/08/ner6/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/08/ner6/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    使用递归神经网络进行语言建模的最新进展表明基于字符的语言建模是可行的。这篇论文提出利用一个预训练的字符语言模型的内部状态，以产生一种新型的字嵌入，称之为上下文字符串向量。我们提出的embedding有明显的优点：（1）在没有任何明确的词汇概念的情况下进行训练，从根本上将单词作为字符序列建模。（2）由其周围语境上下文化，这意味着相同的单词的embedding将根据其不同的语境的不同而不同。我们对先前的embedding进行比较评估，发现我们的embedding对于下游任务非常有用。在四个经典序列标记任务中，我们始终优于当前最优的结果。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/08/ner6/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/08/ner5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/ner5/" itemprop="url">Deep contextualized word representations论文笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T09:22:44+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/08/ner5/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/08/ner5/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    预训练的词向量是很多神经语言理解模型的关键部分，然而学习高质量的表示是很有挑战性的。一个好的词向量应该包含：（1）能够建模词的复杂字符特征（比如语法和语义），（2）能够在不同语境有不同反映（如多义词）。这篇论文中，引入一种新类型的深度上下文词表示，可以应对这些挑战，并且可以容易与当前存在的模型进行整合，有效提升了一系列有挑战的语言理解问题的最优性能。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/08/ner5/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/07/06/ner4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/06/ner4/" itemprop="url">Semi-supervised sequence tagging with bidirectional language models笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-06T16:23:33+08:00">
                2018-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ner/" itemprop="url" rel="index">
                    <span itemprop="name">ner</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/06/ner4/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/06/ner4/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    对于NLP任务，从无标签文本中学习预训练的embedding已经是神经网络的一个标准部分。在这篇论文中，提出了将一个双向的语言模型的context embedding引入序列标注任务中，在NER和chunking两个标准数据集上进行了实验，证明了与其他的某些使用迁移学习和使用附加标注数据和特定任务字典的联合学习相比，效果都要好，超过了以往所有的模型。</p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul>
<li><p>主要贡献是表明在LM embedding中捕获的上下文敏感表示在监督序列标记设置中是有用的。</p>
</li>
<li><p>双向的LM embedding比前向的LM embedding效果要好。</p></li></ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/07/06/ner4/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/17/tf10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/17/tf10/" itemprop="url">CNN卷积神经网络对MNIST数据分类</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-17T14:31:32+08:00">
                2018-05-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/17/tf10/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/17/tf10/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <p>​    Convolutional Neural Networks (CNN) 是神经网络处理图片信息的一大利器. 因为利用卷积神经网络在图像和语音识别方面能够给出更优预测结果, 这一种技术也被广泛的传播可应用. 卷积神经网络最常被应用的方面是计算机的图像识别, 不过因为不断地创新, 它也被应用在视频分析, 自然语言处理, 药物发现, 等等. 近期最火的 Alpha Go, 让计算机看懂围棋, 同样也是有运用到这门技术。</p>
<p>​    这里将建立一个卷积神经网络，它可以把MNIST手写字符的识别准确率提高到99%，读者可能需要一些卷积神经网络的基础知识才能更好地理解本节的内容。</p>
<p>​    程序的开头依旧是导入Tensorflow：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br></pre></td></tr></table></figure>
<p>​    接下来载入MNIST数据集，并建立占位符。占位符x的含义为训练图像，y_为对应训练图像的标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>​    由于要使用的是卷积神经网络对图像进行分类，所以不能再使用784维的向量表示输入的x，而是将其还原为28*28的图片形式。[-1,28,28,1]中的-1表示形状第一维的大小是根据x自动确定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>​    x_image就是输入的训练图像，接下来。我们堆训练图像进行卷积操作，第一层卷积的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"><span class="comment"># 第一层卷积层</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br></pre></td></tr></table></figure>
<p>​    先定义四个函数，函数weight_variable可以返回一个给定形状的变量并自动以截断正态分布初始化，bias_variable同样返回一个给定形状的变量，初始化时所有值是0.1，可分别用这两个函数创建卷积的核与偏置。h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)是真正进行卷积运算，卷积计算后选用ReLU作为激活函数。h_pool1 = max_pool_2x2(h_conv1)是调用函数max_pool_2x2(h_conv1)进行一次池化操作。卷积、激活函数、池化，可以说是一个卷积层的“标配”,通常一个卷积层都会包含这三个步骤，有时会去掉最后的池化操作。</p>
<p>​    tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。 </p>
<p>​    对第一个卷积操作后产生的h_pool1再做一次卷积计算，使用的代码与上面类似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<p>两层卷积之后是全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">   W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">   b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">   h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">   h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">   <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">   keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">   h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<p>​    在全连接中加入了Dropout，它是防止神经网络过拟合的一种手段。在每一步训练时，以一定的概率“去掉”网络中的某些连接，但这种去除不是永久性的，只是在当前步骤中去除，并且每一步去除的连接都是随机选择的。在这个程序中，选择的Dropout概率是0.5,也就是说训练每一个连接都有50%的概率被去除。在测试时保留所有连接。</p>
<p>​    最后，在加入一层全连接层，把上一步得到的h_fc1_drop转换为10个类别的打分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">   W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">   b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">   y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure>
<p>​    y_conv相当于Softmax模型中的Logit，当然可以使用Softmax函数将其转换为10个类别的概率，在定义交叉熵损失。但其实Tensorflow提供了一个更直接的cross_entropy = tf.reduce_mean(    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))函数，它可以直接对Logit定义交叉熵损失，写法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line"><span class="comment"># 同样定义train_step</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>定义测试的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义测试的准确率</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>​    在验证集上计算模型准确率并输出，方便监控训练的进度，也可以据此调整模型的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>​    训练结束后，打印在全体测试集上的准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>​    得到的准确率结果应该在99%左右。与Softmax回归模型相比，使用两层卷积神经网络模型借助了卷积的威力，准确率非常大的提升。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># print(x_image.shape)</span></span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 第一层卷积层</span></span><br><span class="line">    W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line"></span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">    print(h_pool1.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">    print(h_pool2.shape)</span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">    W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">    <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">    W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">    <span class="comment"># cross_entropy = tf.reduce_mean(</span></span><br><span class="line">    <span class="comment">#     tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同样定义train_step</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试的准确率</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">    print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/v_july_v/article/details/51812459" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/51812459</a></p>
<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
<p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a></p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/14/tf9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/14/tf9/" itemprop="url">tensorflow入门笔记1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-14T15:29:59+08:00">
                2018-05-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/14/tf9/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/14/tf9/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          <h2 id="1、MNIST数据集介绍"><a href="#1、MNIST数据集介绍" class="headerlink" title="1、MNIST数据集介绍"></a>1、MNIST数据集介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍MNIST数据集。如果所示，MNIST数据集主要由一些手写数字的图片和相应的标签组成，图片一共有10类，分别对应0~9，共10个阿拉伯数字。如下图：</p>
<p><img src="/2018/05/14/tf9/image1.png" alt="1"></p>
<p>MNIST 数据集可在 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> 获取, 它包含了四个部分:</p>
<ul>
<li><strong>Training set images</strong>: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</li>
<li><strong>Training set labels</strong>: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</li>
<li><strong>Test set images</strong>: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</li>
<li><strong>Test set labels</strong>: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MNIST 数据集来自美国国家标准与技术研究所, <strong>National Institute of Standards and Technology (NIST)</strong>. 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据。 60,000 个训练样本和 10,000 个测试样本. </p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/05/14/tf9/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/11/embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/11/embedding/" itemprop="url">embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-11T21:30:51+08:00">
                2018-05-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/11/embedding/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/11/embedding/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h2 id="搜狐新闻数据-SogouCS"><a href="#搜狐新闻数据-SogouCS" class="headerlink" title="搜狐新闻数据(SogouCS)"></a>搜狐新闻数据(SogouCS)</h2><h5 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h5><p>来自搜狐新闻2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息</p>
<h5 id="格式说明："><a href="#格式说明：" class="headerlink" title="格式说明："></a>格式说明：</h5><p>数据格式为</p>
<doc><br><br><url>页面URL</url><br><br><docno>页面ID</docno><br><br><contenttitle>页面标题</contenttitle><br><br><content>页面内容</content><br><br></doc>

<p>注意：content字段去除了HTML标签，保存的是新闻正文文本</p>
<p>完整版(648MB)：<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.tar.gz" target="_blank" rel="noopener">tar.gz格式</a>，<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.zip" target="_blank" rel="noopener">zip格式</a> </p>
<h2 id="1、构建中文语料库"><a href="#1、构建中文语料库" class="headerlink" title="1、构建中文语料库"></a>1、构建中文语料库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜狐新闻 2.1G</span></span><br><span class="line">tar -zxvf news_sohusite_xml.full.tar.gz </span><br><span class="line">cat news_sohusite_xml.dat | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_sohusite.txt &gt; news_sohusite_cutword.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全网新闻 1.8G</span></span><br><span class="line">tar -zxvf news_tensites_xml.full.tar.gz </span><br><span class="line">cat news_tensites_xml.full.tar.gz | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_tensite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_tensite.txt &gt; news_tensite_cutword.txt</span><br></pre></td></tr></table></figure>
<p>第一条命令解压之后会生成news_sohusite_xml.dat文件</p>
<h2 id="2、利用gensim库进行训练"><a href="#2、利用gensim库进行训练" class="headerlink" title="2、利用gensim库进行训练"></a>2、利用gensim库进行训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment">#  Loading corpus...</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    input_file = <span class="string">"news_sohu_text.txt"</span></span><br><span class="line">    vector_file = <span class="string">"news_sohu_embedding"</span></span><br><span class="line">    <span class="comment"># 保存字典文件</span></span><br><span class="line">    vocab_file = <span class="string">"vocabulary_file"</span></span><br><span class="line">    sentences = word2vec.Text8Corpus(input_file)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Training embedding...</span></span><br><span class="line"><span class="string">    embedding_dim : 100</span></span><br><span class="line"><span class="string">    window_size ：5</span></span><br><span class="line"><span class="string">    minimum_count of word ：5</span></span><br><span class="line"><span class="string">    epoch of model（iter）：10</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 记录各种优先级的东西; 激活日志记录</span></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s: %(levelname)s: %(message)s'</span>, level=logging.INFO)</span><br><span class="line">     <span class="comment"># sg=0 使用cbow训练, sg=1对低频词较为敏感，默认是sg=0</span></span><br><span class="line">    model = word2vec.Word2Vec(sentences, size=<span class="number">100</span>, window=<span class="number">5</span>, min_count=<span class="number">6</span>, iter=<span class="number">10</span>, workers=multiprocessing.cpu_count())</span><br><span class="line">    <span class="comment">#  Saving word_embedding</span></span><br><span class="line">    model.wv.save_word2vec_format(vector_file, fvocab=vocab_file, binary=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>​    input_file是输入文件，如果是训练词向量，必须是分词好的文件，词与词之间空格隔开。如果训练的是字向量，字与字之间同样用空格隔开。一个句子是一个列表，所有的句子是二维列表。</p>
<h2 id="3、加载训练好的embedding并测试"><a href="#3、加载训练好的embedding并测试" class="headerlink" title="3、加载训练好的embedding并测试"></a>3、加载训练好的embedding并测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型，embedding_file是训练好的向量文件</span></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=<span class="keyword">False</span>)</span><br><span class="line">temp1 = model.most_similar(<span class="string">'我'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">temp2 = model.most_similar(<span class="string">'好'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">t = model.similarity(<span class="string">"他"</span>, <span class="string">"她"</span>)  <span class="comment"># 测试相似度</span></span><br><span class="line"></span><br><span class="line">print(temp1)</span><br><span class="line">print(temp2)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<p>我在搜狐新闻训练的向量测试的结果：</p>
<p><code>[(&#39;咱&#39;, 0.8646953105926514), (&#39;他&#39;, 0.857693076133728), (&#39;你&#39;, 0.7627511024475098), (&#39;她&#39;, 0.7525184750556946), (&#39;俺&#39;, 0.7336732149124146), (&#39;它&#39;, 0.7071460485458374), (&#39;牠&#39;, 0.46839216351509094), (&#39;谁&#39;, 0.4390422999858856), (&#39;您&#39;, 0.40126579999923706), (&#39;尤&#39;, 0.3958999514579773)]</code></p>
<p><code>[(&#39;懂&#39;, 0.4896905720233917), (&#39;棒&#39;, 0.4554690718650818), (&#39;是&#39;, 0.4444722533226013), (&#39;佳&#39;, 0.44040754437446594), (&#39;让&#39;, 0.43987756967544556), (&#39;快&#39;, 0.4291210174560547), (&#39;朋&#39;, 0.42199385166168213), (&#39;错&#39;, 0.4218539297580719), (&#39;多&#39;, 0.42170557379722595), (&#39;乖&#39;,</code>0.4164968729019165)]<code>``
</code>0.813737169426`</p>
<h2 id="4、训练注意"><a href="#4、训练注意" class="headerlink" title="4、训练注意"></a>4、训练注意</h2><p>Word2vec 有多个影响训练速度和质量的参数。</p>
<p>其中之一是用来修剪内部字典树的。在一个数以亿计的预料中出现一到两次的单词非常有可能是噪音或不需要被关注的。另外，也没有足够的数据对他们进行有意义的训练。因此，最好的办法就是直接将他们忽略掉。<br> <code>model = Word2Vec(sentences, min_count=10) # default value is 5</code></p>
<ol>
<li>对于设定 <code>min_count</code> 的值，合理的范围是0 - 100，可以根据数据集的规模进行调整。</li>
<li><p>另一个参数是神经网络 NN 层单元数，它也对应了训练算法的自由程度。</p>
<p><code>model = Word2Vec(sentences, size=200) # default value is 100</code><br>更大的 <code>size</code> 值需要更多的训练数据，但也同时可以得到更准确的模型。合理的取值范围是几十到几百。</p>
</li>
</ol>
<p>3.最后一个主要参数是训练并行粒度，用来加速训练。<br> <code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization</code><br> 该参数只有在机器已安装 Cython 情况下才会起到作用。如没有 Cython，则只能单核运行。</p>
<h2 id="5、评估"><a href="#5、评估" class="headerlink" title="5、评估"></a>5、评估</h2><p>Word2vec 训练是一个非监督任务，很难客观地评估结果。评估要<strong>依赖于后续的实际应用场景</strong>。Google 公布了一个包含 20,000 语法语义的测试样例，形式为 “A is to B as C is to D”。</p>
<p>需要注意的是，<strong>如在此测试样例上展示良好性能并不意味着在其它应用场景依然有效，反之亦然</strong>。</p>
<p>参考：</p>
<p>1.<a href="https://www.cnblogs.com/jkmiao/p/7007763.html" target="_blank" rel="noopener">https://www.cnblogs.com/jkmiao/p/7007763.html</a></p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/05/07/dp1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/07/dp1/" itemprop="url">动态规划</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-07T23:32:16+08:00">
                2018-05-07
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/07/dp1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/05/07/dp1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <p>​    <strong>动态规划中递推式的求解方法不是动态规划的本质。</strong> 动态规划的本质，是对问题<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 动态规划是通过<strong>拆分问题，</strong>定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。 本题下的其他答案，大多都是在说递推的求解方法，但<strong>如何拆分问题</strong>，才是动态规划的核心。 而<strong>拆分问题</strong>，靠的就是<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 要解决这个问题，我们首先要<strong>定义这个问题</strong>和这个问题的子问题。 有人可能会问了，题目都已经在这了，我们还需定义这个问题吗？需要，原因就是问题在字面上看，不容易找不出子问题，而没有子问题，这个题目就没办法解决 。状态转移方程，就是定义了问题和子问题之间的关系。 可以看出，状态转移方程就是带有条件的递推式。 </p>
<h2 id="一-阶乘-Factorial"><a href="#一-阶乘-Factorial" class="headerlink" title="一.阶乘(Factorial)"></a>一.阶乘(Factorial)</h2><p>$1\times 2\times3\times\dots\times N$,整数1到$N$的连乘积。$N$阶乘$N!$</p>
<p>分析：$N!$源自$(N-1)!$，如此就递回分割问题了。</p>
<p><img src="/2018/05/07/dp1/./dp1/wps63B6.tmp.jpg" alt="img"></p>
<p>阵列的每一格对应每一个问题。设定第一格的答案，再以回圈依序计算其余答案。</p>
<p><img src="/2018/05/07/dp1/D:/hexo\source\_posts/wps3581.jpg" alt="img"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> f[N];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f[i] = f[i<span class="number">-1</span>] * i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> f = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f *= i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h3><p>总共$N$个问题，每个问题花费$O(1)$时间，总共花费$O(N)$时间</p>
<h3 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h3><p>求$1!$到$N!$:总共$N$个问题，用一条$N$格阵列存储全部问题的答案，空间复杂度为$O(N)$</p>
<p>只求$N!$:用一个变量累计成绩，空间复杂度为$O(1)$</p>
<p>Dynamic Programming: recurrence</p>
<p>Dynamic Programming = Divide and Conquer + Memoization</p>
<p>​    动态规划是分治法的延伸。当递推分割出来的问题，一而再、再而三出现，就运用记忆法储存这些问题的答案，避免重复求解，以空间换取时间。动态规划的过程，就是反复地读取数据、计算数据、储存数据。</p>
<p><img src="/2018/05/07/dp1/wpsB825.jpg" alt="img"></p>
<ol>
<li><p>把原问题递归分割成许多更小的问题。（recurrence）</p>
<p>子问题与原问题的求解方式皆类似。（optimal sub-structure）</p>
<p>子问题会一而再、再而三的出现。（overlapping sub-problems）</p>
</li>
<li><p>设计计算过程：</p>
<p> 确认每个问题需要哪些子问题来计算答案。（recurrence）</p>
<p> 确认总共有哪些问题。（state space）</p>
<p>把问题一一对应到表格。（lookup table）</p>
<p>决定问题的计算顺序。（computational sequence）</p>
</li>
</ol>
<p>​       确认初始值、计算范围（initial states / boundary）   </p>
<ol>
<li><p>实作，主要有两种方式：</p>
<p> Top-down</p>
<p> Bottom-up</p>
</li>
</ol>
<h3 id="1-recurrence"><a href="#1-recurrence" class="headerlink" title="1. recurrence"></a>1. recurrence</h3><p>​    递归分割问题时，当子问题与原问题完全相同，只有数值范围不同，我们称此现象为recurrence，再度出现、一而再出现之意。</p>
<p>【注： recursion 和 recurrence ，中文都翻译为“递归”，然而两者意义大不相同，读者切莫混淆】</p>
<p>​    此处以爬楼梯问题当做范例。先前于递归法章节，已经谈过如何求踏法，而此处要谈如何求踏法数目。</p>
<p><img src="/2018/05/07/dp1/wpsB76A.jpg" alt="img"></p>
<p>​    踏上第五阶，只能从第四阶或从第三阶踏过去，因此爬到五阶源自两个子问题：爬到四阶与爬到三阶。</p>
<p><img src="/2018/05/07/dp1/a1.png" alt="a"></p>
<p>​    爬到五阶的踏法数目，就是综合爬到四阶与爬到三阶的踏法数目。写成数学式子是$f(5)=f(4)+f(3)$，其中$f$表示爬到某阶的踏法数目。</p>
<p>​    依样画葫芦，得到<br>$$<br>f(4)=f(3)+f(2)\\<br>f(3)=f(2)+f(1)<br>$$<br>​    爬到两阶与爬到一阶无法再分割，没有子问题，直接得到<br>$$<br>f(2)=2\\<br>f(1)=1<br>$$<br>整理成一道简明扼要的递归公式：<br>$$<br>f(n)= \begin{cases}<br>1&amp;if   n=1 \\<br>2  &amp; if   n =2 \\<br>f(n-1)+f(n-2) &amp; if  n \ge3  and   n\le 5<br>\end{cases}<br>$$<br>爬到任何一阶的踏法数目，都可以借由这道递归公式求得。<img src="/2018/05/07/dp1/D:/hexo\source\_posts/wpsE565.tmp.png" alt="img">带入实际数值，递归计算即可。</p>
<p>​    为什么分割问题之后，就容易计算答案呢？因为分割问题时，同时也分类了这个问题所有可能答案。分类使得答案的规律变得单纯，于是更容易求得答案。</p>
<p><img src="/2018/05/07/dp1/wps2ACB.jpg" alt="img"></p>
<h3 id="2-、state-space"><a href="#2-、state-space" class="headerlink" title="2 、state space"></a>2 、state space</h3><p>想要计算第五阶的踏法数目。</p>
<p>全部的问题是“爬到一节”、“爬到二阶”、“爬到三阶”、“爬到四阶”、“爬到五阶”。</p>
<p><img src="/2018/05/07/dp1/wps2204.jpg" alt="img"></p>
<p>至于爬到零阶、爬到负一阶。爬到负二阶以及爬到六阶、爬到七阶没有必要计算。</p>
<h3 id="3、lookup-table"><a href="#3、lookup-table" class="headerlink" title="3、lookup table"></a>3、lookup table</h3><p>​    建立六格的阵列，存储五个问题的答案。</p>
<p>​    表格的第零格不使用，第一格式爬到一阶的答案，第二格是爬到二阶的答案，以此类推。</p>
<p><img src="/2018/05/07/dp1/wps75F.jpg" alt="img"></p>
<p>​    如果只计算爬完五阶，也可以建立三个变量交替使用。</p>
<h3 id="4、computational-sequence"><a href="#4、computational-sequence" class="headerlink" title="4、computational sequence"></a>4、computational sequence</h3><p>​    因为每个问题都依赖阶数少一阶、阶数少两阶这两个问题，所以必须由阶数小的问题开始计算。</p>
<p>​    计算顺序是爬到一阶、爬到二阶、……、爬到五阶。设定初始值。</p>
<h3 id="5、-initial-states-boundary"><a href="#5、-initial-states-boundary" class="headerlink" title="5、 initial states / boundary"></a>5、 initial states / boundary</h3><p>​    最先计算的问题时爬到一阶与爬到二阶，必须预先将答案填入表格。写入方程式，才能计算其他问题。心算求得爬到一阶的答案是1，爬到二阶的答案是2。最后计算的问题时原问题是原问题爬到五阶。</p>
<p>​    为了让表格更顺畅。为了让程式更漂亮，可以加入爬到零阶的答案，对应到表格的第零格。爬到零阶的答案，可以运用爬到一阶的答案与爬到两阶的答案，刻意逆推而得。</p>
<p><img src="/2018/05/07/dp1/wps589E.jpg" alt="img"></p>
<p>​    最后可以把初始值，尚待计算的部分、不需计算的部分，整理成一道递归公式：<br>$$<br>f(n)= \begin{cases}<br>0 &amp; if  n<0\\ 0="" 1="" &="" if="" \="" n="0\\" 1&if="" \\="" f(n-1)+f(n-2)="" \ge2="" and="" n\le="" 5\\="">5<br>\end{cases}<br>$$</0\\></p>
<h3 id="6、实现"><a href="#6、实现" class="headerlink" title="6、实现"></a>6、实现</h3><p>​    直接用递归实作，而不使用记忆体存储各个问题的答案，是最直接的方式，也是最慢的方式。时间复杂度是$O(f(n))$。问题一而再、再而三的出现，不断呼叫同样的函数求解，效率不彰。刚接触DP的新手常犯这种错误。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​    正确的DP是一边计算，一边将计算出来的数值存入表格，以便不必重算。这里整理了两种实现方式，各有优缺点。</p>
<p>1)Top-down</p>
<p>2)Bottom-up</p>
<p><img src="/2018/05/07/dp1/wps58E0.jpg" alt="img"></p>
<p>1)Top-down</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 表格，储存全部问题的答案。</span></span><br><span class="line"><span class="keyword">bool</span> solve[<span class="number">6</span>];  <span class="comment">// 记录问题是否已经計算完毕</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> table[n] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 如果已經計算過，就直接讀取表格的答案。</span></span><br><span class="line">    <span class="keyword">if</span> (solve[n]) <span class="keyword">return</span> table[n];</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 如果不曾計算過，就計算一遍，儲存答案。</span></span><br><span class="line">    table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>); <span class="comment">// 將答案存入表格</span></span><br><span class="line">    solve[n] = <span class="literal">true</span>;            <span class="comment">// 已經計算完畢</span></span><br><span class="line">    <span class="keyword">return</span> table[n];</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        solve[i] = <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 合并solve跟table，简化代码。</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 用 0 代表该问题还未计算答案</span></span><br><span class="line"><span class="comment">//  if (table[n] != 0) </span></span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line"><span class="keyword">if</span> (table[n]) </span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line">    <span class="keyword">return</span> table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">     table[i] = <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​    这个实现方式的好处是不必斤斤计较计算顺序，因为代码中的递归结构会迫使最小的问题先被计算。这个实现方式的另一个好处是只计算必要的问题，而不必计算所有可能的问题。</p>
<p>​    这个实现的坏处是代码采用递归结构，不断调用函数，执行效率差。这个实现方式的另一个坏处是无法自由地控制计算顺序，因而无法妥善运用记忆体，浪费了可回收再利用的记忆体。</p>
<p>2）Bottom-up</p>
<p>​    指定一个计算顺序，然后由最小问题开始计算。特色是代码通常只有几个递归。这个实现方式的好处与坏处与前一个方式恰好互补。</p>
<p>​    首先建立表格。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">5</span> + <span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<p>心算爬到零阶的答案、爬到一阶的答案，填入报个当中，作为初始值。分别天道表格的第零格、第一格。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">table[0] = 1;</span><br><span class="line">table[1] = 1;</span><br></pre></td></tr></table></figure>
<p>​    尚待计算的部分就是爬到两阶的答案、……、爬到五阶的答案。通常是使用递归，按照计算顺序来计算。</p>
<p>计算过程的实现方式，有两种迥异的风格。一种是往回取值，是常见的实现方式。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">往回取值</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    table[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        table[i] = table[i<span class="number">-1</span>] + table[i<span class="number">-2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另一种是往后补值，是罕见的实现方式。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">往后补值<span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++) table[i] = <span class="number">0</span>;</span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//  table[1] = 1;   // 剛好可以被算到</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span> &lt;= <span class="number">5</span>) table[i+<span class="number">1</span>] += table[i];</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">2</span> &lt;= <span class="number">5</span>) table[i+<span class="number">2</span>] += table[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="7、总结"><a href="#7、总结" class="headerlink" title="7、总结"></a>7、总结</h3><p>​    第一，先找到原问题和其子问题之间的关系，写成递归公式。如此一来，即可利用递归公式，用子子问题的答案，求出子问题的答案；用子问题的答案，求出原问题的答案。</p>
<p>​    第二。确认可能出现的问题全部总共有哪些；用子问题的答案，求出原问题的答案。</p>
<p>​    第三。有了递归公式之后，就必须安排出一套计算顺序。大问题的答案，总是以小问题的答案来求得的，所以，小问题的答案必须是先算的，否则大问题的答案从何而来呢？一个好的安排方式，不但使代码易写，还可重复利用记忆体空间。</p>
<p>​    第四。记得先将最小，最先被计算的问题，心算出答案，储存如表格，内建与代码之中。一道递归公式必须拥有初始值，才有计算其他项。</p>
<p>​    第五。实现DP的代码时，会建立一个表格，表格存入所有大小问题的答案。安排好每个问题的答案再表格的哪个位置，这样计算时才能知道该在哪里取值。</p>
<p>​    切勿存取超出表格的原始，产生溢位情形，导致答案算错。计算过程当中，一旦某个问题的答案出错，就会如骨牌效应般一个影响一个，造成很难除错。</p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/04/08/tf8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/08/tf8/" itemprop="url">tf8</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-08T15:39:40+08:00">
                2018-04-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/04/08/tf8/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/04/08/tf8/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>​    某些任务需要能够更好的处理<strong>序列</strong>的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个<strong>序列</strong>。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络(Recurrent Neural Network)</strong>。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个<strong>循环神经网络</strong>。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>​    RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。那么，什么是语言模型呢？我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p>
<p>我昨天上学迟到了，老师批评了__。</p>
<p>​    我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p>
<p>​    <strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p>
<p>​    <strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p>
<p>​    使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p>
<p>我 昨天 上学 迟到 了 ，老师 批评 了 。</p>
<p>​    如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p>
<p>​    现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p>
<p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p>
<h2 id="循环神经网络是啥"><a href="#循环神经网络是啥" class="headerlink" title="循环神经网络是啥"></a>循环神经网络是啥</h2><p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p>
<h3 id="基本循环神经网络"><a href="#基本循环神经网络" class="headerlink" title="基本循环神经网络"></a>基本循环神经网络</h3><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：</p>
<p><img src="/2018/04/08/tf8/image1.jpg" alt="1"></p>
<p>​    纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为<strong>循环神经网络</strong>实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的<strong>全连接神经网络</strong>。x是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的<strong>权重矩阵</strong>（读者可以回到第三篇文章<a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a>，看看我们是怎样用矩阵来表示<strong>全连接神经网络</strong>的计算的）；o也是一个向量，它表示<strong>输出层</strong>的值；V是隐藏层到输出层的<strong>权重矩阵</strong>。那么，现在我们来看看W是什么。<strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。<strong>输出层是一个全连接层，它的每个节点都和隐藏层的每个节点相连，隐藏层是循环层。</strong></p>
<p>如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：</p>
<p><img src="/2018/04/08/tf8/image2.jpg" alt="2"></p>
<p>​    </p>
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：<br>$$<br>o_t=g(Vs_t)…..(式1)……(1)\\<br>s_t=f(Ux_t+Ws_{t-1})…..(式2)……(2)<br>$$<br>​    <strong>式1</strong>是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p>从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复把<strong>式2</strong>带入到<strong>式1</strong>，我们将得到：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t)     ……….(3)\\<br>&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(4)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))……….(5)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))……….(6)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+…))))……….(7)<br>\end{align}<br>$$<br>从上面可以看出，<strong>循环神经网络</strong>的输出值$o_t$，是受前面历次输入值$x_t、x_{t-1}、x_{t-2}、x_{t-3}$…影响的，这就是为什么<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p>
<h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>​    对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<p>我的手机坏了，我打算____一部新手机。</p>
<p>​    可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：</p>
<p><img src="/2018/04/08/tf8/image3.png" alt="3"></p>
<p>​    当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中$y_2$的计算。</p>
<p>​    从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_2$取决于$A_2$和$A’_2$。其计算方法为：<br>$$<br>\mathrm{y}_2=g(VA_2+V’A_2’)<br>$$<br>$A_2$和$A’_2$则分别计算：<br>$$<br>\begin{align}<br>A_2&amp;=f(WA_1+U\mathrm{x}_2)………(8)\\<br>A_2’&amp;=f(W’A_3’+U’\mathrm{x}_2)…….(9)\\<br>\end{align}<br>$$<br>​    现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_t-1$有关；反向计算时，隐藏层的值$s’_t$与$s’_{t-1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出双向循环神经网络的计算方法：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V’\mathrm{s}_t’)…….(10)\\<br>\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(11)\\<br>\mathrm{s}_t’&amp;=f(U’\mathrm{x}_t+W’\mathrm{s}_{t+1}’)……(12)\\<br>\end{align}<br>$$</p>
<p>​    从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<h3 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h3><p>前面我们介绍的<strong>循环神经网络</strong>只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p>
<p><img src="/2018/04/08/tf8/image4.png" alt="4"></p>
<p>我们把第i个隐藏层的值表示为$s_t^{(i)}$、$s_t^{‘(i)}$，则<strong>深度循环神经网络</strong>的计算方式可以表示为：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V’^{(i)}\mathrm{s}_t’^{(i)})…….(13)\\<br>\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})……….(14)\\<br>\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\mathrm{s}_t’^{(i-1)}+W’^{(i)}\mathrm{s}_{t+1}’)……..(15)\\<br>…(16)\\<br>\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})……..(17)\\<br>\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\mathrm{x}_t+W’^{(1)}\mathrm{s}_{t+1}’)………(18)\\<br>\end{align}<br>$$</p>
<h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<ol>
<li>前向计算每个神经元的输出值；</li>
<li>反向计算每个神经元的<strong>误差项</strong>$\delta_j$值，它是误差函数E对神经元j的<strong>加权输入</strong>$net_j$的偏导数；</li>
<li>计算每个权重的梯度。</li>
</ol>
<p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p>
<p>循环层如下图所示：</p>
<p><img src="/2018/04/08/tf8/image5.png" alt="5"></p>
<h4 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h4><p>使用前面的<strong>式2</strong>对循环层进行前向计算：<br>$$<br>\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})<br>$$<br>注意，上面的$s_t$、$x_t$、$s_{t-1}$都是向量，用<strong>黑体字母</strong>表示；而U、V是<strong>矩阵</strong>，用大写字母表示。<strong>向量的下标</strong>表示<strong>时刻</strong>，例如，表示在t时刻向量s的值。</p>
<p>​    我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n<em> m$，矩阵W的维度是$n</em>n$。下面是上式展开成矩阵的样子，看起来更直观一些：<br>$$<br>\begin{align}<br>\begin{bmatrix}<br>s_1^t\\<br>s_2^t\\<br>.\.\\<br>s_n^t\\<br>\end{bmatrix}=f(<br>\begin{bmatrix}<br>u_{11} u_{12} … u_{1m}\\<br>u_{21} u_{22} … u_{2m}\\<br>.\.\\<br>u_{n1} u_{n2} … u_{nm}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>.\.\\<br>x_m\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>w_{11} w_{12} … w_{1n}\\<br>w_{21} w_{22} … w_{2n}\\<br>.\.\\<br>w_{n1} w_{n2} … w_{nn}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>s_1^{t-1}\\<br>s_2^{t-1}\\<br>.\.\\<br>s_n^{t-1}\\<br>\end{bmatrix})…..(19)<br>\end{align}<br>$$<br>​    在这里我们用<strong>手写体字母</strong>表示向量的一个<strong>元素</strong>，它的下标表示它是这个向量的第几个元素，它的上标表示第几个<strong>时刻</strong>。例如，$s_t^{j}$表示向量s的第j个元素在t时刻的值。$u_{ji}$表示<strong>输入层</strong>第i个神经元到<strong>循环层</strong>第j个神经元的权重。$w_{ji}$表示<strong>循环层</strong>第t-1时刻的第i个神经元到<strong>循环层</strong>第t个时刻的第j个神经元的权重。</p>
<h4 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h4><p>​    BTPP算法将第l层t时刻的<strong>误差项</strong>$\delta_t^l $值沿两个方向传播，一个方向是其传递到上一层网络，得到，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始时刻，得到，这部分只和权重矩阵W有关。</p>
<p>我们用向量表示神经元在t时刻的<strong>加权输入</strong>，因为：</p>
<p><img src="/2018/04/08/tf8/image6.png" alt="6"></p>
<p>其中，$X_t$为输入，A为模型处理部分，$h_t$为输出。</p>
<p>为了更容易地说明递归神经网络，我们把上图展开，得到： </p>
<p><img src="/2018/04/08/tf8/image7.png" alt="7"></p>
<p>​    这样的一条链状神经网络代表了一个递归神经网络，可以认为它是对相同神经网络的多重复制，每一时刻的神经网络会传递信息给下一时刻。如何理解它呢？假设有这样一个语言模型，我们要根据句子中已出现的词预测当前词是什么，递归神经网络的工作原理如下： </p>
<p><img src="/2018/04/08/tf8/image8.png" alt="8"></p>
<p>其中，W为各类权重，x表示输入，y表示输出，h表示隐层处理状态。</p>
<p>递归神经网络因为具有一定的记忆功能，可以被用来解决很多问题，例如：语音识别、语言模型、机器翻译等。但是它并不能很好地处理长时依赖问题。</p>
<p><strong>长时依赖问题</strong> </p>
<p>​    RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p>
<p>​    有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p>
<p><img src="/2018/04/08/tf8/image9.png" alt="9"></p>
<p>​                        不太长的相关信息和位置间隔</p>
<p>​    但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。</p>
<p>​    不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p>
<p><img src="/2018/04/08/tf8/image10.png" alt="10"></p>
<p>​    在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="https://link.jianshu.com?t=http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。</p>
<p>​    隐含内容和输出结果是相同的内容。</p>
<p>​    TensorFlow 实现 RNN Cell 的位置在 python/ops/rnn_cell_impl.py，首先其实现了一个 RNNCell 类，继承了 Layer 类，其内部有三个比较重要的方法，state_size()、output_size()、<strong>call</strong>() 方法，其中 state_size() 和 output_size() 方法设置为类属性，可以当做属性来调用，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""size(s) of state(s) used by this cell.</span></span><br><span class="line"><span class="string">It can be represented by an Integer, a TensorShape or a tuple of Integers</span></span><br><span class="line"><span class="string">or TensorShapes.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""Integer or TensorShape: size of outputs produced by this cell."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br></pre></td></tr></table></figure>
<p>分别代表 Cell 的状态和输出维度，和 Cell 中的神经元数量有关，但这里两个方法都没有实现，意思是说我们必须要实现一个子类继承 RNNCell 类并实现这两个方法。</p>
<p>另外对于 <strong>call</strong>() 方法，实际上就是当初始化的对象直接被调用的时候触发的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(scope,</span><br><span class="line">                               custom_getter=self._rnn_get_variable) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state, scope=scope)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(vs.get_variable_scope(),</span><br><span class="line">                               custom_getter=self._rnn_get_variable):</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state)</span><br></pre></td></tr></table></figure>
<p>​    实际上是调用了父类 Layer 的 <strong>call</strong>() 方法，但父类中 <strong>call</strong>() 方法中又调用了 call() 方法，而 Layer 类的 call() 方法的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure>
<p>父类的 call() 方法实现非常简单，所以要实现其真正的功能，只需要在继承 RNNCell 类的子类中实现 call() 方法即可。</p>
<p>接下来我们看下 RNN Cell 的最基本的实现，叫做 BasicRNNCell，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicRNNCell</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">  <span class="string">"""The most basic RNN cell.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the RNN cell.</span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, activation=None, reuse=None)</span>:</span></span><br><span class="line">    super(BasicRNNCell, self).__init__(_reuse=reuse)</span><br><span class="line">    self._num_units = num_units</span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line">    self._linear = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">    <span class="string">"""Most basic RNN: output = new_state = act(W * input + U * state + B)."""</span></span><br><span class="line">    <span class="keyword">if</span> self._linear <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      self._linear = _Linear([inputs, state], self._num_units, <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    output = self._activation(self._linear([inputs, state]))</span><br><span class="line">    <span class="keyword">return</span> output, output</span><br></pre></td></tr></table></figure>
<p>可以看到在初始化的时候，最重要的一个参数是 num_units，意思就是这个 Cell 中神经元的个数，另外还有一个参数 activation 即默认使用的激活函数，默认使用的 tanh，reuse 代表该 Cell 是否可以被重新使用。</p>
<p>在 state_size()、output_size() 方法里，其返回的内容都是 num_units，即神经元的个数，接下来 call() 方法中，传入的参数为 inputs 和 state，即输入的 x 和 上一次的隐含状态，首先实例化了一个 _Linear 类，这个类实际上就是做线性变换的类，将二者传递过来，然后直接调用，就实现了 w * [inputs, state] + b 的线性变换，其中 _Linear 类的 <strong>call</strong>() 方法实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, args)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._is_sequence:</span><br><span class="line">        args = [args]</span><br><span class="line">    <span class="keyword">if</span> len(args) == <span class="number">1</span>:</span><br><span class="line">        res = math_ops.matmul(args[<span class="number">0</span>], self._weights)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res = math_ops.matmul(array_ops.concat(args, <span class="number">1</span>), self._weights)</span><br><span class="line">    <span class="keyword">if</span> self._build_bias:</span><br><span class="line">        res = nn_ops.bias_add(res, self._biases)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>​    很明显这里传递了 [inputs, state] 作为 <strong>call</strong>() 方法的 args，会执行 concat() 和 matmul() 方法，然后接着再执行 bias_add() 方法，这样就实现了线性变换。</p>
<p>​    最后回到 BasicRNNCell 的 call() 方法中，在 _linear() 方法外面又包括了一层 _activation() 方法，即对线性变换应用一次 tanh 激活函数处理，作为输出结果。</p>
<p>​    最后返回的结果是 output 和 output，第一个代表 output，第二个代表隐状态，其值也等于 output。</p>

          
        
      
    </div>
    
    
    

    

	<div>
      
	</div>
	
	
	
	
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4"
                alt="goingcoder" />
            
              <p class="site-author-name" itemprop="name">goingcoder</p>
              <p class="site-description motion-element" itemprop="description">匆忙世间的闲人。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/goingcoder" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
              </a>
            </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.hustyrf.top" title="陈冠希" target="_blank">陈冠希</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">goingcoder</span>

  
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>

-->



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"your-duoshuo-shortname"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


















  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
