<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="离弦的博客" type="application/atom+xml" />






<meta name="description" content="循环神经网络​    某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Netwo">
<meta name="keywords" content="Hexo,next">
<meta property="og:type" content="article">
<meta property="og:title" content="tf8">
<meta property="og:url" content="https://goingcoder.github.io/2018/04/08/tf8/index.html">
<meta property="og:site_name" content="离弦的博客">
<meta property="og:description" content="循环神经网络​    某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Netwo">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image1.jpg">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image2.jpg">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image3.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image4.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image5.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image6.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image7.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image8.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image9.png">
<meta property="og:image" content="https://goingcoder.github.io/2018/04/08/tf8/image10.png">
<meta property="og:updated_time" content="2018-04-09T07:29:54.381Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tf8">
<meta name="twitter:description" content="循环神经网络​    某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Netwo">
<meta name="twitter:image" content="https://goingcoder.github.io/2018/04/08/tf8/image1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://goingcoder.github.io/2018/04/08/tf8/"/>





  <title>tf8 | 离弦的博客</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?your-analytics-id";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband">
		
	</div>
	
	
	
	<a href="https://github.com/goingcoder" class="github-corner" aria-label="View source on Github">
		<svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
			<path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
	
	<header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">离弦的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">我若为王,舍我其谁</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'your-swiftype-key','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://goingcoder.github.io/2018/04/08/tf8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="goingcoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="离弦的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">tf8</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-08T15:39:40+08:00">
                2018-04-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/04/08/tf8/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/04/08/tf8/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>​    某些任务需要能够更好的处理<strong>序列</strong>的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个<strong>序列</strong>。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络(Recurrent Neural Network)</strong>。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个<strong>循环神经网络</strong>。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>​    RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。那么，什么是语言模型呢？我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p>
<p>我昨天上学迟到了，老师批评了__。</p>
<p>​    我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p>
<p>​    <strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p>
<p>​    <strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p>
<p>​    使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p>
<p>我 昨天 上学 迟到 了 ，老师 批评 了 。</p>
<p>​    如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p>
<p>​    现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p>
<p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p>
<h2 id="循环神经网络是啥"><a href="#循环神经网络是啥" class="headerlink" title="循环神经网络是啥"></a>循环神经网络是啥</h2><p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p>
<h3 id="基本循环神经网络"><a href="#基本循环神经网络" class="headerlink" title="基本循环神经网络"></a>基本循环神经网络</h3><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：</p>
<p><img src="/2018/04/08/tf8/image1.jpg" alt="1"></p>
<p>​    纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为<strong>循环神经网络</strong>实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的<strong>全连接神经网络</strong>。x是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的<strong>权重矩阵</strong>（读者可以回到第三篇文章<a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a>，看看我们是怎样用矩阵来表示<strong>全连接神经网络</strong>的计算的）；o也是一个向量，它表示<strong>输出层</strong>的值；V是隐藏层到输出层的<strong>权重矩阵</strong>。那么，现在我们来看看W是什么。<strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。<strong>输出层是一个全连接层，它的每个节点都和隐藏层的每个节点相连，隐藏层是循环层。</strong></p>
<p>如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：</p>
<p><img src="/2018/04/08/tf8/image2.jpg" alt="2"></p>
<p>​    </p>
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：<br>$$<br>o_t=g(Vs_t)…..(式1)……(1)\\<br>s_t=f(Ux_t+Ws_{t-1})…..(式2)……(2)<br>$$<br>​    <strong>式1</strong>是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p>从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复把<strong>式2</strong>带入到<strong>式1</strong>，我们将得到：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t)     ……….(3)\\<br>&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(4)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))……….(5)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))……….(6)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+…))))……….(7)<br>\end{align}<br>$$<br>从上面可以看出，<strong>循环神经网络</strong>的输出值$o_t$，是受前面历次输入值$x_t、x_{t-1}、x_{t-2}、x_{t-3}$…影响的，这就是为什么<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p>
<h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>​    对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<p>我的手机坏了，我打算____一部新手机。</p>
<p>​    可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：</p>
<p><img src="/2018/04/08/tf8/image3.png" alt="3"></p>
<p>​    当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中$y_2$的计算。</p>
<p>​    从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_2$取决于$A_2$和$A’_2$。其计算方法为：<br>$$<br>\mathrm{y}_2=g(VA_2+V’A_2’)<br>$$<br>$A_2$和$A’_2$则分别计算：<br>$$<br>\begin{align}<br>A_2&amp;=f(WA_1+U\mathrm{x}_2)………(8)\\<br>A_2’&amp;=f(W’A_3’+U’\mathrm{x}_2)…….(9)\\<br>\end{align}<br>$$<br>​    现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_t-1$有关；反向计算时，隐藏层的值$s’_t$与$s’_{t-1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出双向循环神经网络的计算方法：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V’\mathrm{s}_t’)…….(10)\\<br>\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(11)\\<br>\mathrm{s}_t’&amp;=f(U’\mathrm{x}_t+W’\mathrm{s}_{t+1}’)……(12)\\<br>\end{align}<br>$$</p>
<p>​    从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<h3 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h3><p>前面我们介绍的<strong>循环神经网络</strong>只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p>
<p><img src="/2018/04/08/tf8/image4.png" alt="4"></p>
<p>我们把第i个隐藏层的值表示为$s_t^{(i)}$、$s_t^{‘(i)}$，则<strong>深度循环神经网络</strong>的计算方式可以表示为：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V’^{(i)}\mathrm{s}_t’^{(i)})…….(13)\\<br>\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})……….(14)\\<br>\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\mathrm{s}_t’^{(i-1)}+W’^{(i)}\mathrm{s}_{t+1}’)……..(15)\\<br>…(16)\\<br>\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})……..(17)\\<br>\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\mathrm{x}_t+W’^{(1)}\mathrm{s}_{t+1}’)………(18)\\<br>\end{align}<br>$$</p>
<h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<ol>
<li>前向计算每个神经元的输出值；</li>
<li>反向计算每个神经元的<strong>误差项</strong>$\delta_j$值，它是误差函数E对神经元j的<strong>加权输入</strong>$net_j$的偏导数；</li>
<li>计算每个权重的梯度。</li>
</ol>
<p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p>
<p>循环层如下图所示：</p>
<p><img src="/2018/04/08/tf8/image5.png" alt="5"></p>
<h4 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h4><p>使用前面的<strong>式2</strong>对循环层进行前向计算：<br>$$<br>\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})<br>$$<br>注意，上面的$s_t$、$x_t$、$s_{t-1}$都是向量，用<strong>黑体字母</strong>表示；而U、V是<strong>矩阵</strong>，用大写字母表示。<strong>向量的下标</strong>表示<strong>时刻</strong>，例如，表示在t时刻向量s的值。</p>
<p>​    我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n<em> m$，矩阵W的维度是$n</em>n$。下面是上式展开成矩阵的样子，看起来更直观一些：<br>$$<br>\begin{align}<br>\begin{bmatrix}<br>s_1^t\\<br>s_2^t\\<br>.\.\\<br>s_n^t\\<br>\end{bmatrix}=f(<br>\begin{bmatrix}<br>u_{11} u_{12} … u_{1m}\\<br>u_{21} u_{22} … u_{2m}\\<br>.\.\\<br>u_{n1} u_{n2} … u_{nm}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>.\.\\<br>x_m\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>w_{11} w_{12} … w_{1n}\\<br>w_{21} w_{22} … w_{2n}\\<br>.\.\\<br>w_{n1} w_{n2} … w_{nn}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>s_1^{t-1}\\<br>s_2^{t-1}\\<br>.\.\\<br>s_n^{t-1}\\<br>\end{bmatrix})…..(19)<br>\end{align}<br>$$<br>​    在这里我们用<strong>手写体字母</strong>表示向量的一个<strong>元素</strong>，它的下标表示它是这个向量的第几个元素，它的上标表示第几个<strong>时刻</strong>。例如，$s_t^{j}$表示向量s的第j个元素在t时刻的值。$u_{ji}$表示<strong>输入层</strong>第i个神经元到<strong>循环层</strong>第j个神经元的权重。$w_{ji}$表示<strong>循环层</strong>第t-1时刻的第i个神经元到<strong>循环层</strong>第t个时刻的第j个神经元的权重。</p>
<h4 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h4><p>​    BTPP算法将第l层t时刻的<strong>误差项</strong>$\delta_t^l $值沿两个方向传播，一个方向是其传递到上一层网络，得到，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始时刻，得到，这部分只和权重矩阵W有关。</p>
<p>我们用向量表示神经元在t时刻的<strong>加权输入</strong>，因为：</p>
<p><img src="/2018/04/08/tf8/image6.png" alt="6"></p>
<p>其中，$X_t$为输入，A为模型处理部分，$h_t$为输出。</p>
<p>为了更容易地说明递归神经网络，我们把上图展开，得到： </p>
<p><img src="/2018/04/08/tf8/image7.png" alt="7"></p>
<p>​    这样的一条链状神经网络代表了一个递归神经网络，可以认为它是对相同神经网络的多重复制，每一时刻的神经网络会传递信息给下一时刻。如何理解它呢？假设有这样一个语言模型，我们要根据句子中已出现的词预测当前词是什么，递归神经网络的工作原理如下： </p>
<p><img src="/2018/04/08/tf8/image8.png" alt="8"></p>
<p>其中，W为各类权重，x表示输入，y表示输出，h表示隐层处理状态。</p>
<p>递归神经网络因为具有一定的记忆功能，可以被用来解决很多问题，例如：语音识别、语言模型、机器翻译等。但是它并不能很好地处理长时依赖问题。</p>
<p><strong>长时依赖问题</strong> </p>
<p>​    RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p>
<p>​    有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p>
<p><img src="/2018/04/08/tf8/image9.png" alt="9"></p>
<p>​                        不太长的相关信息和位置间隔</p>
<p>​    但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。</p>
<p>​    不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p>
<p><img src="/2018/04/08/tf8/image10.png" alt="10"></p>
<p>​    在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="https://link.jianshu.com?t=http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。</p>
<p>​    隐含内容和输出结果是相同的内容。</p>
<p>​    TensorFlow 实现 RNN Cell 的位置在 python/ops/rnn_cell_impl.py，首先其实现了一个 RNNCell 类，继承了 Layer 类，其内部有三个比较重要的方法，state_size()、output_size()、<strong>call</strong>() 方法，其中 state_size() 和 output_size() 方法设置为类属性，可以当做属性来调用，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""size(s) of state(s) used by this cell.</span></span><br><span class="line"><span class="string">It can be represented by an Integer, a TensorShape or a tuple of Integers</span></span><br><span class="line"><span class="string">or TensorShapes.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""Integer or TensorShape: size of outputs produced by this cell."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br></pre></td></tr></table></figure>
<p>分别代表 Cell 的状态和输出维度，和 Cell 中的神经元数量有关，但这里两个方法都没有实现，意思是说我们必须要实现一个子类继承 RNNCell 类并实现这两个方法。</p>
<p>另外对于 <strong>call</strong>() 方法，实际上就是当初始化的对象直接被调用的时候触发的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(scope,</span><br><span class="line">                               custom_getter=self._rnn_get_variable) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state, scope=scope)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(vs.get_variable_scope(),</span><br><span class="line">                               custom_getter=self._rnn_get_variable):</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state)</span><br></pre></td></tr></table></figure>
<p>​    实际上是调用了父类 Layer 的 <strong>call</strong>() 方法，但父类中 <strong>call</strong>() 方法中又调用了 call() 方法，而 Layer 类的 call() 方法的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure>
<p>父类的 call() 方法实现非常简单，所以要实现其真正的功能，只需要在继承 RNNCell 类的子类中实现 call() 方法即可。</p>
<p>接下来我们看下 RNN Cell 的最基本的实现，叫做 BasicRNNCell，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicRNNCell</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">  <span class="string">"""The most basic RNN cell.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the RNN cell.</span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, activation=None, reuse=None)</span>:</span></span><br><span class="line">    super(BasicRNNCell, self).__init__(_reuse=reuse)</span><br><span class="line">    self._num_units = num_units</span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line">    self._linear = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">    <span class="string">"""Most basic RNN: output = new_state = act(W * input + U * state + B)."""</span></span><br><span class="line">    <span class="keyword">if</span> self._linear <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      self._linear = _Linear([inputs, state], self._num_units, <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    output = self._activation(self._linear([inputs, state]))</span><br><span class="line">    <span class="keyword">return</span> output, output</span><br></pre></td></tr></table></figure>
<p>可以看到在初始化的时候，最重要的一个参数是 num_units，意思就是这个 Cell 中神经元的个数，另外还有一个参数 activation 即默认使用的激活函数，默认使用的 tanh，reuse 代表该 Cell 是否可以被重新使用。</p>
<p>在 state_size()、output_size() 方法里，其返回的内容都是 num_units，即神经元的个数，接下来 call() 方法中，传入的参数为 inputs 和 state，即输入的 x 和 上一次的隐含状态，首先实例化了一个 _Linear 类，这个类实际上就是做线性变换的类，将二者传递过来，然后直接调用，就实现了 w * [inputs, state] + b 的线性变换，其中 _Linear 类的 <strong>call</strong>() 方法实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, args)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._is_sequence:</span><br><span class="line">        args = [args]</span><br><span class="line">    <span class="keyword">if</span> len(args) == <span class="number">1</span>:</span><br><span class="line">        res = math_ops.matmul(args[<span class="number">0</span>], self._weights)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res = math_ops.matmul(array_ops.concat(args, <span class="number">1</span>), self._weights)</span><br><span class="line">    <span class="keyword">if</span> self._build_bias:</span><br><span class="line">        res = nn_ops.bias_add(res, self._biases)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>​    很明显这里传递了 [inputs, state] 作为 <strong>call</strong>() 方法的 args，会执行 concat() 和 matmul() 方法，然后接着再执行 bias_add() 方法，这样就实现了线性变换。</p>
<p>​    最后回到 BasicRNNCell 的 call() 方法中，在 _linear() 方法外面又包括了一层 _activation() 方法，即对线性变换应用一次 tanh 激活函数处理，作为输出结果。</p>
<p>​    最后返回的结果是 output 和 output，第一个代表 output，第二个代表隐状态，其值也等于 output。</p>

      
    </div>
    
    
    

    

	<div>
      
        
      
	</div>
	
	
	<div>
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	</div>
	
	
	
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/07/tf7/" rel="next" title="tf7">
                <i class="fa fa-chevron-left"></i> tf7
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/07/dp1/" rel="prev" title="动态规划">
                动态规划 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div class="ds-thread" data-thread-key="2018/04/08/tf8/"
           data-title="tf8" data-url="https://goingcoder.github.io/2018/04/08/tf8/">
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars1.githubusercontent.com/u/23649220?s=400&u=964c794854ddfd2a46f55952deb5f1010bbd2982&v=4"
                alt="goingcoder" />
            
              <p class="site-author-name" itemprop="name">goingcoder</p>
              <p class="site-description motion-element" itemprop="description">匆忙世间的闲人。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">38</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/goingcoder" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            
          </div>

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
              </a>
            </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.hustyrf.top" title="陈冠希" target="_blank">陈冠希</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#循环神经网络"><span class="nav-number">1.</span> <span class="nav-text"><a href="#&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;" class="headerlink" title="&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;"></a>&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#语言模型"><span class="nav-number">1.1.</span> <span class="nav-text"><a href="#&#x8BED;&#x8A00;&#x6A21;&#x578B;" class="headerlink" title="&#x8BED;&#x8A00;&#x6A21;&#x578B;"></a>&#x8BED;&#x8A00;&#x6A21;&#x578B;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络是啥"><span class="nav-number">1.2.</span> <span class="nav-text"><a href="#&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;" class="headerlink" title="&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;"></a>&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本循环神经网络"><span class="nav-number">1.2.1.</span> <span class="nav-text"><a href="#&#x57FA;&#x672C;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;" class="headerlink" title="&#x57FA;&#x672C;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;"></a>&#x57FA;&#x672C;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#双向循环神经网络"><span class="nav-number">1.2.2.</span> <span class="nav-text"><a href="#&#x53CC;&#x5411;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;" class="headerlink" title="&#x53CC;&#x5411;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;"></a>&#x53CC;&#x5411;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深度循环神经网络"><span class="nav-number">1.2.3.</span> <span class="nav-text"><a href="#&#x6DF1;&#x5EA6;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;" class="headerlink" title="&#x6DF1;&#x5EA6;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;"></a>&#x6DF1;&#x5EA6;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络的训练"><span class="nav-number">1.3.</span> <span class="nav-text"><a href="#&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;" class="headerlink" title="&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;"></a>&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#循环神经网络的训练算法：BPTT"><span class="nav-number">1.3.1.</span> <span class="nav-text"><a href="#&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;&#x7B97;&#x6CD5;&#xFF1A;BPTT" class="headerlink" title="&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;&#x7B97;&#x6CD5;&#xFF1A;BPTT"></a>&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;&#x7B97;&#x6CD5;&#xFF1A;BPTT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向计算"><span class="nav-number">1.3.1.1.</span> <span class="nav-text"><a href="#&#x524D;&#x5411;&#x8BA1;&#x7B97;" class="headerlink" title="&#x524D;&#x5411;&#x8BA1;&#x7B97;"></a>&#x524D;&#x5411;&#x8BA1;&#x7B97;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#误差项的计算"><span class="nav-number">1.3.1.2.</span> <span class="nav-text"><a href="#&#x8BEF;&#x5DEE;&#x9879;&#x7684;&#x8BA1;&#x7B97;" class="headerlink" title="&#x8BEF;&#x5DEE;&#x9879;&#x7684;&#x8BA1;&#x7B97;"></a>&#x8BEF;&#x5DEE;&#x9879;&#x7684;&#x8BA1;&#x7B97;</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">goingcoder</span>

  
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>

-->



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"your-duoshuo-shortname"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  


















  





  

  

  

  
  

  
  


  

  

</body>
</html>
