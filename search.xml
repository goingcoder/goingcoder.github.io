<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Semi-supervised sequence tagging with bidirectional language models]]></title>
      <url>/2018/07/06/ner4/</url>
      <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    对于NLP任务，从无标签文本中学习预训练的embedding已经是神经网络的一个标准部分。在这篇论文中，提出了将一个双向的语言模型的context embedding引入序列标注任务中，在NER和chunking两个标准数据集上进行了实验，证明了与其他的某些使用迁移学习和使用附加标注数据和特定任务字典的联合学习相比，效果都要好，超过了以往所有的模型。</p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul>
<li><p>主要贡献是表明在LM embedding中捕获的上下文敏感表示在监督序列标记设置中是有用的。</p>
</li>
<li><p>双向的LM embedding比前向的LM embedding效果要好。</p>
<a id="more"></a>
</li>
</ul>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>​    一个语言模型是计算一个序列$(t_1,t_2,…,t_N)$的概率<br>$$<br>p(t_1,t_2,\dots,t_N)=\prod_{k=1}^Np(t_k|t_1,t_2,\dots,t_k-1)<br>$$</p>
<h3 id="TagLM-language-model-augmented-sequence-tagger"><a href="#TagLM-language-model-augmented-sequence-tagger" class="headerlink" title="TagLM(language-model-augmented sequence tagger)"></a>TagLM(language-model-augmented sequence tagger)</h3><p><img src="/2018/07/06/ner4/p1.PNG" alt="1"></p>
<p>​    图的左边是标准的LSTM-CRF模型，这里LSTM使用了两层。图的右边是语言模型。语言模型的加入是在第一层双向LSTM输出之后进行拼接的，因为这个位置效果最好。作者做了三个尝试，将LM embeddings分别拼接在第一层RNN的输入、第一层RNN的输出、第二层RNN的输出。实验中，第二种方案的表现最好。 正是上图展示的那样。</p>
<h3 id="未实验的其他LM-embedding加入方式"><a href="#未实验的其他LM-embedding加入方式" class="headerlink" title="未实验的其他LM embedding加入方式"></a>未实验的其他LM embedding加入方式</h3><ul>
<li>在第一层RNN拼接之后加入一个非线性映射$ f([ h_{k,1};h^{LM}_k])$</li>
<li>对 LM embedding加入attention机制</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>​    在CoNLL2003 NER任务和CoNLL Chunking 任务上进行了实验。</p>
<p><img src="/2018/07/06/ner4/p2.PNG" alt="2"></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>###1.How to use LM embedding?</p>
<p>作者在不同的位置上对LM embedding进行拼接</p>
<ul>
<li><p>在第一层RNN的输入时，<br>  $$</p>
<p>  $$</p>
<p>  $$<br>  x_k=[c_k;,w_k;h^{LM}_k]<br>  $$</p>
</li>
<li><p>在第一层RNN输出时<br>  $$<br>  h_{k,1}=[h_{k,1};h^{LM}_k]<br>  $$</p>
</li>
<li><p>在第二层RNN输出时<br>  $$<br>  h_{k,2}=[h_{k,2};h^{LM}_k]<br>  $$<br>  实验结果</p>
<p>  <img src="/2018/07/06/ner4/p3.PNG" alt="3"></p>
<p>  <strong>结果表明，在第一层RNN输出时拼接效果做好</strong></p>
<h3 id="2-Does-it-matter-which-language-model-to-use"><a href="#2-Does-it-matter-which-language-model-to-use" class="headerlink" title="2.Does it matter which language model to use?"></a>2.Does it matter which language model to use?</h3><p>  不同语言模型的影响。</p>
<p>  <img src="/2018/07/06/ner4/p4.PNG" alt="4"></p>
</li>
<li><p>语言模型的大小是重要的。</p>
</li>
<li><p>双向语言模型比前向语言模型要好</p>
</li>
</ul>
<p>###3.Importance of task specific RNN</p>
<p>​    不使用RNN只使用全连接层，$F_1$为88.71，低于baseline。</p>
<h3 id="4-Dataset-size"><a href="#4-Dataset-size" class="headerlink" title="4.Dataset size"></a>4.Dataset size</h3><p>作者的方法对训练集大小的依赖很小，在使用更大的训练集时，语言模型会有一个明显的提升。</p>
<h3 id="5-Number-of-parameters"><a href="#5-Number-of-parameters" class="headerlink" title="5.Number of parameters"></a>5.Number of parameters</h3><p>没大看懂对比</p>
<h3 id="6-Does-the-LM-transfer-across-domains"><a href="#6-Does-the-LM-transfer-across-domains" class="headerlink" title="6.Does the LM transfer across domains?"></a>6.Does the LM transfer across domains?</h3><p>​    这篇论文证明语言模型是可以跨领域的。将新闻领域训练出来的语言模型，用到科学论文的任务上，也有效果也有提升。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>LM embedding 与word embedding类似语义信息，LM embedding感觉更像序列级别的，在加入LM embedding 到RNN中时，在与第一层RNN拼接时效果也是最好的。word embedding也是包含上下文信息的，感觉两个embedding包含的信息有部分的重叠，虽然有一个点的提升。</li>
<li>对人们比较关心的几个问题都做了附加的分析，让人心服口服，值得学习。</li>
<li>即使只是对基础模型的一个细小改动，增加一个类型信息（本文是语言模型的上下文信息），如果把实验做的充分，结果显著，也是可以发出来文章的。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. <em>ACL</em>. <a href="http://doi.org/10.18653/v1/P17-1194" target="_blank" rel="noopener">http://doi.org/10.18653/v1/P17-1194</a> </p>
<p>[2] <a href="http://oyeblog.com/2017/paper_0_LM/" target="_blank" rel="noopener">http://oyeblog.com/2017/paper_0_LM/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'Semi-supervised sequence tagging with bidirectional language models',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>



]]></content>
      
        <categories>
            
            <category> ner </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ner </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CNN卷积神经网络对MNIST数据分类]]></title>
      <url>/2018/05/17/tf10/</url>
      <content type="html"><![CDATA[<p>​    Convolutional Neural Networks (CNN) 是神经网络处理图片信息的一大利器. 因为利用卷积神经网络在图像和语音识别方面能够给出更优预测结果, 这一种技术也被广泛的传播可应用. 卷积神经网络最常被应用的方面是计算机的图像识别, 不过因为不断地创新, 它也被应用在视频分析, 自然语言处理, 药物发现, 等等. 近期最火的 Alpha Go, 让计算机看懂围棋, 同样也是有运用到这门技术。</p>
<p>​    这里将建立一个卷积神经网络，它可以把MNIST手写字符的识别准确率提高到99%，读者可能需要一些卷积神经网络的基础知识才能更好地理解本节的内容。</p>
<p>​    程序的开头依旧是导入Tensorflow：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br></pre></td></tr></table></figure>
<p>​    接下来载入MNIST数据集，并建立占位符。占位符x的含义为训练图像，y_为对应训练图像的标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>​    由于要使用的是卷积神经网络对图像进行分类，所以不能再使用784维的向量表示输入的x，而是将其还原为28*28的图片形式。[-1,28,28,1]中的-1表示形状第一维的大小是根据x自动确定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>​    x_image就是输入的训练图像，接下来。我们堆训练图像进行卷积操作，第一层卷积的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"><span class="comment"># 第一层卷积层</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br></pre></td></tr></table></figure>
<p>​    先定义四个函数，函数weight_variable可以返回一个给定形状的变量并自动以截断正态分布初始化，bias_variable同样返回一个给定形状的变量，初始化时所有值是0.1，可分别用这两个函数创建卷积的核与偏置。h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)是真正进行卷积运算，卷积计算后选用ReLU作为激活函数。h_pool1 = max_pool_2x2(h_conv1)是调用函数max_pool_2x2(h_conv1)进行一次池化操作。卷积、激活函数、池化，可以说是一个卷积层的“标配”,通常一个卷积层都会包含这三个步骤，有时会去掉最后的池化操作。</p>
<p>​    tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。 </p>
<p>​    对第一个卷积操作后产生的h_pool1再做一次卷积计算，使用的代码与上面类似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<p>两层卷积之后是全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">   W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">   b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">   h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">   h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">   <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">   keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">   h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<p>​    在全连接中加入了Dropout，它是防止神经网络过拟合的一种手段。在每一步训练时，以一定的概率“去掉”网络中的某些连接，但这种去除不是永久性的，只是在当前步骤中去除，并且每一步去除的连接都是随机选择的。在这个程序中，选择的Dropout概率是0.5,也就是说训练每一个连接都有50%的概率被去除。在测试时保留所有连接。</p>
<p>​    最后，在加入一层全连接层，把上一步得到的h_fc1_drop转换为10个类别的打分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">   W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">   b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">   y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure>
<p>​    y_conv相当于Softmax模型中的Logit，当然可以使用Softmax函数将其转换为10个类别的概率，在定义交叉熵损失。但其实Tensorflow提供了一个更直接的cross_entropy = tf.reduce_mean(    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))函数，它可以直接对Logit定义交叉熵损失，写法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line"><span class="comment"># 同样定义train_step</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>定义测试的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义测试的准确率</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>​    在验证集上计算模型准确率并输出，方便监控训练的进度，也可以据此调整模型的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>​    训练结束后，打印在全体测试集上的准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>​    得到的准确率结果应该在99%左右。与Softmax回归模型相比，使用两层卷积神经网络模型借助了卷积的威力，准确率非常大的提升。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># print(x_image.shape)</span></span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 第一层卷积层</span></span><br><span class="line">    W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line"></span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">    print(h_pool1.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">    print(h_pool2.shape)</span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">    W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">    <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">    W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">    <span class="comment"># cross_entropy = tf.reduce_mean(</span></span><br><span class="line">    <span class="comment">#     tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同样定义train_step</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试的准确率</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">    print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/v_july_v/article/details/51812459" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/51812459</a></p>
<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p>
<p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[tensorflow入门笔记1]]></title>
      <url>/2018/05/14/tf9/</url>
      <content type="html"><![CDATA[<h2 id="1、MNIST数据集介绍"><a href="#1、MNIST数据集介绍" class="headerlink" title="1、MNIST数据集介绍"></a>1、MNIST数据集介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍MNIST数据集。如果所示，MNIST数据集主要由一些手写数字的图片和相应的标签组成，图片一共有10类，分别对应0~9，共10个阿拉伯数字。如下图：</p>
<p><img src="/2018/05/14/tf9/image1.png" alt="1"></p>
<p>MNIST 数据集可在 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> 获取, 它包含了四个部分:</p>
<ul>
<li><strong>Training set images</strong>: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</li>
<li><strong>Training set labels</strong>: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</li>
<li><strong>Test set images</strong>: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</li>
<li><strong>Test set labels</strong>: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MNIST 数据集来自美国国家标准与技术研究所, <strong>National Institute of Standards and Technology (NIST)</strong>. 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据。 60,000 个训练样本和 10,000 个测试样本. </p>
<a id="more"></a>
<h2 id="2、下载数据集"><a href="#2、下载数据集" class="headerlink" title="2、下载数据集"></a>2、下载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从tensorflow.examples.tutorials.mnist引入模块。这是TensorFlow为了教学MNIST而提前编制的程序</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># 从MNIST_data/中读取MNIST数据。这条语句在数据不存在时，会自动执行下载</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在执行语句mnist = input_data.read_data_sets(“MNIST_data/“, one_hot=True)时，Tensorflow会检测数据是否存在。当数据不存在时，系统自动将数据下载到MNIST_data/文件夹中。当执行完语句后，可以看到MNIST_data/文件夹下多了4个上述文件。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;成功加载数据后，得到了一个mnist对象，可以通过mnist对象的数学访问到MNIST数据集。</p>
<table>
<thead>
<tr>
<th>属性名</th>
<th>内容</th>
<th>大小</th>
</tr>
</thead>
<tbody>
<tr>
<td>mnist.train,images</td>
<td>训练图像</td>
<td>(55000,784)</td>
</tr>
<tr>
<td>mnist.train.label</td>
<td>训练标签</td>
<td>(55000,10)</td>
</tr>
<tr>
<td>mnist.validation.images</td>
<td>验证图像</td>
<td>（5000,784）</td>
</tr>
<tr>
<td>mnist.validation.labels</td>
<td>验证标签</td>
<td>（5000,10）</td>
</tr>
<tr>
<td>mnist.test.images</td>
<td>测试图像</td>
<td>（10000,784）</td>
</tr>
<tr>
<td>mnist.test.labels</td>
<td>测试标签</td>
<td>（10000,10）</td>
</tr>
</tbody>
</table>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运行下面代码可以查看各个变量的形状大小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看训练数据的大小</span></span><br><span class="line">print(mnist.train.images.shape)  <span class="comment"># (55000, 784)</span></span><br><span class="line">print(mnist.train.labels.shape)  <span class="comment"># (55000, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看验证数据的大小</span></span><br><span class="line">print(mnist.validation.images.shape)  <span class="comment"># (5000, 784)</span></span><br><span class="line">print(mnist.validation.labels.shape)  <span class="comment"># (5000, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看测试数据的大小</span></span><br><span class="line">print(mnist.test.images.shape)  <span class="comment"># (10000, 784)</span></span><br><span class="line">print(mnist.test.labels.shape)  <span class="comment"># (10000, 10)</span></span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 原始的MNIST数据集中包含了60000张训练图片和10000张测试图片。而在tensorflow中，又将原先的60000张训练图片重新划分成了新的55000张训练图片和5000张验证图片。所以在mnist对象中，数据一共分为三部分：mnist.train是训练图片数据，mnist.validation是验证图片数据，mnist.test是测试图片数据，这正好对应了机器学习中额训练集、验证集和测试集、一般来说，会在训练集上训练模型，通过在验证集上调整参数，最后通过测试集确定模型的性能。</p>
<p>MNIST数据集保存为图片</p>
<p>为了加深理解，将MNIST数据集读取出来，并保存为图片文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取MNIST数据集。如果不存在会事先下载。</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 我们把原始图片保存在MNIST_data/raw/文件夹下</span></span><br><span class="line"><span class="comment"># 如果没有这个文件夹会自动创建</span></span><br><span class="line">save_dir = <span class="string">'MNIST_data/raw/'</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(save_dir) <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">    os.makedirs(save_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存前20张图片</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    <span class="comment"># 请注意，mnist.train.images[i, :]就表示第i张图片（序号从0开始）</span></span><br><span class="line">    image_array = mnist.train.images[i, :]</span><br><span class="line">    <span class="comment"># TensorFlow中的MNIST图片是一个784维的向量，我们重新把它还原为28x28维的图像。</span></span><br><span class="line">    image_array = image_array.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    <span class="comment"># 保存文件的格式为 mnist_train_0.jpg, mnist_train_1.jpg, ... ,mnist_train_19.jpg</span></span><br><span class="line">    filename = save_dir + <span class="string">'mnist_train_%d.jpg'</span> % i</span><br><span class="line">    <span class="comment"># 将image_array保存为图片</span></span><br><span class="line">    <span class="comment"># 先用scipy.misc.toimage转换为图像，再调用save直接保存。</span></span><br><span class="line">    scipy.misc.toimage(image_array, cmin=<span class="number">0.0</span>, cmax=<span class="number">1.0</span>).save(filename)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Please check: %s '</span> % save_dir)</span><br></pre></td></tr></table></figure>
<h2 id="3、图像标签的独热表示"><a href="#3、图像标签的独热表示" class="headerlink" title="3、图像标签的独热表示"></a>3、图像标签的独热表示</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变量mnist.train.labels表示训练数据的标签，它的形状是(55000,10)。原始的图像标签是数字0-9，我们完全可以用一个数字来存储图像标签，但为什么这里每个训练标签是一个10维的向量呢？其实，这个10维的向量是原先类别的独热(one-hot)表示。</p>
<h2 id="4、Softmax回归识别MNIST"><a href="#4、Softmax回归识别MNIST" class="headerlink" title="4、Softmax回归识别MNIST"></a>4、Softmax回归识别MNIST</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;softmax回归是一个线性的多分类模型，实际上它是从Logistic回归模型转化而来的。区别是Logistic回归模型是一个两分类模型，而Softmax模型为多分类模型。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在手写体识别问题中，一共有10个类别(0~9)，我们希望对输入的图像计算它属于每个类别的概率。如属于9的概率为70%，属于1的概率为10%等。最后模型预测的结果就是概率最大的那个类别。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先来了解什么是Softmax函数。Softmax函数的主要功能是将各个类别的“打分“转化成合理的概率值。例如，一个样本可能属于三个类别：第一个类别的打分为a，第二个类别的打分为b，第三个类别的打分为c。打分越高代表属于这个类别的概率越高，但是打分本身不代表概率，因为打分的值可以使负数，也可以很大。但概率要求值必须在0~1，并且三类的概率加起来等于1.那么，如何将(a,b,c)转换成合理的概率值呢？方法就是使用Softmax函数。例如，对(a,b,c)使用softmax函数后，相异的值会变成$({e^a \over e^a+e^b+e^c},{e^b \over e^a+e^b+e^c},{e^c \over e^a+e^b+e^c})$，也就是说，第一类的概率用$e^a \over e^a+e^b+e^c$来表示，第二类用$e^b \over e^a+e^b+e^c$来表示，第三类可以用$e^c \over e^a+e^b+e^c$来表示。显然，这三个数都在0~1之间，并且加起来正好等于1，是合理的概率表示。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设$x$是单个样本的特征，$W,b$是Softmax模型的参数。在MNIST数据集中，$x$就代表输入图片，它是一个784维的向量，而$W$是一个矩阵，它的形状为(784,10),$b$是一个10维的向量，10代表的是类别数。Softmax模型的第一步是通过下面的公司计算各个类别的Logit：<br>$$<br>Logit=W^Tx+b<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logit同样是一个10维的向量，它的实际上可以看出样本对于于各个类别的“打分”。接下来使用Softmax函数将他转换成各个类别的概率值：<br>$$<br>y=Softmax(Logit)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Softmax模型输出的y代表各个类别的概率，还可以直接用下面的式子来表示整个Softmax模型：<br>$$<br>y=Softmax(W^Tx+b)<br>$$</p>
<p>##5、Softmax实现</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先导入tensorflow模型,下面是约定俗成的写法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来导入MNIST数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;构建模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建x，x是一个占位符（placeholder），代表待识别的图片</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line"><span class="comment"># W是Softmax模型的参数，将一个784维的输入转换为一个10维的输出</span></span><br><span class="line"><span class="comment"># 在TensorFlow中，变量的参数用tf.Variable表示</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># b是又一个Softmax模型的参数，我们一般叫做“偏置项”（bias）。</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># y=softmax(Wx + b)，y表示模型的输出</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line"><span class="comment"># y_是实际的图像标签，同样以占位符表示。</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里定义了一下占位符(placeholder)和变量(Variable).在tensorflow中无论是占位符还是变量，它们实际上都是“Tensor”。从Tensorflow的名字中就可以看出Tensor在整个系统处于核心地位。Tensor并不是具体的数值，它只是一些我们”希望“Tensorflow系统计算的“节点”。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里的占位符和变量是不同类型的Tensor。先来讲解占位符。占位符不依赖于其它的Tensor，它的值由用户自行传到给Tensorflow，通常用来存储样本数据和标签。如在这里定义了x = tf.placeholder(tf.float32, [None, 784])，它是用来存储训练图片数据的占位符，它的形状为[None, 784]，None表示这一维的大小是任意的，也就是说可以传递任意章训练图片给这个占位符，每张图片用784维的向量表示。同样的，y_ = tf.placeholder(tf.float32, [None, 10])也是一个占位符，它存储训练图片的实际标签。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再来看什么是变量，变量是指在计算过程中可以改变的值，每次计算后变量的值会被保存下来，通常用变量来存储模型的参数。如：W = tf.Variable(tf.zeros([784, 10]))。创建变量时通常要指定某些初始值。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除了变量和占位符之外，还创建了一个y</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个y就是一个依赖x，W，b的Tensor。如果要求Tensorflow计算y的值，那么系统首先会获取$x,W,b$的值，再去计算y的值</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y实际上定义了一个Softmax回归模型，在此可以尝试写出y的形状。假设输入x的形状为(N,784)，其中N表示输入的训练图像的数目。W的形状为（784,10），b的形状为（10），那么Wx+b的形状是(N,10)。Softmax函数不改变结果的形状，所以得到y的形状为(N,10)。也就是说，y的每一行是10维的向量，表示模型预测的样本对应到各个类别的概率。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型的输出是y，而实际的标签为y_,它们应当越相似越好。在Softmax回归模型中，通常使用“交叉熵”损失来衡量相似性。损失越小，模型的输出就和实际标签越接近，模型的预测也就越准确。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Tensorflow中，这样定义交叉熵损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据y, y_构造交叉熵损失</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;构造玩损失之后，下面一步是如何优化损失，让损失减小。这里使用梯度下降优化损失，定义为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有了损失，我们就可以用随机梯度下降针对模型的参数（W和b）进行优化</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>​    Tensorfloe默认会对所有变量计算梯度。这里只定义两个变量$W$和$b$，因此程序将会使用梯度下降法对$W,b$计算梯度并更新它们的值。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在优化之前，必须要创建一个会话(Session),并在会话中对变量进行初始化操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个Session。只有在Session中才能运行优化步骤train_step。</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 运行之前必须要初始化所有变量，分配内存。</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了会话，就可以对变量进行优化了，优化的程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 在mnist.train中取100个训练数据</span></span><br><span class="line">    <span class="comment"># batch_xs是形状为(100, 784)的图像数据，batch_ys是形如(100, 10)的实际标签</span></span><br><span class="line">    <span class="comment"># batch_xs, batch_ys对应着两个占位符x和y_</span></span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 在Session中运行train_step，运行时要传入占位符的值</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与变量不同的是，占位符的值不会被保存，每次可以给占位符传递不同的值。</p>
<p>​    训练完成后，可以检测模型训练的结果，对于的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正确的预测结果</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算预测准确率，它们都是Tensor</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"><span class="comment"># 在Session中运行Tensor可以得到Tensor的值</span></span><br><span class="line"><span class="comment"># 这里是获取最终模型的正确率</span></span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))  <span class="comment"># 0.9052</span></span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型预测y的形状是(N,10)，而实际标签y_的形状是(N,10)，其中N为输入模型1的样本数。tf.argmax(y,1)、tf.argmax(y_,1)的功能是取出数组最大值得下标，可以用来将独热表示以及模型输出转换为数字标签。假设传入四个样本，它们的独热表示y_为（需要通过sess.run(y_)才能获取此Tensor的值，下同）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>tf.argmax(y_,1)就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>​    也就是说，取出每一行最大值对应的下标位置，它们是输入样本的实际标签。假设此时模型的预测输出y为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>],</span><br><span class="line"> [<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>],</span><br><span class="line"> [<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>],</span><br><span class="line"> [<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>]]</span><br></pre></td></tr></table></figure>
<p>tf.argmax(y_,1)就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>​    得到预测的标签和实际标签，接下来通过tf.equal函数来比较它们是否相等，并将结果保存到correct_prediction中。在上述例子中，correct_prediction就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">False</span>]</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即第一个样本和最后一个样本预测是正确的，另外两个样本预测错误。可以用tf.cast(correct_prediction,tf.float32)将比较值转换成float32型的变量，此时True会被转换成1，False会被转换为0.在上述例子中，tf.cast(correct_prediction, tf.float32)的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">1.</span>]</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，用tf.reduce_mean可以计算数组中的所有元素的平均值，相当于得到了模型的预测准确率，如[1.,0.,0.,1.]的平均值为0,5，即50%的分类准确率。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在程序softmax_regression.py中，传入占位符的值是feed_dict={x: mnist.test.images, y_: mnist.test.labels}。也就是说，使用全体测试样本进行测试，测试图片一共10000张，运行的结果为0.9052，即90.52%的准确率。因为softmax回归是一个比较简单的模型，这里的预测准确率并不高。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 导入MNIST教学的模块</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建x，x是一个占位符（placeholder），代表待识别的图片</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line"><span class="comment"># W是Softmax模型的参数，将一个784维的输入转换为一个10维的输出</span></span><br><span class="line"><span class="comment"># 在TensorFlow中，变量的参数用tf.Variable表示</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># b是又一个Softmax模型的参数，我们一般叫做“偏置项”（bias）。</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># y=softmax(Wx + b)，y表示模型的输出</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line"><span class="comment"># y_是实际的图像标签，同样以占位符表示。</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 至此，我们得到了两个重要的Tensor：y和y_。</span></span><br><span class="line"><span class="comment"># y是模型的输出，y_是实际的图像标签，不要忘了y_是独热表示的</span></span><br><span class="line"><span class="comment"># 下面我们就会根据y和y_构造损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据y, y_构造交叉熵损失</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))</span><br><span class="line"><span class="comment"># 有了损失，我们就可以用随机梯度下降针对模型的参数（W和b）进行优化</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Session。只有在Session中才能运行优化步骤train_step。</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 运行之前必须要初始化所有变量，分配内存。</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">print(<span class="string">'start training...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行1000步梯度下降</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 在mnist.train中取100个训练数据</span></span><br><span class="line">    <span class="comment"># batch_xs是形状为(100, 784)的图像数据，batch_ys是形如(100, 10)的实际标签</span></span><br><span class="line">    <span class="comment"># batch_xs, batch_ys对应着两个占位符x和y_</span></span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 在Session中运行train_step，运行时要传入占位符的值</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确的预测结果</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算预测准确率，它们都是Tensor</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"><span class="comment"># 在Session中运行Tensor可以得到Tensor的值</span></span><br><span class="line"><span class="comment"># 这里是获取最终模型的正确率</span></span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id:  'window.location.pathname', // 可选。默认为 location.href
  title:  'tensorflow入门笔记1',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>





]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[embedding]]></title>
      <url>/2018/05/11/embedding/</url>
      <content type="html"><![CDATA[<h2 id="搜狐新闻数据-SogouCS"><a href="#搜狐新闻数据-SogouCS" class="headerlink" title="搜狐新闻数据(SogouCS)"></a>搜狐新闻数据(SogouCS)</h2><h5 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h5><p>来自搜狐新闻2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息</p>
<h5 id="格式说明："><a href="#格式说明：" class="headerlink" title="格式说明："></a>格式说明：</h5><p>数据格式为</p>
<doc><br><br><url>页面URL</url><br><br><docno>页面ID</docno><br><br><contenttitle>页面标题</contenttitle><br><br><content>页面内容</content><br><br></doc>

<p>注意：content字段去除了HTML标签，保存的是新闻正文文本</p>
<p>完整版(648MB)：<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.tar.gz" target="_blank" rel="noopener">tar.gz格式</a>，<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.zip" target="_blank" rel="noopener">zip格式</a> </p>
<h2 id="1、构建中文语料库"><a href="#1、构建中文语料库" class="headerlink" title="1、构建中文语料库"></a>1、构建中文语料库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜狐新闻 2.1G</span></span><br><span class="line">tar -zxvf news_sohusite_xml.full.tar.gz </span><br><span class="line">cat news_sohusite_xml.dat | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_sohusite.txt &gt; news_sohusite_cutword.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全网新闻 1.8G</span></span><br><span class="line">tar -zxvf news_tensites_xml.full.tar.gz </span><br><span class="line">cat news_tensites_xml.full.tar.gz | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_tensite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_tensite.txt &gt; news_tensite_cutword.txt</span><br></pre></td></tr></table></figure>
<p>第一条命令解压之后会生成news_sohusite_xml.dat文件</p>
<h2 id="2、利用gensim库进行训练"><a href="#2、利用gensim库进行训练" class="headerlink" title="2、利用gensim库进行训练"></a>2、利用gensim库进行训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment">#  Loading corpus...</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    input_file = <span class="string">"news_sohu_text.txt"</span></span><br><span class="line">    vector_file = <span class="string">"news_sohu_embedding"</span></span><br><span class="line">    <span class="comment"># 保存字典文件</span></span><br><span class="line">    vocab_file = <span class="string">"vocabulary_file"</span></span><br><span class="line">    sentences = word2vec.Text8Corpus(input_file)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Training embedding...</span></span><br><span class="line"><span class="string">    embedding_dim : 100</span></span><br><span class="line"><span class="string">    window_size ：5</span></span><br><span class="line"><span class="string">    minimum_count of word ：5</span></span><br><span class="line"><span class="string">    epoch of model（iter）：10</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 记录各种优先级的东西; 激活日志记录</span></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s: %(levelname)s: %(message)s'</span>, level=logging.INFO)</span><br><span class="line">     <span class="comment"># sg=0 使用cbow训练, sg=1对低频词较为敏感，默认是sg=0</span></span><br><span class="line">    model = word2vec.Word2Vec(sentences, size=<span class="number">100</span>, window=<span class="number">5</span>, min_count=<span class="number">6</span>, iter=<span class="number">10</span>, workers=multiprocessing.cpu_count())</span><br><span class="line">    <span class="comment">#  Saving word_embedding</span></span><br><span class="line">    model.wv.save_word2vec_format(vector_file, fvocab=vocab_file, binary=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>​    input_file是输入文件，如果是训练词向量，必须是分词好的文件，词与词之间空格隔开。如果训练的是字向量，字与字之间同样用空格隔开。一个句子是一个列表，所有的句子是二维列表。</p>
<h2 id="3、加载训练好的embedding并测试"><a href="#3、加载训练好的embedding并测试" class="headerlink" title="3、加载训练好的embedding并测试"></a>3、加载训练好的embedding并测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型，embedding_file是训练好的向量文件</span></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=<span class="keyword">False</span>)</span><br><span class="line">temp1 = model.most_similar(<span class="string">'我'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">temp2 = model.most_similar(<span class="string">'好'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">t = model.similarity(<span class="string">"他"</span>, <span class="string">"她"</span>)  <span class="comment"># 测试相似度</span></span><br><span class="line"></span><br><span class="line">print(temp1)</span><br><span class="line">print(temp2)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<p>我在搜狐新闻训练的向量测试的结果：</p>
<p><code>[(&#39;咱&#39;, 0.8646953105926514), (&#39;他&#39;, 0.857693076133728), (&#39;你&#39;, 0.7627511024475098), (&#39;她&#39;, 0.7525184750556946), (&#39;俺&#39;, 0.7336732149124146), (&#39;它&#39;, 0.7071460485458374), (&#39;牠&#39;, 0.46839216351509094), (&#39;谁&#39;, 0.4390422999858856), (&#39;您&#39;, 0.40126579999923706), (&#39;尤&#39;, 0.3958999514579773)]</code></p>
<p><code>[(&#39;懂&#39;, 0.4896905720233917), (&#39;棒&#39;, 0.4554690718650818), (&#39;是&#39;, 0.4444722533226013), (&#39;佳&#39;, 0.44040754437446594), (&#39;让&#39;, 0.43987756967544556), (&#39;快&#39;, 0.4291210174560547), (&#39;朋&#39;, 0.42199385166168213), (&#39;错&#39;, 0.4218539297580719), (&#39;多&#39;, 0.42170557379722595), (&#39;乖&#39;,</code>0.4164968729019165)]<code>``
</code>0.813737169426`</p>
<h2 id="4、训练注意"><a href="#4、训练注意" class="headerlink" title="4、训练注意"></a>4、训练注意</h2><p>Word2vec 有多个影响训练速度和质量的参数。</p>
<p>其中之一是用来修剪内部字典树的。在一个数以亿计的预料中出现一到两次的单词非常有可能是噪音或不需要被关注的。另外，也没有足够的数据对他们进行有意义的训练。因此，最好的办法就是直接将他们忽略掉。<br> <code>model = Word2Vec(sentences, min_count=10) # default value is 5</code></p>
<ol>
<li>对于设定 <code>min_count</code> 的值，合理的范围是0 - 100，可以根据数据集的规模进行调整。</li>
<li><p>另一个参数是神经网络 NN 层单元数，它也对应了训练算法的自由程度。</p>
<p><code>model = Word2Vec(sentences, size=200) # default value is 100</code><br>更大的 <code>size</code> 值需要更多的训练数据，但也同时可以得到更准确的模型。合理的取值范围是几十到几百。</p>
</li>
</ol>
<p>3.最后一个主要参数是训练并行粒度，用来加速训练。<br> <code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization</code><br> 该参数只有在机器已安装 Cython 情况下才会起到作用。如没有 Cython，则只能单核运行。</p>
<h2 id="5、评估"><a href="#5、评估" class="headerlink" title="5、评估"></a>5、评估</h2><p>Word2vec 训练是一个非监督任务，很难客观地评估结果。评估要<strong>依赖于后续的实际应用场景</strong>。Google 公布了一个包含 20,000 语法语义的测试样例，形式为 “A is to B as C is to D”。</p>
<p>需要注意的是，<strong>如在此测试样例上展示良好性能并不意味着在其它应用场景依然有效，反之亦然</strong>。</p>
<p>参考：</p>
<p>1.<a href="https://www.cnblogs.com/jkmiao/p/7007763.html" target="_blank" rel="noopener">https://www.cnblogs.com/jkmiao/p/7007763.html</a></p>
]]></content>
      
        
        <tags>
            
            <tag> embedding </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[动态规划]]></title>
      <url>/2018/05/07/dp1/</url>
      <content type="html"><![CDATA[<p>​    <strong>动态规划中递推式的求解方法不是动态规划的本质。</strong> 动态规划的本质，是对问题<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 动态规划是通过<strong>拆分问题，</strong>定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。 本题下的其他答案，大多都是在说递推的求解方法，但<strong>如何拆分问题</strong>，才是动态规划的核心。 而<strong>拆分问题</strong>，靠的就是<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 要解决这个问题，我们首先要<strong>定义这个问题</strong>和这个问题的子问题。 有人可能会问了，题目都已经在这了，我们还需定义这个问题吗？需要，原因就是问题在字面上看，不容易找不出子问题，而没有子问题，这个题目就没办法解决 。状态转移方程，就是定义了问题和子问题之间的关系。 可以看出，状态转移方程就是带有条件的递推式。 </p>
<h2 id="一-阶乘-Factorial"><a href="#一-阶乘-Factorial" class="headerlink" title="一.阶乘(Factorial)"></a>一.阶乘(Factorial)</h2><p>$1\times 2\times3\times\dots\times N$,整数1到$N$的连乘积。$N$阶乘$N!$</p>
<p>分析：$N!$源自$(N-1)!$，如此就递回分割问题了。</p>
<p><img src="/2018/05/07/dp1/./dp1/wps63B6.tmp.jpg" alt="img"></p>
<p>阵列的每一格对应每一个问题。设定第一格的答案，再以回圈依序计算其余答案。</p>
<p><img src="/2018/05/07/dp1/D:/hexo\source\_posts/wps3581.jpg" alt="img"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> f[N];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f[i] = f[i<span class="number">-1</span>] * i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> f = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f *= i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h3><p>总共$N$个问题，每个问题花费$O(1)$时间，总共花费$O(N)$时间</p>
<h3 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h3><p>求$1!$到$N!$:总共$N$个问题，用一条$N$格阵列存储全部问题的答案，空间复杂度为$O(N)$</p>
<p>只求$N!$:用一个变量累计成绩，空间复杂度为$O(1)$</p>
<p>Dynamic Programming: recurrence</p>
<p>Dynamic Programming = Divide and Conquer + Memoization</p>
<p>​    动态规划是分治法的延伸。当递推分割出来的问题，一而再、再而三出现，就运用记忆法储存这些问题的答案，避免重复求解，以空间换取时间。动态规划的过程，就是反复地读取数据、计算数据、储存数据。</p>
<p><img src="/2018/05/07/dp1/wpsB825.jpg" alt="img"></p>
<ol>
<li><p>把原问题递归分割成许多更小的问题。（recurrence）</p>
<p>子问题与原问题的求解方式皆类似。（optimal sub-structure）</p>
<p>子问题会一而再、再而三的出现。（overlapping sub-problems）</p>
</li>
<li><p>设计计算过程：</p>
<p> 确认每个问题需要哪些子问题来计算答案。（recurrence）</p>
<p> 确认总共有哪些问题。（state space）</p>
<p>把问题一一对应到表格。（lookup table）</p>
<p>决定问题的计算顺序。（computational sequence）</p>
</li>
</ol>
<p>​       确认初始值、计算范围（initial states / boundary）   </p>
<ol>
<li><p>实作，主要有两种方式：</p>
<p> Top-down</p>
<p> Bottom-up</p>
</li>
</ol>
<h3 id="1-recurrence"><a href="#1-recurrence" class="headerlink" title="1. recurrence"></a>1. recurrence</h3><p>​    递归分割问题时，当子问题与原问题完全相同，只有数值范围不同，我们称此现象为recurrence，再度出现、一而再出现之意。</p>
<p>【注： recursion 和 recurrence ，中文都翻译为“递归”，然而两者意义大不相同，读者切莫混淆】</p>
<p>​    此处以爬楼梯问题当做范例。先前于递归法章节，已经谈过如何求踏法，而此处要谈如何求踏法数目。</p>
<p><img src="/2018/05/07/dp1/wpsB76A.jpg" alt="img"></p>
<p>​    踏上第五阶，只能从第四阶或从第三阶踏过去，因此爬到五阶源自两个子问题：爬到四阶与爬到三阶。</p>
<p><img src="/2018/05/07/dp1/a1.png" alt="a"></p>
<p>​    爬到五阶的踏法数目，就是综合爬到四阶与爬到三阶的踏法数目。写成数学式子是$f(5)=f(4)+f(3)$，其中$f$表示爬到某阶的踏法数目。</p>
<p>​    依样画葫芦，得到<br>$$<br>f(4)=f(3)+f(2)\\<br>f(3)=f(2)+f(1)<br>$$<br>​    爬到两阶与爬到一阶无法再分割，没有子问题，直接得到<br>$$<br>f(2)=2\\<br>f(1)=1<br>$$<br>整理成一道简明扼要的递归公式：<br>$$<br>f(n)= \begin{cases}<br>1&amp;if   n=1 \\<br>2  &amp; if   n =2 \\<br>f(n-1)+f(n-2) &amp; if  n \ge3  and   n\le 5<br>\end{cases}<br>$$<br>爬到任何一阶的踏法数目，都可以借由这道递归公式求得。<img src="/2018/05/07/dp1/D:/hexo\source\_posts/wpsE565.tmp.png" alt="img">带入实际数值，递归计算即可。</p>
<p>​    为什么分割问题之后，就容易计算答案呢？因为分割问题时，同时也分类了这个问题所有可能答案。分类使得答案的规律变得单纯，于是更容易求得答案。</p>
<p><img src="/2018/05/07/dp1/wps2ACB.jpg" alt="img"></p>
<h3 id="2-、state-space"><a href="#2-、state-space" class="headerlink" title="2 、state space"></a>2 、state space</h3><p>想要计算第五阶的踏法数目。</p>
<p>全部的问题是“爬到一节”、“爬到二阶”、“爬到三阶”、“爬到四阶”、“爬到五阶”。</p>
<p><img src="/2018/05/07/dp1/wps2204.jpg" alt="img"></p>
<p>至于爬到零阶、爬到负一阶。爬到负二阶以及爬到六阶、爬到七阶没有必要计算。</p>
<h3 id="3、lookup-table"><a href="#3、lookup-table" class="headerlink" title="3、lookup table"></a>3、lookup table</h3><p>​    建立六格的阵列，存储五个问题的答案。</p>
<p>​    表格的第零格不使用，第一格式爬到一阶的答案，第二格是爬到二阶的答案，以此类推。</p>
<p><img src="/2018/05/07/dp1/wps75F.jpg" alt="img"></p>
<p>​    如果只计算爬完五阶，也可以建立三个变量交替使用。</p>
<h3 id="4、computational-sequence"><a href="#4、computational-sequence" class="headerlink" title="4、computational sequence"></a>4、computational sequence</h3><p>​    因为每个问题都依赖阶数少一阶、阶数少两阶这两个问题，所以必须由阶数小的问题开始计算。</p>
<p>​    计算顺序是爬到一阶、爬到二阶、……、爬到五阶。设定初始值。</p>
<h3 id="5、-initial-states-boundary"><a href="#5、-initial-states-boundary" class="headerlink" title="5、 initial states / boundary"></a>5、 initial states / boundary</h3><p>​    最先计算的问题时爬到一阶与爬到二阶，必须预先将答案填入表格。写入方程式，才能计算其他问题。心算求得爬到一阶的答案是1，爬到二阶的答案是2。最后计算的问题时原问题是原问题爬到五阶。</p>
<p>​    为了让表格更顺畅。为了让程式更漂亮，可以加入爬到零阶的答案，对应到表格的第零格。爬到零阶的答案，可以运用爬到一阶的答案与爬到两阶的答案，刻意逆推而得。</p>
<p><img src="/2018/05/07/dp1/wps589E.jpg" alt="img"></p>
<p>​    最后可以把初始值，尚待计算的部分、不需计算的部分，整理成一道递归公式：<br>$$<br>f(n)= \begin{cases}<br>0 &amp; if  n<0\\ 0="" 1="" &="" if="" \="" n="0\\" 1&if="" \\="" f(n-1)+f(n-2)="" \ge2="" and="" n\le="" 5\\="">5<br>\end{cases}<br>$$</0\\></p>
<h3 id="6、实现"><a href="#6、实现" class="headerlink" title="6、实现"></a>6、实现</h3><p>​    直接用递归实作，而不使用记忆体存储各个问题的答案，是最直接的方式，也是最慢的方式。时间复杂度是$O(f(n))$。问题一而再、再而三的出现，不断呼叫同样的函数求解，效率不彰。刚接触DP的新手常犯这种错误。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​    正确的DP是一边计算，一边将计算出来的数值存入表格，以便不必重算。这里整理了两种实现方式，各有优缺点。</p>
<p>1)Top-down</p>
<p>2)Bottom-up</p>
<p><img src="/2018/05/07/dp1/wps58E0.jpg" alt="img"></p>
<p>1)Top-down</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 表格，储存全部问题的答案。</span></span><br><span class="line"><span class="keyword">bool</span> solve[<span class="number">6</span>];  <span class="comment">// 记录问题是否已经計算完毕</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> table[n] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 如果已經計算過，就直接讀取表格的答案。</span></span><br><span class="line">    <span class="keyword">if</span> (solve[n]) <span class="keyword">return</span> table[n];</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 如果不曾計算過，就計算一遍，儲存答案。</span></span><br><span class="line">    table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>); <span class="comment">// 將答案存入表格</span></span><br><span class="line">    solve[n] = <span class="literal">true</span>;            <span class="comment">// 已經計算完畢</span></span><br><span class="line">    <span class="keyword">return</span> table[n];</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        solve[i] = <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 合并solve跟table，简化代码。</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 用 0 代表该问题还未计算答案</span></span><br><span class="line"><span class="comment">//  if (table[n] != 0) </span></span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line"><span class="keyword">if</span> (table[n]) </span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line">    <span class="keyword">return</span> table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">     table[i] = <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​    这个实现方式的好处是不必斤斤计较计算顺序，因为代码中的递归结构会迫使最小的问题先被计算。这个实现方式的另一个好处是只计算必要的问题，而不必计算所有可能的问题。</p>
<p>​    这个实现的坏处是代码采用递归结构，不断调用函数，执行效率差。这个实现方式的另一个坏处是无法自由地控制计算顺序，因而无法妥善运用记忆体，浪费了可回收再利用的记忆体。</p>
<p>2）Bottom-up</p>
<p>​    指定一个计算顺序，然后由最小问题开始计算。特色是代码通常只有几个递归。这个实现方式的好处与坏处与前一个方式恰好互补。</p>
<p>​    首先建立表格。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">5</span> + <span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<p>心算爬到零阶的答案、爬到一阶的答案，填入报个当中，作为初始值。分别天道表格的第零格、第一格。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">table[0] = 1;</span><br><span class="line">table[1] = 1;</span><br></pre></td></tr></table></figure>
<p>​    尚待计算的部分就是爬到两阶的答案、……、爬到五阶的答案。通常是使用递归，按照计算顺序来计算。</p>
<p>计算过程的实现方式，有两种迥异的风格。一种是往回取值，是常见的实现方式。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">往回取值</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    table[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        table[i] = table[i<span class="number">-1</span>] + table[i<span class="number">-2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另一种是往后补值，是罕见的实现方式。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">往后补值<span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++) table[i] = <span class="number">0</span>;</span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//  table[1] = 1;   // 剛好可以被算到</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span> &lt;= <span class="number">5</span>) table[i+<span class="number">1</span>] += table[i];</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">2</span> &lt;= <span class="number">5</span>) table[i+<span class="number">2</span>] += table[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="7、总结"><a href="#7、总结" class="headerlink" title="7、总结"></a>7、总结</h3><p>​    第一，先找到原问题和其子问题之间的关系，写成递归公式。如此一来，即可利用递归公式，用子子问题的答案，求出子问题的答案；用子问题的答案，求出原问题的答案。</p>
<p>​    第二。确认可能出现的问题全部总共有哪些；用子问题的答案，求出原问题的答案。</p>
<p>​    第三。有了递归公式之后，就必须安排出一套计算顺序。大问题的答案，总是以小问题的答案来求得的，所以，小问题的答案必须是先算的，否则大问题的答案从何而来呢？一个好的安排方式，不但使代码易写，还可重复利用记忆体空间。</p>
<p>​    第四。记得先将最小，最先被计算的问题，心算出答案，储存如表格，内建与代码之中。一道递归公式必须拥有初始值，才有计算其他项。</p>
<p>​    第五。实现DP的代码时，会建立一个表格，表格存入所有大小问题的答案。安排好每个问题的答案再表格的哪个位置，这样计算时才能知道该在哪里取值。</p>
<p>​    切勿存取超出表格的原始，产生溢位情形，导致答案算错。计算过程当中，一旦某个问题的答案出错，就会如骨牌效应般一个影响一个，造成很难除错。</p>
]]></content>
      
        
        <tags>
            
            <tag> dynamic programming </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf8]]></title>
      <url>/2018/04/08/tf8/</url>
      <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>​    某些任务需要能够更好的处理<strong>序列</strong>的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个<strong>序列</strong>。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络(Recurrent Neural Network)</strong>。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个<strong>循环神经网络</strong>。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>​    RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。那么，什么是语言模型呢？我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p>
<p>我昨天上学迟到了，老师批评了__。</p>
<p>​    我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p>
<p>​    <strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p>
<p>​    <strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p>
<p>​    使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p>
<p>我 昨天 上学 迟到 了 ，老师 批评 了 。</p>
<p>​    如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p>
<p>​    现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p>
<p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p>
<h2 id="循环神经网络是啥"><a href="#循环神经网络是啥" class="headerlink" title="循环神经网络是啥"></a>循环神经网络是啥</h2><p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p>
<h3 id="基本循环神经网络"><a href="#基本循环神经网络" class="headerlink" title="基本循环神经网络"></a>基本循环神经网络</h3><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：</p>
<p><img src="/2018/04/08/tf8/image1.jpg" alt="1"></p>
<p>​    纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为<strong>循环神经网络</strong>实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的<strong>全连接神经网络</strong>。x是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的<strong>权重矩阵</strong>（读者可以回到第三篇文章<a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a>，看看我们是怎样用矩阵来表示<strong>全连接神经网络</strong>的计算的）；o也是一个向量，它表示<strong>输出层</strong>的值；V是隐藏层到输出层的<strong>权重矩阵</strong>。那么，现在我们来看看W是什么。<strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。<strong>输出层是一个全连接层，它的每个节点都和隐藏层的每个节点相连，隐藏层是循环层。</strong></p>
<p>如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：</p>
<p><img src="/2018/04/08/tf8/image2.jpg" alt="2"></p>
<p>​    </p>
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：<br>$$<br>o_t=g(Vs_t)…..(式1)……(1)\\<br>s_t=f(Ux_t+Ws_{t-1})…..(式2)……(2)<br>$$<br>​    <strong>式1</strong>是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p>从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复把<strong>式2</strong>带入到<strong>式1</strong>，我们将得到：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t)     ……….(3)\\<br>&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(4)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))……….(5)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))……….(6)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+…))))……….(7)<br>\end{align}<br>$$<br>从上面可以看出，<strong>循环神经网络</strong>的输出值$o_t$，是受前面历次输入值$x_t、x_{t-1}、x_{t-2}、x_{t-3}$…影响的，这就是为什么<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p>
<h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>​    对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<p>我的手机坏了，我打算____一部新手机。</p>
<p>​    可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：</p>
<p><img src="/2018/04/08/tf8/image3.png" alt="3"></p>
<p>​    当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中$y_2$的计算。</p>
<p>​    从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_2$取决于$A_2$和$A’_2$。其计算方法为：<br>$$<br>\mathrm{y}_2=g(VA_2+V’A_2’)<br>$$<br>$A_2$和$A’_2$则分别计算：<br>$$<br>\begin{align}<br>A_2&amp;=f(WA_1+U\mathrm{x}_2)………(8)\\<br>A_2’&amp;=f(W’A_3’+U’\mathrm{x}_2)…….(9)\\<br>\end{align}<br>$$<br>​    现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_t-1$有关；反向计算时，隐藏层的值$s’_t$与$s’_{t-1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出双向循环神经网络的计算方法：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V’\mathrm{s}_t’)…….(10)\\<br>\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(11)\\<br>\mathrm{s}_t’&amp;=f(U’\mathrm{x}_t+W’\mathrm{s}_{t+1}’)……(12)\\<br>\end{align}<br>$$</p>
<p>​    从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<h3 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h3><p>前面我们介绍的<strong>循环神经网络</strong>只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p>
<p><img src="/2018/04/08/tf8/image4.png" alt="4"></p>
<p>我们把第i个隐藏层的值表示为$s_t^{(i)}$、$s_t^{‘(i)}$，则<strong>深度循环神经网络</strong>的计算方式可以表示为：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V’^{(i)}\mathrm{s}_t’^{(i)})…….(13)\\<br>\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})……….(14)\\<br>\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\mathrm{s}_t’^{(i-1)}+W’^{(i)}\mathrm{s}_{t+1}’)……..(15)\\<br>…(16)\\<br>\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})……..(17)\\<br>\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\mathrm{x}_t+W’^{(1)}\mathrm{s}_{t+1}’)………(18)\\<br>\end{align}<br>$$</p>
<h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<ol>
<li>前向计算每个神经元的输出值；</li>
<li>反向计算每个神经元的<strong>误差项</strong>$\delta_j$值，它是误差函数E对神经元j的<strong>加权输入</strong>$net_j$的偏导数；</li>
<li>计算每个权重的梯度。</li>
</ol>
<p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p>
<p>循环层如下图所示：</p>
<p><img src="/2018/04/08/tf8/image5.png" alt="5"></p>
<h4 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h4><p>使用前面的<strong>式2</strong>对循环层进行前向计算：<br>$$<br>\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})<br>$$<br>注意，上面的$s_t$、$x_t$、$s_{t-1}$都是向量，用<strong>黑体字母</strong>表示；而U、V是<strong>矩阵</strong>，用大写字母表示。<strong>向量的下标</strong>表示<strong>时刻</strong>，例如，表示在t时刻向量s的值。</p>
<p>​    我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n<em> m$，矩阵W的维度是$n</em>n$。下面是上式展开成矩阵的样子，看起来更直观一些：<br>$$<br>\begin{align}<br>\begin{bmatrix}<br>s_1^t\\<br>s_2^t\\<br>.\.\\<br>s_n^t\\<br>\end{bmatrix}=f(<br>\begin{bmatrix}<br>u_{11} u_{12} … u_{1m}\\<br>u_{21} u_{22} … u_{2m}\\<br>.\.\\<br>u_{n1} u_{n2} … u_{nm}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>.\.\\<br>x_m\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>w_{11} w_{12} … w_{1n}\\<br>w_{21} w_{22} … w_{2n}\\<br>.\.\\<br>w_{n1} w_{n2} … w_{nn}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>s_1^{t-1}\\<br>s_2^{t-1}\\<br>.\.\\<br>s_n^{t-1}\\<br>\end{bmatrix})…..(19)<br>\end{align}<br>$$<br>​    在这里我们用<strong>手写体字母</strong>表示向量的一个<strong>元素</strong>，它的下标表示它是这个向量的第几个元素，它的上标表示第几个<strong>时刻</strong>。例如，$s_t^{j}$表示向量s的第j个元素在t时刻的值。$u_{ji}$表示<strong>输入层</strong>第i个神经元到<strong>循环层</strong>第j个神经元的权重。$w_{ji}$表示<strong>循环层</strong>第t-1时刻的第i个神经元到<strong>循环层</strong>第t个时刻的第j个神经元的权重。</p>
<h4 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h4><p>​    BTPP算法将第l层t时刻的<strong>误差项</strong>$\delta_t^l $值沿两个方向传播，一个方向是其传递到上一层网络，得到，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始时刻，得到，这部分只和权重矩阵W有关。</p>
<p>我们用向量表示神经元在t时刻的<strong>加权输入</strong>，因为：</p>
<p><img src="/2018/04/08/tf8/image6.png" alt="6"></p>
<p>其中，$X_t$为输入，A为模型处理部分，$h_t$为输出。</p>
<p>为了更容易地说明递归神经网络，我们把上图展开，得到： </p>
<p><img src="/2018/04/08/tf8/image7.png" alt="7"></p>
<p>​    这样的一条链状神经网络代表了一个递归神经网络，可以认为它是对相同神经网络的多重复制，每一时刻的神经网络会传递信息给下一时刻。如何理解它呢？假设有这样一个语言模型，我们要根据句子中已出现的词预测当前词是什么，递归神经网络的工作原理如下： </p>
<p><img src="/2018/04/08/tf8/image8.png" alt="8"></p>
<p>其中，W为各类权重，x表示输入，y表示输出，h表示隐层处理状态。</p>
<p>递归神经网络因为具有一定的记忆功能，可以被用来解决很多问题，例如：语音识别、语言模型、机器翻译等。但是它并不能很好地处理长时依赖问题。</p>
<p><strong>长时依赖问题</strong> </p>
<p>​    RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p>
<p>​    有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p>
<p><img src="/2018/04/08/tf8/image9.png" alt="9"></p>
<p>​                        不太长的相关信息和位置间隔</p>
<p>​    但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。</p>
<p>​    不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p>
<p><img src="/2018/04/08/tf8/image10.png" alt="10"></p>
<p>​    在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="https://link.jianshu.com?t=http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。</p>
<p>​    隐含内容和输出结果是相同的内容。</p>
<p>​    TensorFlow 实现 RNN Cell 的位置在 python/ops/rnn_cell_impl.py，首先其实现了一个 RNNCell 类，继承了 Layer 类，其内部有三个比较重要的方法，state_size()、output_size()、<strong>call</strong>() 方法，其中 state_size() 和 output_size() 方法设置为类属性，可以当做属性来调用，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""size(s) of state(s) used by this cell.</span></span><br><span class="line"><span class="string">It can be represented by an Integer, a TensorShape or a tuple of Integers</span></span><br><span class="line"><span class="string">or TensorShapes.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""Integer or TensorShape: size of outputs produced by this cell."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br></pre></td></tr></table></figure>
<p>分别代表 Cell 的状态和输出维度，和 Cell 中的神经元数量有关，但这里两个方法都没有实现，意思是说我们必须要实现一个子类继承 RNNCell 类并实现这两个方法。</p>
<p>另外对于 <strong>call</strong>() 方法，实际上就是当初始化的对象直接被调用的时候触发的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(scope,</span><br><span class="line">                               custom_getter=self._rnn_get_variable) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state, scope=scope)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(vs.get_variable_scope(),</span><br><span class="line">                               custom_getter=self._rnn_get_variable):</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state)</span><br></pre></td></tr></table></figure>
<p>​    实际上是调用了父类 Layer 的 <strong>call</strong>() 方法，但父类中 <strong>call</strong>() 方法中又调用了 call() 方法，而 Layer 类的 call() 方法的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure>
<p>父类的 call() 方法实现非常简单，所以要实现其真正的功能，只需要在继承 RNNCell 类的子类中实现 call() 方法即可。</p>
<p>接下来我们看下 RNN Cell 的最基本的实现，叫做 BasicRNNCell，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicRNNCell</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">  <span class="string">"""The most basic RNN cell.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the RNN cell.</span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, activation=None, reuse=None)</span>:</span></span><br><span class="line">    super(BasicRNNCell, self).__init__(_reuse=reuse)</span><br><span class="line">    self._num_units = num_units</span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line">    self._linear = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">    <span class="string">"""Most basic RNN: output = new_state = act(W * input + U * state + B)."""</span></span><br><span class="line">    <span class="keyword">if</span> self._linear <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      self._linear = _Linear([inputs, state], self._num_units, <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    output = self._activation(self._linear([inputs, state]))</span><br><span class="line">    <span class="keyword">return</span> output, output</span><br></pre></td></tr></table></figure>
<p>可以看到在初始化的时候，最重要的一个参数是 num_units，意思就是这个 Cell 中神经元的个数，另外还有一个参数 activation 即默认使用的激活函数，默认使用的 tanh，reuse 代表该 Cell 是否可以被重新使用。</p>
<p>在 state_size()、output_size() 方法里，其返回的内容都是 num_units，即神经元的个数，接下来 call() 方法中，传入的参数为 inputs 和 state，即输入的 x 和 上一次的隐含状态，首先实例化了一个 _Linear 类，这个类实际上就是做线性变换的类，将二者传递过来，然后直接调用，就实现了 w * [inputs, state] + b 的线性变换，其中 _Linear 类的 <strong>call</strong>() 方法实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, args)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._is_sequence:</span><br><span class="line">        args = [args]</span><br><span class="line">    <span class="keyword">if</span> len(args) == <span class="number">1</span>:</span><br><span class="line">        res = math_ops.matmul(args[<span class="number">0</span>], self._weights)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res = math_ops.matmul(array_ops.concat(args, <span class="number">1</span>), self._weights)</span><br><span class="line">    <span class="keyword">if</span> self._build_bias:</span><br><span class="line">        res = nn_ops.bias_add(res, self._biases)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>​    很明显这里传递了 [inputs, state] 作为 <strong>call</strong>() 方法的 args，会执行 concat() 和 matmul() 方法，然后接着再执行 bias_add() 方法，这样就实现了线性变换。</p>
<p>​    最后回到 BasicRNNCell 的 call() 方法中，在 _linear() 方法外面又包括了一层 _activation() 方法，即对线性变换应用一次 tanh 激活函数处理，作为输出结果。</p>
<p>​    最后返回的结果是 output 和 output，第一个代表 output，第二个代表隐状态，其值也等于 output。</p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[tf7]]></title>
      <url>/2018/04/07/tf7/</url>
      <content type="html"><![CDATA[<p>过拟合</p>
<p>解决办法：</p>
<p>​    方法一: 增加数据量, 大部分过拟合产生的原因是因为数据量太少了. 如果我们有成千上万的数据, 红线也会慢慢被拉直, 变得没那么扭曲 . </p>
<p>​    方法二:运用正规化. L1, l2 regularization等等, 这些方法适用于大多数的机器学习, 包括神经网络. 他们的做法大同小异, 我们简化机器学习的关键公式为 y=Wx 。 W为机器需要学习到的各种参数。在过拟合中, W 的值往往变化得特别大或特别小。为了不让W变化太大, 我们在计算误差上做些手脚. 原始的 cost 误差是这样计算, cost = 预测值-真实值的平方. 如果 W 变得太大, 我们就让 cost 也跟着变大, 变成一种惩罚机制. 所以我们把 W 自己考虑进来. 这里 abs 是绝对值. 这一种形式的<strong>正规化</strong>, 叫做 l1 正规化。L2 正规化和 l1 类似, 只是绝对值换成了平方. 其他的l3, l4 也都是换成了立方和4次方等等. 形式类似. 用这些方法,我们就能保证让学出来的线条不会过于扭曲.<br>$$<br>l1,l2…regularization\\<br>y=Wx\\<br>L1:cost=(Wx-real \, y)^2+abs(W)\\<br>L2:cost=(Wx-real \, y)^2+(W)^2<br>$$<br>​    还有一种专门用在神经网络的正规化的方法, 叫作 dropout。在训练的时候, 我们随机忽略掉一些神经元和神经联结 , 是这个神经网络变得”不完整”. 用一个不完整的神经网络训练一次。到第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象其实每次训练的时候, 我们都让每一次预测结果都不会依赖于其中某部分特定的神经元. 像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数。Dropout 的做法是从根本上让神经网络没机会过度依赖。</p>
<p>​    举个Regression (回归)的例子。</p>
<p><img src="/2018/04/07/tf7/image1.png" alt="1"></p>
<p>​    第三条曲线存在overfitting问题，尽管它经过了所有的训练点，但是不能很好的反应数据的趋势，预测能力严重不足。 TensorFlow提供了强大的dropout方法来解决overfitting问题。</p>
<h2 id="建立-dropout-层"><a href="#建立-dropout-层" class="headerlink" title="建立 dropout 层"></a>建立 dropout 层</h2>]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[tensorboard可视化]]></title>
      <url>/2018/04/06/tf6/</url>
      <content type="html"><![CDATA[<p>​    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了更方便 TensorFlow 程序的理解、调试与优化，有一套叫做 TensorBoard 的可视化工具。你可以用 TensorBoard 来展现你的 TensorFlow 图像，绘制图像生成的定量指标图以及附加数据。TensorBoard 通过读取 TensorFlow 的事件文件来运行。TensorFlow 的事件文件包括了你会在 TensorFlow 运行中涉及到的主要数据。。显示的神经网络差不多是这样的：</p>
<a id="more"></a>
<p><img src="/2018/04/06/tf6/image1.PNG" alt="1"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时我们也可以展开看每个layer中的一些具体的结构：</p>
<p><img src="/2018/04/06/tf6/image2.PNG" alt="1"></p>
<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化器optimizer</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer'</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">            Weights = tf.Variable(tf.random_normal([insize,outsize]), name=<span class="string">'W'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">            biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>, name=<span class="string">'b'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">            Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line">y = np.random.random(y_data.shape)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</span><br><span class="line">    xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">'x_input'</span>)</span><br><span class="line">    ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">'y_input'</span>)</span><br><span class="line"><span class="comment"># add hidden layer</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line"><span class="comment"># add output layer</span></span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 这里对矩阵按行求和没有什么作用，因为数据每行只有一个实例，还是它本身</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                          reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"logs/"</span>, sess.graph)</span><br><span class="line">    sess.run(init)</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;执行完成会在logs文件夹下生成一个事件文件。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;打开终端，切换到py文件的目录，输入：tensorboard –logdir=./logs</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意：</strong>文件的路径一定要加“.” ,否则会找不到那个事件文件</p>
<p>在浏览器打开终端显示的链接：<a href="http://DESKTOP-RKATBUI:6006" target="_blank" rel="noopener">http://DESKTOP-RKATBUI:6006</a></p>
<p>即可看到tensorboard的界面,如下。</p>
<p><img src="/2018/04/06/tf6/image3.PNG" alt="3"></p>
<p>参考：</p>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'tensorboard可视化',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tensorflow增加层和数据可视化]]></title>
      <url>/2018/04/06/tf5/</url>
      <content type="html"><![CDATA[<p>添加层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span></span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([insize,outsize]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>完整的一个小例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([insize,outsize]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line">y = np.random.random(y_data.shape)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 这里对矩阵按行求和没有什么作用，因为数据每行只有一个实例，还是它本身</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                      reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br></pre></td></tr></table></figure>
<p>数据可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([insize,outsize]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line">y = np.random.random(y_data.shape)</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 这里对矩阵按行求和没有什么作用，因为数据每行只有一个实例，还是它本身</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                      reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ax.scatter(x_data, y_data)</span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                ax.lines.remove(lines[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)</span><br><span class="line">            lines = ax.plot(x_data, prediction_value, <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">            <span class="comment"># ax.lines.remove(lines[0])</span></span><br><span class="line">            plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'tensorflow增加层和数据可视化',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf.placeholder 与 tf.Variable区别]]></title>
      <url>/2018/04/06/tf4/</url>
      <content type="html"><![CDATA[<p>二者的主要区别在于：</p>
<p><strong>tf.Variable</strong>：主要在于一些可训练变量（trainable variables），比如模型的权重（weights，W）或者偏置值（bias）；</p>
<p><strong>声明时，必须提供初始值；<em>**</em></strong></p>
<p>名称的真实含义，在于变量，也即在真实训练时，其值是会改变的，自然事先需要指定初始值； </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable( tf.truncated_normal([IMAGE_PIXELS, 		                               hidden1_units],stddev=<span class="number">1.</span>/math.sqrt(float(IMAGE_PIXELS)), name=<span class="string">'weights'</span>))</span><br><span class="line">biases = tf.Variable(tf.zeros([hidden1_units]), name=<span class="string">'biases'</span>)</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p><strong>tf.placeholder</strong>：用于得到传递进来的真实的训练样本：</p>
<p><strong>不必指定初始值</strong>，可在运行时，通过 Session.run 的函数的 feed_dict 参数指定；这也是其命名的原因所在，仅仅作为一种占位符；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">images_placeholder = tf.placeholder(tf.float32, shape=[batch_size, IMAGE_PIXELS])</span><br><span class="line">labels_placeholder = tf.placeholder(tf.int32, shape=[batch_size])</span><br></pre></td></tr></table></figure>
<p>如下则是二者真实的使用场景：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(FLAGS.max_steps):</span><br><span class="line">    feed_dict = &#123;</span><br><span class="line">        images_placeholder = images_feed,</span><br><span class="line">        labels_placeholder = labels_feed</span><br><span class="line">    &#125;</span><br><span class="line">    _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)</span><br></pre></td></tr></table></figure>
<p>当执行这些操作时，tf.Variable 的值将会改变，也即被修改，这也是其名称的来源（variable，变量）。</p>
<p>那么，什么时候该用tf.placeholder，什么时候该使用tf.Variable之类直接定义参数呢？</p>
<p>答案是，<strong>tf.Variable适合一些需要初始化或被训练而变化的权重或参数，而tf.placeholder适合通常不会改变的被训练的数据集。</strong></p>
<p>小例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">output = tf.multiply(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(output,  feed_dict=&#123;input1: <span class="number">7.</span>, input2: <span class="number">2.</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">14.0</span></span><br></pre></td></tr></table></figure>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'tf.placeholder 与 tf.Variable区别',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tensorflow-变量]]></title>
      <url>/2018/04/04/tf3/</url>
      <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;训练模型时，需要使用<strong>变量(Variables)</strong>保存和更新参数。Variables是包含张量(tensor)的内存缓冲。变量必须要先被<strong>初始化(initialize)</strong>，而且可以在训练时和训练后<strong>保存(save)</strong>到磁盘中。之后可以再<strong>恢复(restore)</strong>保存的变量值来训练和测试模型。 </p>
<h1 id="1-创建-Creation"><a href="#1-创建-Creation" class="headerlink" title="1.创建(Creation)"></a>1.创建(Creation)</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建Variable，需将一个tensor传递给Variable()构造函数。可以使用TensorFlow提供的许多ops(操作)初始化张量，参考<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html" target="_blank" rel="noopener">constants or random values</a>。这些ops都要求指定tensor的<strong>shape(形状)</strong>。比如</p>
<blockquote>
<h4 id="Create-two-variables"><a href="#Create-two-variables" class="headerlink" title="Create two variables."></a>Create two variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), </span><br><span class="line">&gt; name=”weights”) </span><br><span class="line">&gt; biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=”biases”)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
</blockquote>
<p>&nbsp;<a id="more"></a></p>
<p>&nbsp;调用tf.Variable()函数在graph中增加以下几个ops:<br>- 一个<code>Variable</code> op ,负责保存变量值。<br>- 一个<code>initializer</code> op,负责将变量设为初始值，这实际是<code>tf.assign</code> op。<br>- 初始值的op，比如<code>zeros</code> op 。</p>
<p><code>tf.Variable()</code>返回一个<code>tf.Variable</code>类的实例。</p>
<p>使用tf.Variable()时，如果系统检测到重名，会做自动处理，不会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w_1 = tf.Variable(<span class="number">3</span>,name=<span class="string">"w"</span>)</span><br><span class="line">w_2 = tf.Variable(<span class="number">1</span>,name=<span class="string">"w"</span>)</span><br><span class="line">print(w_1.name)</span><br><span class="line">print(w_2.name)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w:<span class="number">0</span></span><br><span class="line">w_1:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<h1 id="2-设备安置-Device-placement"><a href="#2-设备安置-Device-placement" class="headerlink" title="2.设备安置(Device placement)"></a>2.设备安置(Device placement)</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 with tf.device(…): block，将一个变量安置在一个设备上。</p>
<p>Pin a variable to CPU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(“/cpu:<span class="number">0</span>”): </span><br><span class="line">	v = tf.Variable(…)</span><br></pre></td></tr></table></figure>
<p>Pin a variable to GPU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(“/gpu:<span class="number">0</span>”): </span><br><span class="line">	v = tf.Variable(…)</span><br></pre></td></tr></table></figure>
<p>Pin a variable to a particular parameter server task.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(“/job:ps/task:<span class="number">7</span>”): </span><br><span class="line">v = tf.Variable(…)</span><br></pre></td></tr></table></figure>
<p>改变变量的一些ops,比如<code>v.assign()</code>和<code>tf.train.Optimizer</code>需要与变量在同一个设备上。</p>
<h1 id="3-初始化-Initialization"><a href="#3-初始化-Initialization" class="headerlink" title="3.初始化(Initialization)"></a>3.初始化(Initialization)</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在运行模型中其他操作之前，必须先对变量进行初始化。最简单的初始化方法是添加一个对所有变量进行初始化的op,然后再使用model前运行此op。</p>
<h2 id="3-1全局初始化"><a href="#3-1全局初始化" class="headerlink" title="3.1全局初始化"></a>3.1全局初始化</h2><p>使用<code>tf.global_variables_initializer()</code>添加一个op来运行初始化。要在完全构建完模型后，在一个对话(Session)中运行它。</p>
<blockquote>
<h4 id="Create-two-variables-1"><a href="#Create-two-variables-1" class="headerlink" title="Create two variables."></a>Create two variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=”weights”) </span><br><span class="line">&gt; biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=”biases”) </span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Add-an-op-to-initialize-the-variables"><a href="#Add-an-op-to-initialize-the-variables" class="headerlink" title="Add an op to initialize the variables."></a>Add an op to initialize the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; init_op = tf.global_variables_initializer()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Later-when-launching-the-model"><a href="#Later-when-launching-the-model" class="headerlink" title="Later, when launching the model"></a>Later, when launching the model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">&gt; <span class="comment"># Run the init operation. </span></span><br><span class="line">&gt; sess.run(init_op) </span><br><span class="line">&gt; … </span><br><span class="line">&gt; <span class="comment"># Use the model </span></span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
</blockquote>
<h2 id="3-2-用其他变量值创建变量"><a href="#3-2-用其他变量值创建变量" class="headerlink" title="3.2 用其他变量值创建变量"></a>3.2 用其他变量值创建变量</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用变量A的值初始化另一个变量B，需使用变量A的属性(property)<code>initialized_value()</code>。可以直接使用变量A的初始值，也可以用之计算新的值。</p>
<p>Create a variable with a random value.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=”weights”)</span><br></pre></td></tr></table></figure>
<p>Create another variable with the same value as ‘weights’.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w2 = tf.Variable(weights.initialized_value(), name=”w2”)</span><br></pre></td></tr></table></figure>
<p>Create another variable with twice the value of ‘weights’</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">2.0</span>, name=”w_twice”)</span><br></pre></td></tr></table></figure>
<h1 id="4-保存和恢复Saving-and-Restoring"><a href="#4-保存和恢复Saving-and-Restoring" class="headerlink" title="4.保存和恢复Saving and Restoring"></a>4.保存和恢复Saving and Restoring</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方法是用<code>tf.train.Saver</code>对象，此构造函数在graph中为所有变量(or a specified list)添加save和restore ops。saver对象提供运行这些ops的方法，并指定读写checkpoint files的路径。</p>
<h2 id="4-1-checkpoint文件"><a href="#4-1-checkpoint文件" class="headerlink" title="4.1 checkpoint文件"></a>4.1 checkpoint文件</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变量存储在一个<strong>二进制</strong>文件中，包含从变量名称到张量值的映射。</p>
<p>创建checkpoint files时，可以选择性地选择变量名称来保存。默认情况，它使用每个Variable的<code>Variable.name</code>属性。</p>
<p>可以使用<a href="https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/python/tools/inspect_checkpoint.py" target="_blank" rel="noopener">inspect_checkpoint</a>库查看checkpoint file中的变量，还有<code>print_tensrs_in_checkpoint_file</code>函数。</p>
<h1 id="4-2保存变量"><a href="#4-2保存变量" class="headerlink" title="4.2保存变量"></a>4.2保存变量</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用<code>tf.train.Saver()</code>创建一个<code>Saver</code>对象来管理模型中所有变量。</p>
<blockquote>
<h4 id="Create-some-variables"><a href="#Create-some-variables" class="headerlink" title="Create some variables."></a>Create some variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; v1 = tf.Variable(…, name=”v1”) </span><br><span class="line">&gt; v2 = tf.Variable(…, name=”v2”) </span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Add-an-op-to-initialize-the-variables-1"><a href="#Add-an-op-to-initialize-the-variables-1" class="headerlink" title="Add an op to initialize the variables."></a>Add an op to initialize the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; init_op = tf.global_variables_initializer()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Add-ops-to-save-and-restore-all-the-variables"><a href="#Add-ops-to-save-and-restore-all-the-variables" class="headerlink" title="Add ops to save and restore all the variables."></a>Add ops to save and restore all the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver = tf.train.Saver()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Later-launch-the-model-initialize-the-variables-do-some-work-save-the-variables-to-disk"><a href="#Later-launch-the-model-initialize-the-variables-do-some-work-save-the-variables-to-disk" class="headerlink" title="Later, launch the model, initialize the variables, do some work, save the variables to disk."></a>Later, launch the model, initialize the variables, do some work, save the variables to disk.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">&gt; 	sess.run(init_op)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Do-some-work-with-the-model"><a href="#Do-some-work-with-the-model" class="headerlink" title="Do some work with the model."></a>Do some work with the model.</h4><p>..</p>
<h4 id="Save-the-variables-to-disk"><a href="#Save-the-variables-to-disk" class="headerlink" title="Save the variables to disk."></a>Save the variables to disk.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; save_path = saver.save(sess, “/tmp/model.ckpt”) </span><br><span class="line">&gt; print(“Model saved <span class="keyword">in</span> file: %s” % save_path)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
</blockquote>
<p>先初始化变量，再操作模型，最后保存变量。</p>
<h2 id="4-3恢复变量"><a href="#4-3恢复变量" class="headerlink" title="4.3恢复变量"></a>4.3恢复变量</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用同样的Saver对象恢复变量，恢复变量时，就不用先初始化变量了。</p>
<blockquote>
<h4 id="Create-some-variables-1"><a href="#Create-some-variables-1" class="headerlink" title="Create some variables."></a>Create some variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; v1 = tf.Variable(…, name=”v1”) </span><br><span class="line">&gt; v2 = tf.Variable(…, name=”v2”) </span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Add-ops-to-save-and-restore-all-the-variables-1"><a href="#Add-ops-to-save-and-restore-all-the-variables-1" class="headerlink" title="Add ops to save and restore all the variables."></a>Add ops to save and restore all the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver = tf.train.Saver()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Later-launch-the-model-use-the-saver-to-restore-variables-from-disk-and"><a href="#Later-launch-the-model-use-the-saver-to-restore-variables-from-disk-and" class="headerlink" title="Later, launch the model, use the saver to restore variables from disk, and"></a>Later, launch the model, use the saver to restore variables from disk, and</h4><h4 id="do-some-work-with-the-model"><a href="#do-some-work-with-the-model" class="headerlink" title="do some work with the model."></a>do some work with the model.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Restore-variables-from-disk"><a href="#Restore-variables-from-disk" class="headerlink" title="Restore variables from disk."></a>Restore variables from disk.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver.restore(sess, “/tmp/model.ckpt”) </span><br><span class="line">&gt; print(“Model restored.”)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Do-some-work-with-the-model-1"><a href="#Do-some-work-with-the-model-1" class="headerlink" title="Do some work with the model"></a>Do some work with the model</h4><p>…</p>
</blockquote>
<p>无初始化操作，先恢复变量，再操模型。</p>
<h1 id="4-4选择保存和恢复的变量"><a href="#4-4选择保存和恢复的变量" class="headerlink" title="4.4选择保存和恢复的变量"></a>4.4选择保存和恢复的变量</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果不给<code>tf.train.Saver</code>传递任何参数，Saver会在graph中处理所有变量。每个变量会存在他们创建时的name下。</p>
<p>通过给<code>tf.train.Saver</code>传递一个Python字典，可以指定保存变量的name。key是要在checkpoint file中使用的name, values指要管理的变量。</p>
<h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h3><ul>
<li>可以创建多个saver，分别保存不同的变量集合。</li>
<li>如果在对话开始时，只恢复了部分变量，就要对其他变量运行initializer op。参考<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#variables_initializer" target="_blank" rel="noopener">tf.variables_initializer() </a></li>
</ul>
<blockquote>
<h4 id="Create-some-variables-2"><a href="#Create-some-variables-2" class="headerlink" title="Create some variables."></a>Create some variables.</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; v1 = tf.Variable(…, name=”v1”) </span><br><span class="line">&gt; v2 = tf.Variable(…, name=”v2”) </span><br><span class="line">&gt; … </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Add-ops-to-save-and-restore-only-‘v2’-using-the-name-“my-v2”"><a href="#Add-ops-to-save-and-restore-only-‘v2’-using-the-name-“my-v2”" class="headerlink" title="Add ops to save and restore only ‘v2’ using the name “my_v2”"></a>Add ops to save and restore only ‘v2’ using the name “my_v2”</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver = tf.train.Saver(&#123;“my_v2”: v2&#125;) </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<h4 id="Use-the-saver-object-normally-after-that"><a href="#Use-the-saver-object-normally-after-that" class="headerlink" title="Use the saver object normally after that."></a>Use the saver object normally after that.</h4></blockquote>
<p>一个小例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line">print(state.name)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer() <span class="comment"># must have if define variable</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a href="https://blog.csdn.net/muyiyushan/article/details/65442052" target="_blank" rel="noopener">https://blog.csdn.net/muyiyushan/article/details/65442052</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'tensorflow-变量',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tensorflow-会话（session）]]></title>
      <url>/2018/04/04/tf2/</url>
      <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tensorflow中的会话是来执行定义好的运算的。会话拥有并管理Tensorflow程序运行时的所有资源。当计算完成之后需要关闭会话来帮助系统回收资源，否则可能出现资源泄露的问题。 Tensorflow中使用会话的模式一般有两种，第一种模式需要明确调用会话生成函数和关闭会话函数，流程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">matrix1 =  tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># 输出：[[12]]</span></span><br></pre></td></tr></table></figure>
<p>​    <a id="more"></a></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用这种模式时，所有计算完成后，需要明确调用Session.close()函数关闭会话并释放资源。然而当程序因为异常而退出时，关闭会话的函数可能就不会被执行而导致资源泄露。为了解决异常退出导致资源泄露的问题，Tensorflow可以通过Python的上下文管理器来使用会话。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">matrix1 =  tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method2,sess会自动关闭</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result2 = sess.run(product)</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过Python的上下文管理器的机制，只要所有的计算放在with的内部就可以。当上下文管理器退出时会自动释放所有资源。</p>
<h2 id="上下文管理器"><a href="#上下文管理器" class="headerlink" title="上下文管理器"></a>上下文管理器</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在使用Python编程中，可以会经常碰到这种情况：有一个特殊的语句块，在执行这个语句块之前需要先执行一些准备动作；当语句块执行完成后，需要继续执行一些收尾动作。</p>
<p>例如：当需要操作文件或数据库的时候，首先需要获取文件句柄或者数据库连接对象，当执行完相应的操作后，需要执行释放文件句柄或者关闭数据库连接的动作。</p>
<p>又如，当多线程程序需要访问临界资源的时候，线程首先需要获取互斥锁，当执行完成并准备退出临界区的时候，需要释放互斥锁。</p>
<p>​    对于这些情况，Python中提供了<strong>上下文管理器（Context Manager）</strong>的概念，可以通过上下文管理器来定义/控制代码块执行前的准备动作，以及执行后的收尾动作。</p>
<h2 id="上下文管理协议"><a href="#上下文管理协议" class="headerlink" title="上下文管理协议"></a>上下文管理协议</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么在Python中怎么实现一个上下文管理器呢？这里，又要提到两个”魔术方法”__enter__和__exit__，下面就是关于这两个方法的具体介绍。</p>
<ul>
<li><p><strong>enter</strong>(self) Defines what the context manager should do at the beginning of the block created by the with statement. Note that the return value of <strong>enter</strong> is bound to the target of the with statement, or the name after the as.</p>
</li>
<li><p><strong>exit</strong>(self, exception_type, exception_value, traceback) Defines what the context manager should do after its block has been executed (or terminates). It can be used to handle exceptions, perform cleanup, or do something always done immediately after the action in the block. If the block executes successfully, exception_type, exception_value, and traceback will be None. Otherwise, you can choose to handle the exception or let the user handle it; if you want to handle it, make sure <strong>exit</strong> returns True after all is said and done. If you don’t want the exception to be handled by the context manager, just let it happen.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;也就是说，当我们需要创建一个上下文管理器类型的时候，就需要实现<strong>enter</strong>和<strong>exit</strong>方法，这对方法就称为<strong>上下文管理协议（Context Manager Protocol）</strong>，定义了一种运行时上下文环境。</p>
</li>
</ul>
<h2 id="with语句"><a href="#with语句" class="headerlink" title="with语句"></a>with语句</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Python中，可以通过with语句来方便的使用上下文管理器，with语句可以在代码块运行前进入一个运行时上下文（执行__enter__方法），并在代码块结束后退出该上下文（执行__exit__方法）。</p>
<p>with语句的语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> context_expr [<span class="keyword">as</span> var]:</span><br><span class="line">    with_suite</span><br></pre></td></tr></table></figure>
<ul>
<li>context_expr是支持上下文管理协议的对象，也就是上下文管理器对象，负责维护上下文环境</li>
<li>as var是一个可选部分，通过变量方式保存上下文管理器对象</li>
<li>with_suite就是需要放在上下文环境中执行的语句块</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Python的内置类型中，很多类型都是支持上下文管理协议的，例如file，thread.LockType，threading.Lock等等。这里我们就以file类型为例，看看with语句的使用。</p>
<h3 id="with语句简化文件操作"><a href="#with语句简化文件操作" class="headerlink" title="with语句简化文件操作"></a>with语句简化文件操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要写一个文件的时候，一般都会通过下面的方式。代码中使用了try-finally语句块，即使出现异常，也能保证关闭文件句柄。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">logger = open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    logger.write(<span class="string">'Hello '</span>)</span><br><span class="line">    logger.write(<span class="string">'World'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    logger.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> logger.closed</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实，Python的内置file类型是支持上下文管理协议的，可以直接通过内建函数dir()来查看file支持的方法和属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> dir(file)</span><br><span class="line">[<span class="string">'__class__'</span>, <span class="string">'__delattr__'</span>, <span class="string">'__doc__'</span>, <span class="string">'__enter__'</span>, <span class="string">'__exit__'</span>, <span class="string">'__format__'</span>, <span class="string">'</span></span><br><span class="line"><span class="string">__getattribute__'</span>, <span class="string">'__hash__'</span>, <span class="string">'__init__'</span>, <span class="string">'__iter__'</span>, <span class="string">'__new__'</span>, <span class="string">'__reduce__'</span>,</span><br><span class="line"><span class="string">'__reduce_ex__'</span>, <span class="string">'__repr__'</span>, <span class="string">'__setattr__'</span>, <span class="string">'__sizeof__'</span>, <span class="string">'__str__'</span>, <span class="string">'__subclass</span></span><br><span class="line"><span class="string">hook__'</span>, <span class="string">'close'</span>, <span class="string">'closed'</span>, <span class="string">'encoding'</span>, <span class="string">'errors'</span>, <span class="string">'fileno'</span>, <span class="string">'flush'</span>, <span class="string">'isatty'</span>, <span class="string">'</span></span><br><span class="line"><span class="string">mode'</span>, <span class="string">'name'</span>, <span class="string">'newlines'</span>, <span class="string">'next'</span>, <span class="string">'read'</span>, <span class="string">'readinto'</span>, <span class="string">'readline'</span>, <span class="string">'readlines'</span>,</span><br><span class="line"><span class="string">'seek'</span>, <span class="string">'softspace'</span>, <span class="string">'tell'</span>, <span class="string">'truncate'</span>, <span class="string">'write'</span>, <span class="string">'writelines'</span>, <span class="string">'xreadlines'</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，可以通过with语句来简化上面的代码，代码的效果是一样的，但是使用with语句的代码更加的简洁：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> logger:</span><br><span class="line">    logger.write(<span class="string">'Hello '</span>)</span><br><span class="line">    logger.write(<span class="string">'World'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">print</span> logger.closed</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a href="https://www.cnblogs.com/wilber2013/p/4638967.html" target="_blank" rel="noopener">https://www.cnblogs.com/wilber2013/p/4638967.html</a></p>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'tensorflow-会话（session）',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>































]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[tf1]]></title>
      <url>/2018/04/04/tf1/</url>
      <content type="html"><![CDATA[<h2 id="TensorFlow-简介"><a href="#TensorFlow-简介" class="headerlink" title="TensorFlow 简介"></a>TensorFlow 简介</h2><p>​    TensorFlow™ 是一个使用数据流图进行数值计算的开源软件库。图中的节点代表数学运算， 而图中的边则代表在这些节点之间传递的多维数组（张量）。这种灵活的架构可让您使用一个 API 将计算工作部署到桌面设备、服务器或者移动设备中的一个或多个 CPU 或 GPU。 TensorFlow 最初是由 Google 机器智能研究部门的 Google Brain 团队中的研究人员和工程师开发的，用于进行机器学习和深度神经网络研究， 但它是一个非常基础的系统，因此也可以应用于众多其他领域。</p>
<p>​    TensorFlow是开源数学计算引擎，由Google创造，用Apache 2.0协议发布。TF的API是Python的，但底层是C++。和Theano不同，TF兼顾了工业和研究，在RankBrain、DeepDream等项目中使用。TF可以在单个CPU或GPU，移动设备以及大规模分布式系统中使用。</p>
<a id="more"></a>
<p>TF的计算是用<strong>图</strong>表示的：</p>
<ul>
<li>节点：节点进行计算，有一个或者多个输入输出。节点间的数据叫张量：多维实数数组。</li>
<li>边缘：定义数据、分支、循环和覆盖的图，也可以进行高级操作，例如等待某个计算完成。</li>
<li>操作：取一个输入值，得出一个输出值，例如，加减乘除。</li>
</ul>
<p>使用 TensorFlow, 你必须明白 TensorFlow:</p>
<ul>
<li>使用图 (graph) 来表示计算任务.</li>
<li>在被称之为 <code>会话 (Session)</code> 的上下文 (context) 中执行图.</li>
<li>使用 tensor 表示数据，每个 Tensor 是一个类型化的多维数组</li>
<li>通过 <code>变量 (Variable)</code> 维护状态.</li>
<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据。</li>
</ul>
<p>​    TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op.</p>
<p>一个简单例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># create data,100维的列表</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">print(x_data.shape)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### create tensorflow structure start ###</span></span><br><span class="line"><span class="comment"># 创建一个给定类型的数组，将其填充在一个均匀分布的随机样本[0, 1)中</span></span><br><span class="line">Weights = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">y = Weights * x_data + biases</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment">### create tensorflow structure end ###</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)          <span class="comment"># Very important</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, sess.run(Weights), sess.run(biases))</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/basic_usage.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/basic_usage.html</a></p>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'linux-1:源码安装',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>



























]]></content>
      
        <categories>
            
            <category> tensorflow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[dl2]]></title>
      <url>/2018/04/02/dl2/</url>
      <content type="html"><![CDATA[<h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：</p>
<p><img src="/2018/04/02/dl2/image1.png" alt="1"></p>
<p>​    计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量$x$，权重向量$w$是(偏置项是$w_0$)，激活函数是sigmoid函数，则其输出$y$：<br>$$<br>y=sigmioid(w^T  x )<br>$$<br>sigmoid函数的定义如下：<br>$$<br>sigmoid(x)={1\over 1+e^{-x}}<br>$$<br>将其代入前面的式子，得到<br>$$<br>y={1\over 1+e^{-w^Tx}}<br>$$<br>sigmoid函数是一个非线性函数，值域是(0,1)。函数图像如下图所示</p>
<p><img src="/2018/04/02/dl2/image2.jpg" alt="2"></p>
<p>sigmoid函数的导数是：<br>$$<br>y=sigmoid(x)…………(1)\\<br>y’=y(1-y)……………(2)<br>$$</p>
<h2 id="神经网络是啥"><a href="#神经网络是啥" class="headerlink" title="神经网络是啥"></a>神经网络是啥</h2><p><img src="/2018/04/02/dl2/image3.jpeg" alt="3"></p>
<p>​    神经网络其实就是按照<strong>一定规则</strong>连接起来的多个<strong>神经元</strong>。上图展示了一个<strong>全连接(full connected, FC)</strong>神经网络，通过观察上面的图，我们可以发现它的规则包括：</p>
<ul>
<li><p>神经元按照<strong>层</strong>来布局。最左边的层叫做<strong>输入层</strong>，负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>，因为它们对于外部来说是不可见的。</p>
</li>
<li><p>同一层的神经元之间没有连接。</p>
</li>
<li><p>第N层的每个神经元和第N-1层的<strong>所有</strong>神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。</p>
</li>
<li><p>每个连接都有一个<strong>权值</strong>。</p>
<p>上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p>
</li>
</ul>
<h2 id="计算神经网络的输出"><a href="#计算神经网络的输出" class="headerlink" title="计算神经网络的输出"></a>计算神经网络的输出</h2><p>神经网络实际上就是一个输入向量$x$到输出向量$y$的函数，即：<br>$$<br>y=f_{network}(x)<br>$$<br>​    根据输入计算神经网络的输出，需要首先将输入向量的每个元素的值赋给神经网络的输入层的对应神经元，然后根据<strong>式1</strong>依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量。</p>
<p>接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p>
<p><img src="/2018/04/02/dl2/image4.png" alt="4"></p>
<p>​    如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$w_{41},w_{42},w_{43}$。那么，我们怎样计算节点4的输出值呢$a_4$？</p>
<p>​    为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是<strong>输入层</strong>的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。我们要求<strong>输入向量的维度和输入层神经元个数相同</strong>，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把$x_1$赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p>
<p>​    一旦我们有了节点1、2、3的输出值，我们就可以根据<strong>式1</strong>计算节点4的输出值$a_4$<br>$$<br>a_4=sigmoid(w^Tx)………………..(3)\\<br>=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})…………..(4)<br>$$</p>
<p>上式的$w_{4b}$是节点4的<strong>偏置项</strong>，图中没有画出来。而$w_{41},w_{42},w_{43}$分别为节点1、2、3到节点4连接的权重，在给权重$w_{ji}$编号时，我们把目标节点的编号$j$放在前面，把源节点的编号$i$放在后面。</p>
<p>​    同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$：<br>$$<br>y_1=sigmoid(w^Ta)……………(5)\\<br>=sigmoid(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})……………(6)<br>$$<br>​    同理，我们还可以计算出的值$y_2$。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$时，神经网络的输出向量$\vec{y}=\begin{bmatrix}y_1\\y_2\end{bmatrix} $这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong></p>
<h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：<br>$$<br>a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\<br>a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\<br>a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\<br>a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\<br>$$<br>接着，定义网络的输入向量$x$和隐藏层每个节点的权重向量$w$。令<br>$$<br>\begin{align}<br>\vec{x}&amp;=\begin{bmatrix}x_1\\x_2\\x_3\\1\end{bmatrix}……..(7)\\<br>\vec{w}_4&amp;=[w_{41},w_{42},w_{43},w_{4b}……………….(8)]\\<br>\vec{w}_5&amp;=[w_{51},w_{52},w_{53},w_{5b}]…………………..(9)\\<br>\vec{w}_6&amp;=[w_{61},w_{62},w_{63},w_{6b}]……………………..(10)\\<br>\vec{w}_7&amp;=[w_{71},w_{72},w_{73},w_{7b}]………………(11)\\<br>f&amp;=sigmoid ………………..(12)<br>\end{align}<br>$$</p>
<p>代入到前面的一组式子，得到<br>$$<br>\begin{align}<br>a_4&amp;=f(\vec{w_4}\centerdot\vec{x})…………..(13)\\<br>a_5&amp;=f(\vec{w_5}\centerdot\vec{x})…………(14)\\<br>a_6&amp;=f(\vec{w_6}\centerdot\vec{x})…………(15)\\<br>a_7&amp;=f(\vec{w_7}\centerdot\vec{x})…………(16)<br>\end{align}<br>$$<br>​    现在，我们把上述计算$a_4,a_5,a_6,a_7$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令<br>$$<br>\vec{a}=<br>\begin{bmatrix}<br>a_4 \\<br>a_5 \\<br>a_6 \\<br>a_7 \\<br>\end{bmatrix},\qquad W=<br>\begin{bmatrix}<br>\vec{w}_4 \\<br>\vec{w}_5 \\<br>\vec{w}_6 \\<br>\vec{w}_7 \\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>w_{41},w_{42},w_{43},w_{4b} \\<br>w_{51},w_{52},w_{53},w_{5b} \\<br>w_{61},w_{62},w_{63},w_{6b} \\<br>w_{71},w_{72},w_{73},w_{7b} \\<br>\end{bmatrix}<br>,\qquad f(<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>x_3\\<br>.\\<br>.\\<br>.\\<br>\end{bmatrix})=<br>\begin{bmatrix}<br>f(x_1)\\<br>f(x_2)\\<br>f(x_3)\\<br>.\\<br>.\\<br>.\\<br>\end{bmatrix}<br>$$<br>带入前面的一组式子，得到<br>$$<br>\vec{a}=f(W\centerdot\vec{x})\qquad (式2)<br>$$</p>
<p>​    在<strong>式2</strong>中，$f$是激活函数，在本例中是sigmoid函数；是某一层的权重矩阵；$\vec x$是某层的输入向量；$\vec a$是某层的输出向量。<strong>式2</strong>说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p>
<p>​    每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$W_1,W_2,W_3,W_4$，每个隐藏层的输出分别是$\vec a_1,\vec a_2,\vec a_3$，神经网络的输入为$\vec x$，神经网络的输入为$\vec y$，如下图所示：</p>
<p><img src="/2018/04/02/dl2/image5.png" alt="5"></p>
<p>则每一层的输出向量的计算可以表示为：<br>$$<br>\begin{align}<br>&amp;\vec{a}_1=f(W_1\centerdot\vec{x})………(17)\\<br>&amp;\vec{a}_2=f(W_2\centerdot\vec{a}_1)………(18)\\<br>&amp;\vec{a}_3=f(W_3\centerdot\vec{a}_2)……..(19)\\<br>&amp;\vec{y}=f(W_4\centerdot\vec{a}_3)………..(20)\\<br>\end{align}<br>$$<br>这就是神经网络输出值的计算方法。</p>
<h2 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h2><p>​    现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p>
<p>​    接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<h3 id="反向传播算法-Back-Propagation"><a href="#反向传播算法-Back-Propagation" class="headerlink" title="反向传播算法(Back Propagation)"></a>反向传播算法(Back Propagation)</h3><p>​    我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。</p>
<p>我们假设每个训练样本为$(\vec x,\vec t)$，其中向量$\vec x$是训练样本的特征，而$\vec t$是样本的目标值。</p>
<p><img src="/2018/04/02/dl2/image4.png" alt="6"></p>
<p>​    首先，我们根据上一节介绍的算法，用样本的特征$\vec x$，计算出神经网络中每个隐藏层节点的输出$a_i$，以及输出层每个节点的输出$y_i$。</p>
<p>​    然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$：</p>
<ul>
<li>对于输出层节点$i$，<br>$$<br>\delta_i=y_i(1-y_i)(t_i-y_i)\qquad(式3)<br>$$<br>​</li>
</ul>
<p>其中，$\delta_i$是节点的误差项，$y_i$是节点的<strong>输出值</strong>，$t_i$是样本对应于节点$i$的<strong>目标值</strong>。举个例子，根据上图，对于输出层节点8来说，它的输出值是$y_i$，而样本的目标值是$t_i$，带入上面的公式得到节点8的误差项$\delta_8$应该是：<br>$$<br>\delta_8=y_1(1-y_1)(t_1-y_1)<br>$$</p>
<ul>
<li>对于隐藏层节点，</li>
</ul>
<p>$$<br>\delta_i=a_i(1-a_i)\sum_{k\in{outputs}}w_{ki}\delta_k\qquad(式4)<br>$$</p>
<p>其中，是$a_i$节点$i$的输出值，$w_{ki}$是节点$i$到它的下一层节点$k$的连接的权重，$\delta_k$是节点$i$的下一层节点的误差项。例如，对于隐藏层节点4来说，计算方法如下：<br>$$<br>\delta_4=a_4(1-a_4)(w_{84}\delta_8+w_{94}\delta_9)<br>$$<br>最后，更新每个连接上的权值：<br>$$<br>w_{ji}\gets w_{ji}+\eta\delta_jx_{ji}\qquad(式5)<br>$$<br>​    其中，$w_{ji}$是节点$i$到节点$j$的权重，$\eta$是一个成为<strong>学习速率</strong>的常数，$\delta_j$是节点$j$的误差项，$x_{ji}$是节点$i$传递给节点$j$的输入。例如，权重$w_{84}$的更新方法如下：<br>$$<br>w_{84}\gets w_{84}+\eta\delta_8 a_4<br>$$<br>类似的，权重$w_{41}$的更新方法如下<br>$$<br>w_{41}\gets w_{41}+\eta\delta_4 x_1<br>$$<br>偏置项的输入值永远为1。例如，节点4的偏置项$w_{4b}$应该按照下面的方法计算：<br>$$<br>w_{4b}\gets w_{4b}+\eta\delta_4<br>$$<br>​    我们已经介绍了神经网络每个节点误差项的计算和权重更新方法。显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据<strong>式5</strong>来更新所有的权重。</p>
<h3 id="反向传播算法的推导"><a href="#反向传播算法的推导" class="headerlink" title="反向传播算法的推导"></a>反向传播算法的推导</h3><p>反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt提出感知器算法将近30年之后才被发明和普及的。对此，Bengio这样回应道：</p>
<blockquote>
<p>很多看似显而易见的想法只有在事后才变得显而易见。</p>
</blockquote>
<p>接下来，我们用链式求导法则来推导反向传播算法，也就是上一小节的<strong>式3</strong>、<strong>式4</strong>、<strong>式5</strong>。</p>
<p><strong>前方高能预警——接下来是数学公式重灾区，读者可以酌情阅读，不必强求。</strong></p>
<p>按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用<strong>随机梯度下降</strong>优化算法去求目标函数最小值时的参数值。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：<br>$$<br>E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2<br>$$<br>​    其中，$E_d$表示是样本$d$的误差。</p>
<p><img src="/2018/04/02/dl2/image4.png" alt="7"></p>
<p>观察上图，我们发现权重$w_{ji}$仅能通过影响节点$j$的输入值影响网络的其它部分，设$net_j$是节点$j$的<strong>加权输入</strong>，即<br>$$<br>\begin{align}<br>net_j&amp;=\vec{w_j}\centerdot\vec{x_j}………(21)\\<br>&amp;=\sum_{i}{w_{ji}}x_{ji}……..(22)<br>\end{align}<br>$$<br>$E_d$是$net_j$的函数，而$net_j$是$w_{ji}$的函数。根据链式求导法则，可以得到：<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{w_{ji}}}&amp;=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{net_j}}{\partial{w_{ji}}}…..(23)\\<br>&amp;=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}…..(24)\\<br>&amp;=\frac{\partial{E_d}}{\partial{net_j}}x_{ji}…….(24)<br>\end{align}<br>$$<br>上式中，$x_{ji}$是节点$i$传递给节点$j$的输入值，也就是节点$i$的输出值。</p>
<p>对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分<strong>输出层</strong>和<strong>隐藏层</strong>两种情况。</p>
<h4 id="输出层权值训练"><a href="#输出层权值训练" class="headerlink" title="输出层权值训练"></a>输出层权值训练</h4><p>对于<strong>输出层</strong>来说，$net_j$仅能通过节点$j$的输出值$y_i$来影响网络其它部分，也就是说$E_d$是$y_i$的函数，而$y_i$是$net_j$的函数，其中$y_i=sigmoid(net_j)$。所以我们可以再次使用链式求导法则：<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{net_j}}&amp;=\frac{\partial{E_d}}{\partial{y_j}}\frac{\partial{y_j}}{\partial{net_j}}……(26)\\<br>\end{align}<br>$$<br>考虑上式第一项:<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{y_j}}&amp;=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2…..(27)\\<br>&amp;=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2……(28)\\<br>&amp;=-(t_j-y_j)<br>\end{align}<br>$$<br>考虑上式第二项：<br>$$<br>\begin{align}<br>\frac{\partial{y_j}}{\partial{net_j}}&amp;=\frac{\partial sigmoid(net_j)}{\partial{net_j}}…..(30)\\<br>&amp;=y_j(1-y_j)  …….(31)\\<br>\end{align}<br>$$<br>将第一项和第二项带入，得到：<br>$$<br>\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)<br>$$<br>如果令$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$,也就是一个节点的误差项是网络误差对这个节点输入的偏导数的相反数。带入上式，得到：<br>$$<br>\delta_j=(t_j-y_j)y_j(1-y_j)<br>$$<br>上式就是<strong>式3</strong>。</p>
<p>将上述推导带入随机梯度下降公式，得到：<br>$$<br>\begin{align}<br>w_{ji}&amp;\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}…….(32)\\<br>&amp;=w_{ji}+\eta(t_j-y_j)y_j(1-y_j)x_{ji}………(33)\\<br>&amp;=w_{ji}+\eta\delta_jx_{ji}……..(34)<br>\end{align}<br>$$<br>上式就是<strong>式5</strong>。</p>
<h4 id="隐藏层权值训练"><a href="#隐藏层权值训练" class="headerlink" title="隐藏层权值训练"></a>隐藏层权值训练</h4><p>现在我们要推导出隐藏层的$\frac{\partial{E_d}}{\partial{net_j}}$。</p>
<p>​    首先，我们需要定义节点$j$的所有直接下游节点的集合$Downstream(j)$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到只能通过影响$Downstream(j)$再影响$E_d$。设$net_k$是节点$j$的下游节点的输入，则$E_d$是$net_k$的函数，而$net_k$是$net_j$的函数。因为$net_k$有多个，我们应用全导数公式，可以做出如下推导：<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{net_j}}&amp;=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}\frac{\partial{net_k}}{\partial{net_j}}…..(35)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{net_j}}……….(36)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{net_j}}……(37)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_kw_{kj}\frac{\partial{a_j}}{\partial{net_j}}……..(38)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_kw_{kj}a_j(1-a_j)………(39)\\<br>&amp;=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}…….(40)<br>\end{align}<br>$$<br>因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：<br>$$<br>\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}<br>$$<br>上式就是<strong>式4</strong>。</p>
<p><strong>——数学公式警报解除——</strong></p>
<p>​    至此，我们已经推导出了反向传播算法。需要注意的是，我们刚刚推导出的训练规则是根据激活函数是sigmoid函数、平方和误差、全连接网络、随机梯度下降优化算法。如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样。但是无论怎样，训练规则的推导方式都是一样的，应用链式求导法则进行推导即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">for l in range(len(self.weights)):</span><br><span class="line">    # a.append(self.activation(np.dot(a[l], self.weights[l])))</span><br><span class="line"></span><br><span class="line">    if l == len(self.weights) - 1 :</span><br><span class="line">        #添加偏置的输入1</span><br><span class="line">        result = self.activation(np.dot(a[l], self.weights[l]))</span><br><span class="line">        # result = list(self.activation(np.dot(a[l], self.weights[l]))).append(1)</span><br><span class="line">        a.append(np.array(result))</span><br><span class="line">    else:</span><br><span class="line">        result = self.activation(np.dot(a[l], self.weights[l]))</span><br><span class="line">        result = np.concatenate((result, [1]))  # 先将p_变成list形式进行拼接，注意输入为一个tuple</span><br><span class="line">        a.append(np.array(result))</span><br></pre></td></tr></table></figure>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[dl1]]></title>
      <url>/2018/03/31/dl1/</url>
      <content type="html"><![CDATA[<h2 id="线性单元"><a href="#线性单元" class="headerlink" title="线性单元"></a>线性单元</h2><p>​    感知器有一个问题，当面对的数据集不是<strong>线性可分</strong>的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个<strong>可导</strong>的<strong>线性函数</strong>来替代感知器的<strong>阶跃函数</strong>，这种感知器就叫做<strong>线性单元</strong>。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。</p>
<p>为了简单起见，我们可以设置线性单元的激活函数$f$为</p>
<p>$f(x)=x$</p>
<p>这样的线性单元如下图所示    </p>
<p><img src="/2018/03/31/dl1/image1.png" alt="t1"></p>
<p>感知器</p>
<p><img src="/2018/03/31/dl1/image2.png" alt="t"></p>
<p>​    替换激活函数之后，<strong>线性单元</strong>将返回一个<strong>实数值</strong>而不是<strong>0,1分类</strong>。因此线性单元用来解决<strong>回归</strong>问题而不是<strong>分类</strong>问题。</p>
<h3 id="线性单元的模型"><a href="#线性单元的模型" class="headerlink" title="线性单元的模型"></a>线性单元的模型</h3><p>​    当我们说<strong>模型</strong>时，我们实际上在谈论根据输入$x$预测输出$y$的<strong>算法</strong>。比如，$x$可以是一个人的工作年限，$y$可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如：</p>
<p>$y=h(x)=w*x+b$</p>
<p>​    函数$h(x)$叫做<strong>假设</strong>，而$w,b$是它的<strong>参数</strong>。我们假设参数$w=1000$，参数$b=500$，如果一个人的工作年限是5年的话，我们的模型会预测他的月薪为</p>
<p>$y=h(x)=1000*5+500=5500（元）$</p>
<p>​    你也许会说，这个模型太不靠谱了。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为<strong>特征</strong>。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他</p>
<p>$x=(5,IT,百度，T6)$</p>
<p>​    既然输入$x$变成了一个具备四个特征的向量，相对应的，仅仅一个参数$w$就不够用了，我们应该使用4个参数$w_1,w_2,w_3,w_4$，每个特征对应一个。这样，我们的模型就变成</p>
<p>$y=h(x)=w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+w_4</em>x_4+b$</p>
<p>​    其中，$x_1$对应工作年限，$x_2$对应行业，$x_3$对应公司，$x_4$对应职称。</p>
<p>​    为了书写和计算方便，我们可以$w_0$令等于$b$，同时令$w_0$对应于特征$x_0$。由于$x_0$其实并不存在，我们可以令它的值永远为1。也就是说</p>
<p>$b=w_0*x_0$ 其中$x_0=1$</p>
<p>​    这样上面的式子就可以写成</p>
<p>$y=h(x)=w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+w_4</em>x_4+b$              ………….(1)</p>
<p>$=w_0<em>x_0+w_1</em>x_1+w_2<em>x_2+w_3</em>x_3+w_4*x_4$                    ………….(2)</p>
<p>​    我们还可以把上式写成向量的形式</p>
<p>​    $y=h(x)=w^Tx$</p>
<p>​    长成这种样子模型就叫做<strong>线性模型</strong>，因为输出$y$就是输入特征$x_1,x_2,x_3,…$的<strong>线性组合</strong>。</p>
<h3 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h3><p>​    接下来，我们需要关心的是这个模型如何训练，也就是参数$w$取什么值最合适。</p>
<p>​    机器学习有一类学习方法叫做<strong>监督学习</strong>，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征$x$，也包括对应的输出$y$(也叫做<strong>标记，label</strong>)。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业…)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征)，也看到对应问题的答案(标记)。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。</p>
<p>​    另外一类学习方法叫做<strong>无监督学习</strong>，这种方法的训练样本中只有而没有。模型可以总结出特征的一些规律，但是无法知道其对应的答案$y$。</p>
<p>​    很多时候，既有$x$又有的$y$训练样本是很少的，大部分样本都只有$x$。比如在语音到文本(STT)的识别任务中，$x$是语音，$y$是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并<strong>标注</strong>上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用<strong>无监督学习方法</strong>先做一些<strong>聚类</strong>，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。</p>
<h3 id="线性单元的目标函数"><a href="#线性单元的目标函数" class="headerlink" title="线性单元的目标函数"></a>线性单元的目标函数</h3><p>​    现在，让我们只考虑<strong>监督学习</strong>。</p>
<p>​    在监督学习下，对于一个样本，我们知道它的特征$x$，以及标记$y$。同时，我们还可以根据模型计算得到输出。注意这里面我们用表示训练样本里面的<strong>标记</strong>，也就是<strong>实际值</strong>；用带上划线的表示模型计算的出来的<strong>预测值</strong>。我们当然希望模型计算出来的和越接近越好。</p>
<p>​    数学上有很多方法来表示的$\overline y$和$y$的接近程度，比如我们可以用$\overline y$和$y$的差的平方的$1\over2$来表示它们的接近程度</p>
<p>$e={1\over2}(y-\overline y)^2$</p>
<p>​    我们把$e$叫做<strong>单个样本</strong>的<strong>误差</strong>。至于为什么前面要乘$1\over 2$，是为了后面计算方便。</p>
<p>训练数据中会有很多样本，比如$N$个，我们可以用训练数据中<strong>所有样本</strong>的误差的<strong>和</strong>，来表示模型的误差$E$，也就是$E=e^{(1)}+e^{(2)}+e^{(3)}+\dots+e^{(n)}$</p>
<p>​    上式的$e^{(1)}$表示第一个样本的误差，$e^{(2)}$表示第二个样本的误差……。</p>
<p>​    我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样</p>
<p>$E=e^{(1)}+e^{(2)}+e^{(3)}+\dots+e^{(n)}$ …………………(3)</p>
<p>$\sum_{i=1}^n e^{(i)} $                                             …………………(4)</p>
<p>$={1\over 2}\sum_{i=1}^n(y^(i)-\overline y^{(i)})^2$         .     …………………(5)</p>
<p>其中</p>
<p>$\overline y^{(i)}=h(x^{(i)})$  ……………………..(6)</p>
<p>$=w^Tx^{(i)}$ ………………………..(7)</p>
<p>(5)中，$x^{(i)}$表示第$i$个训练样本的特征，$y^{(i)}$表示第$i$个样本的标记，我们也可以用<strong>元组</strong>$(x^{(i)},y^{(i)})$表示第$i$个 <strong>训练样本</strong>。$\overline y^{(i)}$则是模型对第$i$个样本的<strong>预测值</strong>。</p>
<p>​    我们当然希望对于一个训练数据集来说，误差最小越好，也就是(5)的值越小越好。对于特定的训练数据集来说，$(x^{(i)},y^{(i)})$的值都是已知的，所以(式2)其实是参数$w$的函数。</p>
<p>$E(w)={1\over 2}\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})^2 $               ……………………..(8)</p>
<p>$={1\over 2}\sum_{i=1}^n(y^{(i)}-w^Tx^{(i)})^2$ ………………………….(9)</p>
<p>​    由此可见，模型的训练，实际上就是求取到合适的，使(5)取得最小值。这在数学上称作<strong>优化问题</strong>，而就是我们优化的目标，称之为<strong>目标函数</strong>。</p>
<h3 id="梯度下降优化算法"><a href="#梯度下降优化算法" class="headerlink" title="梯度下降优化算法"></a>梯度下降优化算法</h3><p>​    我们学过怎样求函数的极值。函数$y=f(x)$的极值点，就是它的导数$f’(x)=0$的那个点。因此我们可以通过解方程$f’(x)=0$，求得函数的极值点$(x_0,y_0)$。</p>
<p>​    不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点『试』出来。如下图所示：</p>
<p><img src="/2018/03/31/dl1/image3.png" alt="3"></p>
<p>​    首先，我们随便选择一个点开始，比如上图的点$x_0$。接下来，每次迭代修改$x$的为，经过数次迭代后最终达到函数最小值点。</p>
<p>​    你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数$y=f(x)$的<strong>梯度</strong>的<strong>相反方向</strong>来修改$x$。什么是<strong>梯度</strong>呢？梯度<strong>是一个向量，它指向</strong>函数值<strong>上升最快</strong>的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。</p>
<p>按照上面的讨论，我们就可以写出梯度下降算法的公式</p>
<p>$x_{new}=x_{old}-\eta\nabla f(x)$</p>
<p>​    其中，$\nabla$是<strong>梯度算子</strong>，$\nabla f(x)$就是指的梯度。$\eta$是步长，也称作<strong>学习速率</strong>。</p>
<p>​    对于上一节列出的目标函数(式5)</p>
<p>$E(w)={1\over2}\sum_{i=1}^n(y^{(i)}-\overline y^{(i)}$</p>
<p>梯度下降算法可以写成</p>
<p>$w_{new}=w_{old}+\eta\nabla E(w)$</p>
<p>我们要来求取$\nabla E(w)$，然后带入上式，就能得到线性单元的参数修改规则。</p>
<p>关于$\nabla E(w)$的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。在这里，您只需要知道，经过一大串推导，目标函数$E(w)$的梯度是</p>
<p>$\nabla E(w)=-\sum_{i=1}^{n}(y^{(i)}-\overline y^{(i)})x^{(i)}$</p>
<p>因此，线性单元的参数修改规则最后是这个样子</p>
<p>$w_{new}=w_{old}+\eta\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})$</p>
<p>有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。</p>
<p>需要说明的是，如果每个样本有M个特征，则上式w，x都是M+1维向量（因为我们加上了一个恒为1的虚拟特征$x_0$,参考前面的内容），而$y$是标量。用高逼格的数学符号表示 ，就是</p>
<p>$w,x\in R^{(M+1)}$</p>
<p>$y\in R^1$</p>
<p>为了让您看明白说的是啥，我吐血写下下面这个解释(写这种公式可累可累了)。因为$w,x$是M+1维<strong>列向量</strong>，所以(式3)可以写成<br>$$<br>\begin{bmatrix}<br>w_0 \\<br>w_1\\<br>w_2\\<br>\dots\\<br>w_m\\<br>\end{bmatrix}=\begin{bmatrix}<br>w_0 \\<br>w_1\\<br>w_2\\<br>\dots\\<br>w_m\\<br>\end{bmatrix}+\eta\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})\begin{bmatrix}<br>1 \\<br>x_1^{(i)}\\<br>x_2^{(i)}\\<br>\dots\\<br>x_m^{(i)}\\<br>\end{bmatrix}<br>$$</p>
<h4 id="nabla-E-w-的推导"><a href="#nabla-E-w-的推导" class="headerlink" title="$\nabla E(w)$的推导"></a>$\nabla E(w)$的推导</h4><p>​    这一节你尽可以跳过它，并不太会影响到全文的理解。当然如果你非要弄明白每个细节，那恭喜你骚年，机器学习的未来一定是属于你的。</p>
<p>​    首先，我们先做一个简单的前戏。我们知道函数的梯度的定义就是它相对于各个变量的<strong>偏导数</strong>，所以我们写下下面的式子<br>$$<br>\nabla E(w)={\partial\over\partial w}E(w)                …………….(10)\\<br>={\partial\over \partial w}{1\over2}\sum_{i=1}^n(y^{(i)} - \overline y^{(i)})………….(11)<br>$$<br>​    可接下来怎么办呢？我们知道和的导数等于导数的和，所以我们可以先把求和符号$\sum$里面的导数求出来，然后再把它们加在一起就行了，也就是<br>$$<br>{\partial \over \partial w  }{1\over 2}\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})^2…………(12)\\<br>={1\over 2}\sum_{i=1}^n{\partial\over \partial w}(y^{(i)}-\overline y^{(i)})^2…………(13)<br>$$</p>
<p>​    现在我们可以不管高大上的$\sum$了，先专心把里面的导数求出来。<br>$$<br>{\partial \over \partial w}(y^{(i)}-\overline y^{(i)})^2………………….(14)\\<br>={\partial \over \partial w}({y^{(i)}}^2-2\overline y^{(i)} +{\overline y^{(i)}}^2)………………….(15)<br>$$<br>​    我们知道，$y$是与$w$无关的常数，而$\overline y=w^Tx$,下面我们根据链式求导法则来求导。<br>$$<br>{\partial E(w)\over \partial w}={\partial E(\overline y)\over \partial\overline y}{\partial\overline y\over \partial w}<br>$$</p>
<p>我们分别计算上式等号右边的两个偏导数<br>$$<br>{\partial E(w)\over \partial \overline y}={\partial \over\partial \overline y}({y^{(i)}}^2-2\overline y^{(i)}y^{(i)}+{\overline y^{(i)}}^2)…………(16)\\<br>=-2y^{(i)}+2\overline y ^{(i)}……………….(17)<br>$$</p>
<p>$$<br>{\partial \overline y \over \partial w}={\partial \over\partial w}w^Tx ……………………..(18)\\<br>=x……………………(19)<br>$$<br>代入，我们求得$\sum$里面的偏导数是<br>$$<br>{\partial \over \partial w}(y^{(i)}-\overline y^{(i)})^2………………………..(20)\\<br>=2(-y^{(i)}+\overline y^{(i)})x……………………….(21)<br>$$<br>最后代入$\nabla E(w)$,求得<br>$$<br>\nabla E(w)={1\over2}\sum_{i=1}^n{\partial\over \partial w}(y^{(i)}-\overline y^{(i)})^2…………..(22)\\<br>={1\over 2}\sum_{i=1}^n2(-y^{(i)}+\overline y^{(i)})x…………….(23)\\<br>=-\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})x……………….(25)<br>$$</p>
<h3 id="随机梯度下降算法-Stochastic-Gradient-Descent-SGD"><a href="#随机梯度下降算法-Stochastic-Gradient-Descent-SGD" class="headerlink" title="随机梯度下降算法(Stochastic Gradient Descent, SGD)"></a>随机梯度下降算法(Stochastic Gradient Descent, SGD)</h3><p>​    如果我们根据上面式子来训练模型，那么我们每次更新的迭代，要遍历训练数据中<strong>所有的样本</strong>进行计算，我们称这种算法叫做<strong>批梯度下降(Batch Gradient Descent)</strong>。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新并不一定按照减少的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别</p>
<p><img src="/2018/03/31/dl1/image4.png" alt="4"></p>
<p>​    如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。</p>
<p>​    最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>事实上，一个机器学习算法其实只有两部分</p>
<ul>
<li><p><strong>模型</strong> 从输入特征预测输入的那个函数</p>
</li>
<li><p><strong>目标函数</strong> 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的<strong>最优值</strong>。很多时候我们只能获得目标函数的<strong>局部最小(最大)值</strong>，因此也只能得到模型参数的<strong>局部最优值</strong>。</p>
<p>因此，如果你想最简洁的介绍一个算法，列出这两个函数就行了。</p>
<p>接下来，你会用<strong>优化算法</strong>去求取目标函数的最小(最大)值。<strong>[随机]梯度{下降|上升}</strong>算法就是一个<strong>优化算法</strong>。针对同一个<strong>目标函数</strong>，不同的<strong>优化算法</strong>会推导出不同的<strong>训练规则</strong>。我们后面还会讲其它的优化算法。</p>
<p>其实在机器学习中，算法往往并不是关键，真正的关键之处在于选取特征。选取特征需要我们人类对问题的深刻理解，经验、以及思考。而<strong>神经网络</strong>算法的一个优势，就在于它能够自动学习到应该提取什么特征，从而使算法不再那么依赖人类，而这也是神经网络之所以吸引人的一个方面。</p>
<p>现在，经过漫长的烧脑，你已经具备了学习<strong>神经网络</strong>的必备知识。下一篇文章，我们将介绍本系列文章的主角：<strong>神经网络</strong>，以及用来训练神经网络的大名鼎鼎的算法：<strong>反向传播</strong>算法。至于现在，我们应该暂时忘记一切，尽情奖励自己一下吧。</p>
</li>
</ul>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[逻辑回归]]></title>
      <url>/2018/03/12/lr/</url>
      <content type="html"><![CDATA[<h3 id="逻辑斯谛分布"><a href="#逻辑斯谛分布" class="headerlink" title="逻辑斯谛分布"></a>逻辑斯谛分布</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍<strong>逻辑斯谛分布</strong>，该分布的定义是</p>
<p>设$X$是连续随机变量,$X$服从逻辑斯谛分布是指$X$ 服从如下分布函数和密度函数:</p>
<p>$F(x)=P(X\le x)={1\over 1+e^{-(x-\mu)}/\gamma}$ …………..(1)</p>
<p>$f(x)=F^,(X\le x)={e^{-(x-\mu)/\gamma}\over \gamma(1+e^{-(x-\mu)/\gamma})}$ …………(2)</p>
<p>式中，$\mu$为位置参数 ，$\gamma&gt;0$ 为形状参数。</p>
<a id="more"></a>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;逻辑斯蒂分布的密度函数$f(x)$ 和分布函数$F(x)$的图形如下图所示。</p>
<p><img src="/2018/03/12/lr/lr.jpg" alt="图片"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布函数属于逻辑斯蒂函数，其图形是一条$S$形曲线(sigmoid curve).该曲线以点$(\mu,{1 \over 2})$为中心对称，即满足</p>
<p>$F(-x+\mu)-{1\over 2}=-F(x+\mu)+{1\over 2}$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$ 的值越小，曲线在中心附近增长越快。</p>
<h3 id="二项逻辑斯蒂回归模型"><a href="#二项逻辑斯蒂回归模型" class="headerlink" title="二项逻辑斯蒂回归模型"></a>二项逻辑斯蒂回归模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二项逻辑回归模型(binomial logistic regression model)是一种分类模型，由条件概率分布$P(Y|X)$ 表示，形式为参数化的逻辑斯蒂分布。这里，随机变量$X$ 取值为实数，随机变量$Y$取值为1或0。我们通过监督学习的方法来估计模型参数。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一种由条件概率表示的模型，其条件概率模型如下：</p>
<p>$P(Y=1|x)={exp(w \cdot x +b) \over 1+exp(w \cdot x +b)}$   ………….(3)</p>
<p>$P(Y=0|x)={1 \over 1+exp(w \cdot x +b)}$   ………….(4)</p>
<p><img src="/2018/03/12/lr/lr2.png" alt="图片"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图 逻辑回归网络</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里，$x\in R^n$ 是输入，$Y \in{0,1}$是输出，$w\in R^n$和$b\in R$是参数，$w$称为权重向量，$b$称为偏置，$w\cdot x$为$w$和$x$的內积。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于给定的输入实例$x$，按照(3)和(4)可以求得$P(Y=1|x)$和$P(Y=0|x)$。逻辑斯蒂回归比较两个条件概率值得大小，将实例$x$分到概率值较大的那一类。 有时为了方便，将权值向量和输入向量加以扩充，仍记作$w$，$x$，即$w=(w^{(1)},w^{(2)},\dots,w^{n},b) ^T$，$x=(x^{(1)},x^{(2)},\dots,x^{(n)},1)^T$ 。这时，逻辑斯蒂回归模型如下：</p>
<p>$P(Y=1|x)={exp(w \cdot x) \over 1+exp(w \cdot x)}$ ………..(5)</p>
<p>$P(Y=0|x)={1 \over 1+exp(w \cdot x)}$ ………..(6)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实分子分母同时除以指数部分，就变成sigmoid函数了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在考查逻辑斯蒂回归模型的特点。一个时间的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率是$p\over{1-p}$，该事件的对数几率(log odds)或者logit函数是</p>
<p>$logit(p)=log{p\over{1-p}}$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对逻辑斯蒂回归而言，由式（5）与（6）得</p>
<p>$log{P(Y=1|x)\over {1-P(Y=1|x)}}=w \cdot x$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就是说，在逻辑斯蒂回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。或者说输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型。反过来讲，如果知道权值向量，给定输入$x$，就能求出$Y=1$的概率：</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意 </strong>，这里$x\in R^{n+1},w\in R^{n+1}$。通过逻辑斯蒂回归模型定义式（5）可以将线性函数$w\cdot x$转换为概率：</p>
<p>$P(Y=1|x)={exp(w\cdot x)\over{1+exp(w\cdot x)}}$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近无穷，概率值就越接近0。这样的模型就是逻辑斯蒂回归模型。</p>
<h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据给定的数据集，把参数$w$给求出来了。要找参数$w$，首先就是得把代价函数给定义出来，也就是目标函数。首先想到的是类型线性回归的做法，利用误差平方来当代价函数。</p>
<p>$J(w)=\sum_{i}{1\over 2}(\phi(z^{i})-y^i)^2$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中 $z^i=w\cdot x$ ，$i$表示第$i$个样本点，$y^i$表示第$i$个样本的真实值，$\phi(z^{i})$表示第$i$个样本的预测值 。</p>
<p>$\phi(z^{i}) ={1\over 1+e^{-z^i}}$ ，这是一个非凸函数，这就意味着代价函数有着许多的局部最小值，不利于我们的求解。</p>
<p><img src="/2018/03/12/lr/lr3.png" alt="3"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;逻辑斯蒂回归模型学习中，对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)}$,其中，$x_i\in R^n,y_i\in{0,1}$可以应用极大似然估计法估计模型参数，从而得到逻辑斯蒂回归模型。</p>
<p>设：</p>
<p>$P(Y=1|x)=\pi(x)$</p>
<p>$P(Y=0|x)=1-\pi(x)$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于每一个观察到的样本$(x_i,y_i)$ 出现的概率是：</p>
<p>​    $P(x_i,y_i)=P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}=\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解释一下，当$y=1$的时候，后面的那一项没有了，那就只剩下$x$属于1类的概率，当$y=0$的时候，第一项没有了，那就只剩下后面那个$x$属于0的概率。不管$y$是0还是1，上面得到的都是$(x,y)$出现的概率 。那我们的整个样本集，也就是$n$个独立的样本出现的似然函数为（因为每个样本都是独立的，所以$n$的样本出现的概率就是他们各自出现的概率相乘）：</p>
<p>似然函数为$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$    </p>
<p>对数似然函数为</p>
<p>$L(w)=\sum_{i=1}^N[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]$</p>
<p>$=\sum_{i=1}^N \left[ y_ilog{\pi(x_i)\over{1-\pi(x_i)}}+log(1-\pi(x_i)) \right ]$</p>
<p>$=\sum_{i=1}^N[y_i(w\cdot x_i)-log(1+exp(w\cdot x_i))]$</p>
<p>对$L(w)$ 求极大值，得到$w$的估计值。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;书上没有写出完整的参数估计算法，但给出了其对数似然函数，经过简单的证明可以得出该函数可以得出该函数是单调上。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以使用$L(w)$作为目标函数，求似然函数的最大值，加个负号，就可以变成最小值，与我们平常的机器学习中的损失函数对应起来。下面对公式进行推导。</p>
<p>$L(w)=\sum_{i=1}^N[y_i(w\cdot x_i)-log(1+exp(w\cdot x_i))]$</p>
<p>$-L(w)=\sum_{i=1}^N[(log(1+exp(w\cdot x_i))-y_i(w\cdot x_i)]$</p>
<p>${\partial (-L(w))\over w}=\sum_{i=1}^N\left({ exp(w\cdot x_i)\cdot x_i \over 1+exp(w\cdot x_i)} - y_i\cdot x_i\right)$</p>
<p>${\partial (-L(w))\over w}=\sum_{i=1}^N\left (\left({ exp(w\cdot x_i) \over 1+exp(w\cdot x_i)} - y_i\right) \cdot x_i\right)$</p>
<p>${\partial  (-L(w)) \over w }=\sum_{i=1}^N\left (\left({1 \over 1+exp-(w\cdot x_i)} - y_i\right) \cdot x_i\right)$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们令导数为0，会发现无法解析求解。没办法，只能借助高大上的迭代来搞定了。这里选用了经典的梯度下降算法。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每次选择一个误分类点，用上述梯度对$w$进行更新即可，注意由于梯度中包含指数操作，所以需要一个很小的学习率。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样，问题就变成以对数似然函数为目标函数的最优化问题。逻辑斯蒂回归学习中通常采用的方法是梯度下降法及拟牛顿法。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设$w$的极大似然估计值是$\hat w$ ，那么学到的逻辑斯蒂回归模型为</p>
<p>$P(Y=1|x)={exp(\hat w\cdot x ) \over 1+exp(\hat w \cdot x)}$</p>
<p>$P(Y=0|x)={1 \over 1+exp(\hat w \cdot x)}$</p>
<h3 id="多项逻辑回归"><a href="#多项逻辑回归" class="headerlink" title="多项逻辑回归"></a>多项逻辑回归</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面介绍的逻辑斯蒂回归模型是二项分类模型，用于二类分类，用于二类分类。可以将其推广为多项逻辑斯蒂回归(multi-nominal logistic regression model)，用于多类分类。假设离散型随机变量$Y$的取值集合是${1,2,\dots,K}$,那么多项逻辑回归模型是        </p>
<p>$P(Y=k|x)={exp(w_k \cdot x)}\over 1+{\sum_{k=1}^{K-1}exp(w_k\cdot x)}$ ，$k=1,2,\dots,K-1$    ………………..(7)</p>
<p>$P(Y=K|x)={1 \over \sum_{i=1}^{K-1}exp(w_k\cdot x)},k=1,2,\dots,K-1$ ………………..(8)</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里，$x\in R^{n+1},w_k\in R^{n+1}$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二项逻辑斯蒂回归的参数法也可以推广到多项逻辑斯蒂回归。</p>
<p>参考：《统计学习方法》，李航</p>
<p><a href="http://www.hankcs.com/ml/the-logistic-regression-and-the-maximum-entropy-model.html" target="_blank" rel="noopener">http://www.hankcs.com/ml/the-logistic-regression-and-the-maximum-entropy-model.html</a></p>
<p><a href="http://blog.csdn.net/wds2006sdo/article/details/53084871" target="_blank" rel="noopener">http://blog.csdn.net/wds2006sdo/article/details/53084871</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '逻辑回归',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>

]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[极大似然估计法MLE]]></title>
      <url>/2018/03/10/mle/</url>
      <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法（$Method  of  Maximum  Likelihood  Estimation –MLE$）</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法最早由高斯（C.F.Gauss）提出。后来为费歇在1912年的文章中重新提出，并且证明了这个方法的一些性质。极大似然估计这一名称也是费歇（R.A.Fisher）给的。这是一种目前仍然得到广泛应用的方法。它是建立在极大似然原理的基础上的一个统计方法。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 在数理统计学中，似然函数是一种关于统计模型中的<strong>参数</strong>的<strong>函数</strong>，表示模型参数中的似然性。 似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。<strong>“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性</strong>，但是在<strong>统计学</strong>中，“似然性”和“或然性”或“概率”又有明确的区分。<strong>概率</strong>用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而<strong>似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。</strong> 有人说，概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法的依据就是：<strong>概率最大的事件最可能发生</strong></p>
<a id="more"></a>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法最早由高斯（C.F.Gauss）提出。后来为费歇在1912年的文章中重新提出，并且证明了这个方法的一些性质。极大似然估计这一名称也是费歇（R.A.Fisher）给的。这是一种目前仍然得到广泛应用的方法。它是建立在极大似然原理的基础上的一个统计方法。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 看论文的时候读到这样一句话：</p>
<p>​    <em>Assuming  the  training  instances  are  independently  sampled,the  likelihood  function  of  parameters   $\theta={w,\alpha,\beta}$ given  the  observations  D  can  be  factored  as</em><br>$$<br>Pr[D|\theta]=\prod_{i=1}^NPr[y_i^1,\dots,y_i^R|x_i;\theta]<br>$$<br>&nbsp; &nbsp; &nbsp; &nbsp; 原来只关注公式，所以一带而过。再重新看这个公式前的描述，细思极恐。</p>
<p> <em>the <strong>likelihood function</strong> of the parameters θ = {w,α,β} <strong>given the observations D</strong> can be factored as..</em></p>
<p><strong>两个疑问</strong>：</p>
<ul>
<li>likelihood function_为什么会写成条件概率的形式？</li>
<li><strong>given</strong>的明明是D，为什么到后面的公式里，却变成了$given  \theta$呢？</li>
</ul>
<p>&nbsp; &nbsp; &nbsp; &nbsp; <strong>常说的概率是指给定参数后，预测即将发生的事件的可能性</strong>。拿硬币这个例子来说，我们已知一枚均匀硬币的正反面概率分别是0.5，要预测抛两次硬币，硬币都朝上的概率：</p>
<p>$H$代表$Head$，表示头朝上</p>
<p>$p(HH | p_H = 0.5) = 0.5*0.5 = 0.25.$</p>
<p> &nbsp;   这种写法其实有点误导，后面的这个其实是作为参数存在的，而不是一个随机变量，因此不能算作是条件概率，更靠谱的写法应该是 $p(HH;p=0.5)。$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;而似然概率正好与这个过程相反，<strong>我们关注的量</strong>不再是事件的发生概率，而是已<strong>知发生了某些事件，我们希望知道参数应该是多少。</strong></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们已经抛了两次硬币，并且知道了结果是两次头朝上，这时候，我希望知道这枚硬币抛出去正面朝上的概率为0.5的概率是多少？正面朝上的概率为0.8的概率是多少？</p>
<p>如果我们希望知道正面朝上概率为0.5的概率，这个东西就叫做似<strong>然函数</strong>，可以说成是对某一个参数的猜想$（p=0.5）$的概率，这样表示成(条件)概率就是</p>
<p>$L(p_H=0.5|HH) =P(HH|p_H=0.5) = $（另一种写法）$P(HH;p_H=0.5).$</p>
<p>为什么可以写成这样？我觉得可以这样来想：</p>
<p>$L(\theta|x)=f(x|\theta)$</p>
<p>这里$\theta$是未知参数，它属于参数空间。</p>
<p>&nbsp;$f(x|\theta)$是一个密度函数，特别地，它表示给定$\theta$ 下关于联合概率样本值$x$ 的联合密度函数。前者是关于$\theta$的函数，后者是关于$x$的函数。所以这里的等号理解为函数值形式的相等，而不是两个函数本身是同一函数。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>似然函数本身也是一种概率</strong>，我们可以把$L(pH=0.5|HH)$写成$P(pH=0.5|HH)$; 而根据贝叶斯公式，$P(pH=0.5|HH) = {P(pH=0.5,HH)\over P(HH)}$；既然$HH$是已经发生的事件，理所当然$P(HH) = 1$,所以：</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 在<strong>统计学</strong>中，我们关心的是在<strong>已知一系列投掷的结果时</strong>，关于硬币投掷时正面朝上的可能性的信息。<br>我们可以建立一个统计模型：假设硬币投出时会有$p_H$的概率正面朝上，而有$1-p_H$的概率反面朝上。<br>这时，条件概率可以改写成似然函数：</p>
<p>$L(p_H|HH)=P(HH|p_H=0.5)=0.25$</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 也就是说，对于取定的似然函数，在观测到两次投掷都是正面朝上时，$p_H=0.5$ 的<strong>似然性</strong>是0.25。如果考虑$p_H=0.6$，那么似然函数的值也会改变。</p>
<p>$L(p_H|HH)=P(HH|p_H=0.6)=0.36$ 注意到似然函数的值也变大了。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 这说明，如果参数$p_H$的取值变成0.6的话，结果观测到连续两次正面朝上的概率要比假设时$p_H=0.5$更大。也就是说，参数$p_H$取成0.6要比取成0.5更有说服力，更为“合理”。总之，<strong>似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大。</strong>对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。</p>
<p> &nbsp; &nbsp; &nbsp; 在这个例子中，似然函数实际上等于：</p>
<p>$L(\theta|HH)=P(HH|p_H=\theta)=\theta^2$ 其中$0 \le p_H\le 1$</p>
<p>如果取$p_H=1$ ，那么似然函数达到最大值1.也就是说，当连续观测到两次证明朝上时，假设硬币投掷正面朝上的概率为1是最合理的。</p>
<p>类似地，如果观测到三次投掷硬币，头两次正面朝上，第三次反面朝上，那么似然函数将会是</p>
<p>$L(\theta|HHT)=P(HHT|p_H=\theta)=\theta^2(1-\theta)$ ,其中$T$表示反面朝上，$0\le p_H \le 1$</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 这时候，似然函数的最大值将会在$p_H={2\over3}$的时候取到。也就是说，当观测到三次投掷中前两次正面朝上时，估计硬币投掷时正面朝上的概率$p_H={2\over 3}$是合理的。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 若一试验中有$n$个可能结果$A_1，A_2，…，A_n$,现在做一试验，若事件$A_i$发生了，则认为事件$A_i$在这$n$个可能结果中出现的概率最大。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 一次试验就出现的事件（应该）有较大的概率</p>
<p> &nbsp; &nbsp; &nbsp; <strong>极大似然估计</strong>就是在一次抽样中，若得到观测值$x_1,…,x_n$则选取$\hat\theta(x_1,…,x_n)$作为$\theta$的估计值。使得当$\theta=\hat\theta(x_1,…,x_n)$样本出现的概率最大。</p>
<p><strong>极大似然概率，就是在已知观测的数据的前提下，找到使得似然概率最大的参数值。</strong></p>
<ol>
<li><p><strong>若总体$X$为离散型</strong></p>
<p>设分布律$P{X=k}=p(x;\theta)$,$\theta$为待估参数，$\theta\in\Theta$ ,$X_1,X_2,…,X_n$是来自总体$X$的样本，若$x_1,x_2,…,x_n$为相对于$X_1,X_2,…,X_n$的样本值，<br>$L(\theta)=L(x_1,x_2,…,x_n;\theta)\prod p(x_i;\theta),\theta \in \Theta$</p>
</li>
</ol>
<p>$L(\theta)$称为样本似然函数</p>
<p>若$L(x_1,x_2,…,x_n;\hat\theta)  =  max_{\theta\in\Theta}  L(x_1,x_2,…,x_n;\theta)$  </p>
<p>$ \hat(x_1,x_2,\dots,x_n)$,参数$\theta$的极大似然估计值。</p>
<ol>
<li><strong>设总体$X$为连续型</strong></li>
</ol>
<p>&nbsp; &nbsp; &nbsp; &nbsp; 设概率密度为$f(x;\theta),$$\theta$为待估参数， $\theta\in \Theta$,$X_1,x_2, \dots ,X_n$是 来自总体$X$的样本，若$x_1,x_2,\dots,x_n$为相应于$X_1,X_2,\dots,X_n$的样本值，</p>
<p>$$<br>L(\theta)=L(x_1,x_2,…,x_n;\theta)\prod f(x_i;\theta)<br>$$<br>$\hat \theta(x_1,x_2,\dots,x_n)$,参数$\theta$的极大似然估计值。</p>
<p>极大似然法求估计值的步骤：（一般情况下）</p>
<p>1）构造似然函数$L(\theta):$<br>$$<br>L(\theta)=\prod p(x_i;\theta)(离散型)<br>$$</p>
<p>$$<br>L(\theta)=\prod f(x_i;\theta)(离散型)<br>$$</p>
<p>2)取对数：$ln  L(\theta)$;</p>
<p>3)令 $d ln L  \over d\theta$=$0$;</p>
<p>4)解似然方程得到$\theta$ 的极大似然估计值$\hat\theta$</p>
<p><strong>说明</strong>：若似然方程（组）无解，或似然函数不可导，此法失效，改用其他方法。</p>
<p><strong>例1</strong>： 设$X$服从 参数$\lambda(\lambda&gt;0)$的泊松分布，$x_1,x_2,\dots,x_n$是来自于$X$的一个样本值,求$\lambda$的极大似然估计值</p>
<p><strong>解</strong>：因为$X$的分布律为</p>
<p>$P{X=x}$=${\lambda^x \over x! }e^{-\lambda}$, $(x=0,1,2,\dots,n)$</p>
<p>所以$\lambda$的似然函数为<br>$$<br>L(\lambda)=\prod_{i=1}^{n} ({\lambda ^{x_i} \over x_i!  }{e^{-\lambda}})=e^{-n\lambda}{\lambda^{\sum_{i=1}^nx_i}\over \prod_{i=1}^{n}(x_i!)}<br>$$</p>
<p>$$<br>ln  L(\lambda)  = -n\lambda+(\sum_{i=1}^{n}x_i)ln\lambda-\sum_{i=1}^{n}{(x_i!)},<br>$$</p>
<p>令${d\over{d \lambda}}lnL(\lambda)  = -n+{\sum_{i=1}^nx_i \over \lambda} = 0$</p>
<p>解得$\lambda$的极大似然估计值为$\hat\lambda={1\over n}\sum_{i=1}^nx_i=\overline x$</p>
<p>这有估计值与矩估计值是相同的。</p>
<p><strong>例2</strong>  设总体$X \sim N(\mu,\sigma^2)$,$\mu,\delta^2$为未知参数， $x_1,x_2,\dots,x_n$是来自$X$的一个样本值，求$\mu,\sigma^2$的极大似然估计值。</p>
<p><strong>解</strong>：$X$的概率密度为$f(x;\mu,\sigma^2)={1 \over \sqrt{2\pi}\sigma  }e^{-{(x-\mu)^2 \over 2\sigma^2}}$,</p>
<p>似然函数为<br>$$<br>L(\mu,\sigma ^2)= \prod_{i=1}^n{1 \over \sqrt{2\pi}}e{(x_i-\mu)^2 \over 2\sigma^2},<br>$$</p>
<p>$$<br>lnL(\mu,\sigma ^2)=-{n\over 2}ln(2\pi)-{n\over 2}ln\sigma^2-{1\over 2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2,<br>$$</p>
<p>令<br>$$</p>
<p>f(n) =<br>\begin{cases}<br>{\partial \over \partial\mu}lnL(\mu,\sigma^2)=0   \\<br>{\partial \over \partial\sigma^2}lnL(\mu,\sigma^2)=0   \\<br>\end{cases}</p>
<p>$$</p>
<p>$$<br>\begin{cases}<br>{1\over \sigma^2} [\sum_{i=1}^nx_i-n\mu]=0,\dots(1) \\<br>-{n\over 2\sigma^2}+{1\over (\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2=0,\dots,(2)<br>\end{cases}<br>$$</p>
<p>故$\mu$和$\sigma^2$的 极大似然估计值为</p>
<p>&nbsp;$\hat\mu={1\over n}\sum_{i=1}^n\overline x$,$   \,\hat\sigma^2={1\over n}\sum_{i=1}^n(x_i-\overline x )$</p>
<p>这一估计值与矩估计值是相同的。</p>
<p><strong>例3</strong> 设总体$X$服从$[0,\theta]$上的均匀分布，$\theta&gt;0$未知，$x_1,x_2,\dots,x_n$是来自总体$X$的样本值，求出$\theta$的极大似然估计值。</p>
<p><strong>解</strong>：记$x_{(h)}=max(x_1,x_2,\dots,x_n)$,</p>
<p>$X$的概率密度为$f(x;\theta)=\begin{cases} {1\over \theta}, 0\le x \le \theta \\0,其他 \end{cases}$</p>
<p>所以似然函数为$L(\theta)=\begin{cases} {1\over \theta^n},x_{(h)} \le \theta \\0,其他 \end{cases}$</p>
<p>对于满足$x_{h}\le\theta$的任意$\theta$有</p>
<p>$$<br>L(\theta)={1\over\theta^n\le{1\over(x_{(h)})^n}}<br>$$</p>
<p>即似然函数$L(\theta)$在$\theta=x_h$时取得极大值，$\theta$的极大似然估计值为$\hat\theta=x_{(h)}=max_{1\le i\le n}x_i$</p>
<p>这一估计值与矩估计是不相同的。</p>
<p>###矩法估计值与极大似然估计值的比较</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">矩法估计法</th>
<th style="text-align:center">极大似然估计法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">依 据</td>
<td style="text-align:center">大数定律</td>
<td style="text-align:center">极大似然思想</td>
</tr>
<tr>
<td style="text-align:center">运 算</td>
<td style="text-align:center">较简单（可能会有信息量损失）</td>
<td style="text-align:center">较复杂</td>
</tr>
<tr>
<td style="text-align:center">精 度</td>
<td style="text-align:center">一般较低</td>
<td style="text-align:center">一般较高</td>
</tr>
</tbody>
</table>
<p>注意：</p>
<p>1）矩法估计值与极大似然估计值不一定相同</p>
<p>2）不是所以极大似然估计法都需要建立似然方程求解。</p>
<p>​    最大似然估计是似然函数最初也是最自然的应用。上文已经提到，似然函数取得最大值表示相应的参数能够使得统计模型最为合理。从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是概率密度函数或概率质量函数），整理之后求最大值。实际应用中一般会取<strong>似然函数的对数作为求最大值的函数</strong>，这样求出的最大值和直接求最大值得到的结果是相同的。<strong>似然函数的最大值不一定唯一，也不一定存在</strong>。与矩法估计比较，最大似然估计的精确度较高，信息损失较少，但计算量较大。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就不难理解，在$data  mining$领域，许多求参数的方法最终都归结为最大化似然概率的问题。回到这个硬币的例子上来，在观测到$HH$的情况下，$pH = 1$$是最合理的（却未必符合真实情况，因为数据量太少的缘故）。</p>
<p>参考：</p>
<ol>
<li>极大似然估计法的原理和方法PPT</li>
</ol>
<ol>
<li><a href="https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0</a></li>
<li><a href="http://www.cnblogs.com/zhsuiy/p/4822020.html" target="_blank" rel="noopener">http://www.cnblogs.com/zhsuiy/p/4822020.html</a></li>
</ol>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title:  '极大似然估计法MLE',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[matplotlib-1学习]]></title>
      <url>/2018/02/02/matplotlib1/</url>
      <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">print(plt.plot([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>第1句，这是导入Matplotlib接口的主要子模块pyplot绘图的首选格式。这是最好的做法，也是为了避免对全局名称空间的污染，强烈鼓励永远不要像这样导入接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> &lt;module&gt; <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<p>第2句，此代码行是实际的绘图命令。我们只指定了一个值列表，这些值表示要绘制的点的垂直坐标。matplotlib将使用一个隐式的水平值列表，从0（前值）到n-1（n为列表中的条目数），纵轴表示Y轴，横坐标表示X轴。</p>
<p>第3句，这条命令实际上是打开包含绘图图像的窗口。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = range(<span class="number">6</span>)</span><br><span class="line">plt.plot(x, [xi**<span class="number">2</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="1-多线图"><a href="#1-多线图" class="headerlink" title="1.多线图"></a>1.多线图</h2><p>如果需要在一个图中画出多条线，我们只需要在show之前多次plot就可以了。Matplotlib会给每条线自动选择不同颜色。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = range(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">plt.plot(x, [xi*<span class="number">1.5</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.plot(x, [xi*<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.plot(x, [xi/<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>也可以写成下列形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, [xi*<span class="number">1.5</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x], x, [xi*<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x], x,[xi/<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, x*<span class="number">1.5</span>, x, x*<span class="number">3.0</span>, x, x/<span class="number">3.0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2、网格"><a href="#2、网格" class="headerlink" title="2、网格"></a>2、网格</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.grid(<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3、坐标轴"><a href="#3、坐标轴" class="headerlink" title="3、坐标轴"></a>3、坐标轴</h2><p>你可能已经注意到，matplotlib为了精确地包含绘制的数据集自动设置图的界限。有时我们想自己设置坐标轴的界限。我们可以这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.py</span><br><span class="line">plot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> npx = np.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">plt.plot(x, x*<span class="number">1.5</span>, x, x*<span class="number">3.0</span>, x, x/<span class="number">3.0</span>)</span><br><span class="line">print(plt.axis()) <span class="comment"># shows the current axis limits values，</span></span><br><span class="line"><span class="comment"># (0.85, 4.15, -0.25000000000000006, 12.583333333333334)</span></span><br><span class="line"><span class="comment"># plt.axis([0, 5, -1, 13]) 		# 	set new axes limits</span></span><br><span class="line"><span class="comment"># [0, 5, -1, 13]</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>如果axis()函数没有参数，它的返回值是实际的界限，有两种方式给这个函数传，一是通过四个值得列表[xmin, xmax, ymin, ymax] ，二是键值对的方式。</p>
<p>我们也可以只限制某个轴的最大值或者最小值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.axis(xmin=NNN, ymax=NNN)</span><br></pre></td></tr></table></figure>
<p>或者通过xlim()和ylim()函数只限制某一个坐标轴的界限。</p>
<h2 id="4、给坐标增加一个标签"><a href="#4、给坐标增加一个标签" class="headerlink" title="4、给坐标增加一个标签"></a>4、给坐标增加一个标签</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">plt.xlabel(<span class="string">'This is the X axis'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'This is the Y axis'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="5、增加一个标题"><a href="#5、增加一个标题" class="headerlink" title="5、增加一个标题"></a>5、增加一个标题</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">'Simple plot'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="6、增加图例"><a href="#6、增加图例" class="headerlink" title="6、增加图例"></a>6、增加图例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, x*<span class="number">1.5</span>, label=<span class="string">'Normal'</span>)</span><br><span class="line">plt.plot(x, x*<span class="number">3.0</span>, label=<span class="string">'Fast'</span>)</span><br><span class="line">plt.plot(x, x/<span class="number">3.0</span>, label=<span class="string">'Slow'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<h2 id="7、保存图到文件"><a href="#7、保存图到文件" class="headerlink" title="7、保存图到文件"></a>7、保存图到文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.savefig(<span class="string">'plot123.png'</span>)</span><br><span class="line">plt.savefig(<span class="string">'plot123.png'</span>, dpi=<span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<h2 id="8、Plot-支持可选的第三个参数"><a href="#8、Plot-支持可选的第三个参数" class="headerlink" title="8、Plot()支持可选的第三个参数"></a>8、Plot()支持可选的第三个参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X, Y, <span class="string">'&lt;format&gt;'</span>, ...)</span><br></pre></td></tr></table></figure>
<p>plot方法的关键字参数color(或c)用来设置线的颜色。</p>
<p>控制颜色</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(y, <span class="string">'y'</span>)</span><br><span class="line">plt.plot(y+<span class="number">1</span>, <span class="string">'m'</span>)</span><br><span class="line">plt.plot(y+<span class="number">2</span>, <span class="string">'c'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>1）颜色的缩写</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>Color abbreviation </strong></th>
<th style="text-align:center"><strong>Color Name </strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">b</td>
<td style="text-align:center">blue</td>
</tr>
<tr>
<td style="text-align:center">c</td>
<td style="text-align:center">cyan</td>
</tr>
<tr>
<td style="text-align:center">g</td>
<td style="text-align:center">green</td>
</tr>
<tr>
<td style="text-align:center">k</td>
<td style="text-align:center">black</td>
</tr>
<tr>
<td style="text-align:center">m</td>
<td style="text-align:center">magenta</td>
</tr>
<tr>
<td style="text-align:center">r</td>
<td style="text-align:center">red</td>
</tr>
<tr>
<td style="text-align:center">w</td>
<td style="text-align:center">white</td>
</tr>
<tr>
<td style="text-align:center">y</td>
<td style="text-align:center">yellow 2</td>
</tr>
</tbody>
</table>
<p>2）十六进制字符 #FF00FF</p>
<p>3）(r, g, b) 或 (r, g, b, a)，其中 r g b a 取均为[0, 1]之间</p>
<p>4）[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色</p>
<p>也可写成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x1, y1, fmt1, x2, y2, fmt2, ...)</span><br><span class="line">plt.plot(y, <span class="string">'y'</span>, y+<span class="number">1</span>, <span class="string">'m'</span>, y+<span class="number">2</span>, <span class="string">'c'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="9、控制线条样式"><a href="#9、控制线条样式" class="headerlink" title="9、控制线条样式"></a>9、控制线条样式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(y, <span class="string">'--'</span>, y+<span class="number">1</span>, <span class="string">'-.'</span>, y+<span class="number">2</span>, <span class="string">':'</span>)</span><br></pre></td></tr></table></figure>
<p>-  solid</p>
<p>–  dashed</p>
<p>-.  dashdot</p>
<p>:    dotted</p>
<p>‘’, ‘ ‘,    None</p>
<h2 id="10、控制标记样式"><a href="#10、控制标记样式" class="headerlink" title="10、控制标记样式"></a>10、控制标记样式</h2><table>
<thead>
<tr>
<th style="text-align:center"><strong>Marker abbreviation </strong></th>
<th style="text-align:center"><strong>Marker style </strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">.</td>
<td style="text-align:center">Point marker</td>
</tr>
<tr>
<td style="text-align:center">,</td>
<td style="text-align:center">Pixel marker</td>
</tr>
<tr>
<td style="text-align:center">o</td>
<td style="text-align:center">Circle marker</td>
</tr>
<tr>
<td style="text-align:center">v</td>
<td style="text-align:center">Triangle down marker</td>
</tr>
<tr>
<td style="text-align:center">^</td>
<td style="text-align:center">Triangle up marker</td>
</tr>
<tr>
<td style="text-align:center">&lt;</td>
<td style="text-align:center">Triangle left marker</td>
</tr>
<tr>
<td style="text-align:center">&gt;</td>
<td style="text-align:center">Triangle right marker</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">Tripod down marker</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">Tripod up marker</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">Tripod left marker</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">Tripod right marker</td>
</tr>
<tr>
<td style="text-align:center">s</td>
<td style="text-align:center">Square marker</td>
</tr>
<tr>
<td style="text-align:center">p</td>
<td style="text-align:center">Pentagon marker</td>
</tr>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:center">Star marker</td>
</tr>
<tr>
<td style="text-align:center">h</td>
<td style="text-align:center">Hexagon marker</td>
</tr>
<tr>
<td style="text-align:center">H</td>
<td style="text-align:center">Rotated hexagon marker</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">Plus marker</td>
</tr>
<tr>
<td style="text-align:center">x</td>
<td style="text-align:center">Cross (x) marker</td>
</tr>
<tr>
<td style="text-align:center">D</td>
<td style="text-align:center">Diamond marker</td>
</tr>
<tr>
<td style="text-align:center">d</td>
<td style="text-align:center">Thin diamond marker</td>
</tr>
<tr>
<td style="text-align:center">\</td>
<td style="text-align:center"></td>
<td>Vertical line (vline symbol) marker</td>
</tr>
<tr>
<td style="text-align:center">_</td>
<td style="text-align:center">Horizontal line (hline symbol) marker</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y = np.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.3</span>)</span><br><span class="line">plt.plot(y, <span class="string">'cx--'</span>, y+<span class="number">1</span>, <span class="string">'mo:'</span>, y+<span class="number">2</span>, <span class="string">'kp-.'</span>);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>传递关键字参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y = np.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.3</span>)</span><br><span class="line">plt.plot(y, color=<span class="string">'blue'</span>, linestyle=<span class="string">'dashdot'</span>, linewidth=<span class="number">4</span>,</span><br><span class="line">marker=<span class="string">'o'</span>, markerfacecolor=<span class="string">'red'</span>, markeredgecolor=<span class="string">'black'</span>,</span><br><span class="line">markeredgewidth=<span class="number">3</span>, markersize=<span class="number">12</span>);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>如果想画两条一样颜色的线，可以这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x1, y1, x2, y2, color=<span class="string">'green'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="11、X轴和Y轴的刻度"><a href="#11、X轴和Y轴的刻度" class="headerlink" title="11、X轴和Y轴的刻度"></a>11、X轴和Y轴的刻度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locs, labels = plt.xticks()</span><br></pre></td></tr></table></figure>
<p>不加任何参数的画，tick函数返回当前位置和标签，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(plt.xticks())</span><br><span class="line"><span class="comment">#(array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]), &lt;a list of 6 Text xticklabel objects&gt;)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line">x = [<span class="number">5</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>] </span><br><span class="line">plt.plot(x); </span><br><span class="line">plt.xticks(range(len(x)), [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>]); </span><br><span class="line">plt.yticks(range(<span class="number">1</span>, <span class="number">8</span>, <span class="number">2</span>)); </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>使用字符来代表x的刻度</p>
<h2 id="12、中文字体不显示的问题"><a href="#12、中文字体不显示的问题" class="headerlink" title="12、中文字体不显示的问题"></a>12、中文字体不显示的问题</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>] <span class="comment"># 指定默认字体</span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span> <span class="comment"># 解决保存图像是负号'-'显示为方块的问题</span></span><br></pre></td></tr></table></figure>
<p>参考：Matplotlib for python development</p>
<p>python中matplotlib的颜色及线条控制：<a href="http://blog.csdn.net/qq_26376175/article/details/67637151" target="_blank" rel="noopener">http://blog.csdn.net/qq_26376175/article/details/67637151</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'matplotlib-1学习',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> matplot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> matplot </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[linux-4：linux配置信息]]></title>
      <url>/2018/01/30/linux4/</url>
      <content type="html"><![CDATA[<h2 id="1-操作系统信息"><a href="#1-操作系统信息" class="headerlink" title="1.操作系统信息"></a>1.操作系统信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -a</span><br></pre></td></tr></table></figure>
<h2 id="2-CPU相关信息"><a href="#2-CPU相关信息" class="headerlink" title="2.CPU相关信息"></a>2.CPU相关信息</h2><p>1) lscpu:显示CPU架构信息</p>
<p>$ lscpu</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                4　　　　　　　　　　#总处理器核心数量(虚拟)</span><br><span class="line">On-line CPU(s) list:   0-3</span><br><span class="line">Thread(s) per core:    1　　　　　　　　　#每个核心支持的线程数量。1表示只支持一个线程，即不支持超线程</span><br><span class="line">Core(s) per socket:    1　　　　　　　　　#每个处理器的核心数量</span><br><span class="line">Socket(s):             4　　　　　　　　　#处理器数量</span><br><span class="line">NUMA node(s):          1</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6   #CPU家族编号6</span><br><span class="line">Model:                 63     #代表处理器内部型号15</span><br><span class="line">Stepping:              0     </span><br><span class="line">CPU MHz:               2599.998</span><br><span class="line">BogoMIPS:              5199.99</span><br><span class="line">Hypervisor vendor:     VMware　　　　　　　#管理程序供应商</span><br><span class="line">Virtualization type:   full</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              256K</span><br><span class="line">L3 cache:              30720K</span><br><span class="line">NUMA node0 CPU(s):     0-3</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>2)总核数 = 物理CPU个数 X 每颗物理CPU的核数</p>
<p>总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数</p>
<p>查看物理CPU个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l</span><br></pre></td></tr></table></figure>
<p>3)查看每个物理CPU中core的个数(即核数)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br></pre></td></tr></table></figure>
<p>4)查看逻辑CPU的个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l</span><br></pre></td></tr></table></figure>
<p>5) 查看CPU信息（型号）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</span><br></pre></td></tr></table></figure>
<h2 id="3-内存信息"><a href="#3-内存信息" class="headerlink" title="3.内存信息"></a>3.内存信息</h2><p>查看当前系统的内存使用情况 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">free -m    # 以M为单位，-g表示以G为单位</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/meminfo</span><br></pre></td></tr></table></figure>
<h2 id="4-硬盘信息"><a href="#4-硬盘信息" class="headerlink" title="4.硬盘信息"></a>4.硬盘信息</h2><p>1)lsblk：blk是block的缩写。列出块设备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[xxx@localhost ~]$ lsblk</span><br><span class="line">NAME                        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sr0                          11:0    1 1024M  0 rom  </span><br><span class="line">sdb                           8:16   0  100G  0 disk </span><br><span class="line">└─sdb1                        8:17   0  100G  0 part /data</span><br><span class="line">sda                           8:0    0   60G  0 disk </span><br><span class="line">├─sda1                        8:1    0  500M  0 part /boot</span><br><span class="line">└─sda2                        8:2    0 59.5G  0 part </span><br><span class="line">  ├─VolGroup-lv_root (dm-0) 253:0    0   50G  0 lvm  /</span><br><span class="line">  ├─VolGroup-lv_swap (dm-1) 253:1    0    4G  0 lvm  [SWAP]</span><br><span class="line">  └─VolGroup-lv_home (dm-2) 253:2    0  5.6G  0 lvm  /home</span><br></pre></td></tr></table></figure>
<p>其中，TYPE=disk表示硬盘。可以看出，硬盘分为sda和sdb，一共160G。</p>
<p>2) df：查看硬盘使用情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[xxx@localhost ~]$ df -h</span><br><span class="line">Filesystem                    Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/VolGroup-lv_root   50G  1.7G   46G   4% /</span><br><span class="line">tmpfs                         3.9G     0  3.9G   0% /dev/shm</span><br><span class="line">/dev/sda1                     485M   39M  421M   9% /boot</span><br><span class="line">/dev/mapper/VolGroup-lv_home  5.5G  165M  5.1G   4% /home</span><br><span class="line">/dev/sdb1                      99G  188M   94G   1% /data</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p><a href="http://www.cnblogs.com/emanlee/p/3587571.html" target="_blank" rel="noopener">http://www.cnblogs.com/emanlee/p/3587571.html</a></p>
<p><a href="https://www.cnblogs.com/alwu007/p/6024631.html" target="_blank" rel="noopener">https://www.cnblogs.com/alwu007/p/6024631.html</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'linux-4：linux配置信息',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> liunx </category>
            
        </categories>
        
        
        <tags>
            
            <tag> linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[linux-3：top命令]]></title>
      <url>/2018/01/26/linux3/</url>
      <content type="html"><![CDATA[<h2 id="1-作用"><a href="#1-作用" class="headerlink" title="1.作用"></a>1.作用</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;显示linux任务。top程序提供运行系统的动态实时视图。它可以显示系统概要信息以及当前由Linux内核管理的任务列表。所显示的系统摘要信息的类型以及任务所显示的信息的类型、顺序和大小都是用户可配置的，并且可以在重新启动时对该配置进行持久化。</p>
<h2 id="2-语法"><a href="#2-语法" class="headerlink" title="2.语法"></a>2.语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top(选项)</span><br></pre></td></tr></table></figure>
<h2 id="3-常用选项"><a href="#3-常用选项" class="headerlink" title="3.常用选项"></a>3.常用选项</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-b：以批处理模式操作；</span><br><span class="line">-c：显示完整的命令，例如使用的命令python test.py；</span><br><span class="line">-d：指定屏幕刷新间隔时间，并覆盖个人配置文件或启动默认值中的相应值， -d ss.tt (seconds.tenths)；</span><br><span class="line">-H：线程切换，当加上这个选项的时候，只会显示所有个人的显示</span><br><span class="line">-s：保密模式；</span><br><span class="line">-S：累积模式；</span><br><span class="line">-i&lt;时间&gt;：设置间隔时间；</span><br><span class="line">-u&lt;用户名&gt;：指定用户名，显示指定用户线程；</span><br><span class="line">-p&lt;进程号&gt;：指定进程；</span><br><span class="line">-n&lt;次数&gt;：循环显示的次数。</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="4-top交互命令"><a href="#4-top交互命令" class="headerlink" title="4.top交互命令"></a>4.top交互命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">回车键或者空格键：刷新显示</span><br><span class="line">&quot;?&quot;或&quot;h&quot;：显示帮助画面，给出一些简短的命令总结说明；</span><br><span class="line">k：终止一个进程；</span><br><span class="line">i：忽略闲置和僵死进程，这是一个开关式命令；</span><br><span class="line">q：退出程序</span><br><span class="line">r：重新安排一个进程的优先级别；</span><br><span class="line">S：切换到累计模式；</span><br><span class="line">s：改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s；</span><br><span class="line">m：切换显示内存信息；</span><br><span class="line">t：切换显示进程和CPU状态信息；</span><br><span class="line">c：切换显示命令名称和完整命令行；</span><br><span class="line">M：根据驻留内存大小进行排序；</span><br><span class="line">P：根据CPU使用百分比大小进行排序；p</span><br><span class="line">w：将当前设置写入~/.toprc文件中。</span><br></pre></td></tr></table></figure>
<h2 id="5-例子"><a href="#5-例子" class="headerlink" title="5.例子"></a>5.例子</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">top - 09:44:56 up 16 days, 21:23,  1 user,  load average: 9.59, 4.75, 1.92</span><br><span class="line">Tasks: 145 total,   2 running, 143 sleeping,   0 stopped,   0 zombie</span><br><span class="line">Cpu(s): 99.8%us,  0.1%sy,  0.0%ni,  0.2%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Mem:   4147888k total,  2493092k used,  1654796k free,   158188k buffers</span><br><span class="line">Swap:  5144568k total,       56k used,  5144512k free,  2013180k cached</span><br></pre></td></tr></table></figure>
<ul>
<li>top - 09:44:56[当前系统时间],</li>
</ul>
<ul>
<li>16 days[系统已经运行了16天],</li>
<li>1 user[个用户当前登录],</li>
<li>load average: 9.59, 4.75, 1.92[系统负载，即任务队列的平均长度]</li>
<li>Tasks: 145 total[总进程数],</li>
<li>2 running[正在运行的进程数],</li>
<li>143 sleeping[睡眠的进程数],</li>
<li>0 stopped[停止的进程数],</li>
<li>0 zombie[冻结进程数],</li>
<li>Cpu(s): 99.8%us[用户空间占用CPU百分比],</li>
<li>0.1%sy[内核空间占用CPU百分比],</li>
<li>0.0%ni[用户进程空间内改变过优先级的进程占用CPU百分比],</li>
<li>0.2%id[空闲CPU百分比], 0.0%wa[等待输入输出的CPU时间百分比],</li>
<li>0.0%hi[硬中断（Hardware IRQ）占用CPU的百分比],</li>
<li>0.0%si [] 软中断（Software Interrupts）占用CPU的百分比]</li>
<li>0.0%st[],</li>
<li>Mem: 4147888k total[物理内存总量],</li>
<li>2493092k used[使用的物理内存总量],</li>
<li>1654796k free[空闲内存总量],</li>
<li>158188k buffers[用作内核缓存的内存量]</li>
<li>Swap:  5144568k total[交换区总量],</li>
<li>56k used[使用的交换区总量],</li>
<li>5144512k free[空闲交换区总量],</li>
<li>2013180k cached[缓冲的交换区总量],</li>
</ul>
<p><img src="/2018/01/26/linux3/a.png" alt="top命令视图"></p>
<ul>
<li>PID 进程id</li>
<li>USER 进程所有者</li>
<li>PR 进程优先级</li>
<li>NI nice值。负值表示高优先级，正值表示低优先级</li>
<li>VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES</li>
<li>RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA</li>
<li>SHR 共享内存大小，单位kb</li>
<li>S 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程</li>
<li>%CPU 上次更新到现在的CPU时间占用百分比</li>
<li>%MEM 进程使用的物理内存百分比</li>
<li>TIME+ 进程使用的CPU时间总计，单位1/100秒</li>
<li>COMMAND 进程名称（命令名/命令行）</li>
</ul>
<p><strong>注意</strong>：TIME+代表的是进程使用CPU的时间，不是进程启动到现在的时间。因此，如果一个人进程使用CPU的时间很少，即使这个进程已经存在很长时间了，TIME+也是个很小的数值。如果使用多个CPU或者是多核的CPU，那么这个时间是多个CPU的时间的和。例如16:39.33,表示16分钟39秒十分之3秒百分之3秒。</p>
<p><a href="http://man.linuxde.net/top" target="_blank" rel="noopener">http://man.linuxde.net/top</a></p>
<p><a href="https://www.cnblogs.com/ronli/p/centos-top.html" target="_blank" rel="noopener">https://www.cnblogs.com/ronli/p/centos-top.html</a></p>
<p><a href="https://linux.die.net/man/1/top" target="_blank" rel="noopener">https://linux.die.net/man/1/top</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'linux-3：top命令',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>















]]></content>
      
        <categories>
            
            <category> liunx </category>
            
        </categories>
        
        
        <tags>
            
            <tag> linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[errors1]]></title>
      <url>/2018/01/23/errors1/</url>
      <content type="html"><![CDATA[<p>​    今天又遇到换行符的错误。想起以前换行符中遇到的坑。在不同的操作系统默认的换行符不一样，字符处理的时候会遇到一些莫名奇妙的问题。windows的换行是\r\n，linux的是\n，mac的是\r。在网络编程中，如果把Windows的一个文件数据传输到linux系统中的另一个文件，如果计算两个文件的字节数会发现两个系统同样的文本数据字节数会不一样。就是因为换行符导致的，通常模式下，换行符，空格等都是空白符，不可见的。在notepad++中视图显示所有字符会发现Windows中的换行符为CRLF，而linux中换行符为LF。利用notepad++可以将Windows文本文件转换为unix格式的文件。</p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id:  'window.location.pathname', // 可选。默认为 location.href
  title:  'errors1',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> error </category>
            
        </categories>
        
        
        <tags>
            
            <tag> errors </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[markdown学习]]></title>
      <url>/2018/01/23/markdown/</url>
      <content type="html"><![CDATA[<p>special symbol in markdown</p>
<p><a href="http://www.math.harvard.edu/texman/node21.html#SECTION00084000000000000000" target="_blank" rel="noopener">http://www.math.harvard.edu/texman/node21.html#SECTION00084000000000000000</a></p>
<h2 id="1-空白类型列举"><a href="#1-空白类型列举" class="headerlink" title="1.空白类型列举"></a>1.空白类型列举</h2><table>
<thead>
<tr>
<th style="text-align:center">两个quad空格</th>
<th style="text-align:center">a \qquad b</th>
<th style="text-align:center">ab</th>
<th style="text-align:center">两个<em>m</em>的宽度</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">quad空格</td>
<td style="text-align:center">a \quad b</td>
<td style="text-align:center">ab</td>
<td style="text-align:center">一个<em>m</em>的宽度</td>
</tr>
<tr>
<td style="text-align:center">大空格</td>
<td style="text-align:center">a b</td>
<td style="text-align:center">a b</td>
<td style="text-align:center">1/3<em>m</em>宽度</td>
</tr>
<tr>
<td style="text-align:center">中等空格</td>
<td style="text-align:center">a\;b</td>
<td style="text-align:center">ab</td>
<td style="text-align:center">2/7<em>m</em>宽度</td>
</tr>
<tr>
<td style="text-align:center">小空格</td>
<td style="text-align:center">a\,b</td>
<td style="text-align:center">ab</td>
<td style="text-align:center">1/6<em>m</em>宽度</td>
</tr>
<tr>
<td style="text-align:center">没有空格</td>
<td style="text-align:center">ab</td>
<td style="text-align:center">ab</td>
<td style="text-align:center">正常宽度</td>
</tr>
<tr>
<td style="text-align:center">紧贴</td>
<td style="text-align:center">a!b</td>
<td style="text-align:center">ab</td>
<td style="text-align:center">缩进1/6<em>m</em>宽度</td>
</tr>
</tbody>
</table>
<p>偏导 \partial $ \partial$</p>
<a id="more"></a>
<h2 id="2-希腊字母"><a href="#2-希腊字母" class="headerlink" title="2.希腊字母"></a>2.希腊字母</h2><table>
<thead>
<tr>
<th style="text-align:center">输入</th>
<th style="text-align:center">显示</th>
<th style="text-align:center">输入</th>
<th style="text-align:center">显示</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>$\alpha$</code></td>
<td style="text-align:center">αα</td>
<td style="text-align:center"><code>$A$</code></td>
<td style="text-align:center">AA</td>
</tr>
<tr>
<td style="text-align:center"><code>$\beta$</code></td>
<td style="text-align:center">ββ</td>
<td style="text-align:center"><code>$B$</code></td>
<td style="text-align:center">BB</td>
</tr>
<tr>
<td style="text-align:center"><code>$\gamma$</code></td>
<td style="text-align:center">γγ</td>
<td style="text-align:center"><code>$\Gamma$</code></td>
<td style="text-align:center">ΓΓ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\delta$</code></td>
<td style="text-align:center">δδ</td>
<td style="text-align:center"><code>$\Delta$</code></td>
<td style="text-align:center">ΔΔ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\epsilon$</code></td>
<td style="text-align:center">ϵϵ</td>
<td style="text-align:center"><code>$E$</code></td>
<td style="text-align:center">EE</td>
</tr>
<tr>
<td style="text-align:center"><code>$\zeta$</code></td>
<td style="text-align:center">ζζ</td>
<td style="text-align:center"><code>$Z$</code></td>
<td style="text-align:center">ZZ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\eta$</code></td>
<td style="text-align:center">ηη</td>
<td style="text-align:center"><code>$H$</code></td>
<td style="text-align:center">HH</td>
</tr>
<tr>
<td style="text-align:center"><code>$\theta$</code></td>
<td style="text-align:center">θθ</td>
<td style="text-align:center"><code>$\Theta$</code></td>
<td style="text-align:center">ΘΘ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\iota$</code></td>
<td style="text-align:center">ιι</td>
<td style="text-align:center"><code>$I$</code></td>
<td style="text-align:center">II</td>
</tr>
<tr>
<td style="text-align:center"><code>$\kappa$</code></td>
<td style="text-align:center">κκ</td>
<td style="text-align:center"><code>$K$</code></td>
<td style="text-align:center">KK</td>
</tr>
<tr>
<td style="text-align:center"><code>$\lambda$</code></td>
<td style="text-align:center">λλ</td>
<td style="text-align:center"><code>$\Lambda$</code></td>
<td style="text-align:center">ΛΛ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\nu$</code></td>
<td style="text-align:center">νν</td>
<td style="text-align:center"><code>$N$</code></td>
<td style="text-align:center">NN</td>
</tr>
<tr>
<td style="text-align:center"><code>$\mu$</code></td>
<td style="text-align:center">μμ</td>
<td style="text-align:center"><code>$M$</code></td>
<td style="text-align:center">MM</td>
</tr>
<tr>
<td style="text-align:center"><code>$\xi$</code></td>
<td style="text-align:center">ξξ</td>
<td style="text-align:center"><code>$\Xi$</code></td>
<td style="text-align:center">ΞΞ</td>
</tr>
<tr>
<td style="text-align:center"><code>$o$</code></td>
<td style="text-align:center">oo</td>
<td style="text-align:center"><code>$O$</code></td>
<td style="text-align:center">OO</td>
</tr>
<tr>
<td style="text-align:center"><code>$\pi$</code></td>
<td style="text-align:center">ππ</td>
<td style="text-align:center"><code>$\Pi$</code></td>
<td style="text-align:center">ΠΠ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\rho$</code></td>
<td style="text-align:center">ρρ</td>
<td style="text-align:center"><code>$P$</code></td>
<td style="text-align:center">PP</td>
</tr>
<tr>
<td style="text-align:center"><code>$\sigma$</code></td>
<td style="text-align:center">σσ</td>
<td style="text-align:center"><code>$\Sigma$</code></td>
<td style="text-align:center">ΣΣ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\tau$</code></td>
<td style="text-align:center">ττ</td>
<td style="text-align:center"><code>$T$</code></td>
<td style="text-align:center">TT</td>
</tr>
<tr>
<td style="text-align:center"><code>$\upsilon$</code></td>
<td style="text-align:center">υυ</td>
<td style="text-align:center"><code>$\Upsilon$</code></td>
<td style="text-align:center">ΥΥ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\phi$</code></td>
<td style="text-align:center">ϕϕ</td>
<td style="text-align:center"><code>$\Phi$</code></td>
<td style="text-align:center">ΦΦ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\chi$</code></td>
<td style="text-align:center">χχ</td>
<td style="text-align:center"><code>$X$</code></td>
<td style="text-align:center">XX</td>
</tr>
<tr>
<td style="text-align:center"><code>$\psi$</code></td>
<td style="text-align:center">ψψ</td>
<td style="text-align:center"><code>$\Psi$</code></td>
<td style="text-align:center">ΨΨ</td>
</tr>
<tr>
<td style="text-align:center"><code>$\omega$</code></td>
<td style="text-align:center">ωω</td>
<td style="text-align:center"><code>$\Omega$</code></td>
<td style="text-align:center">ΩΩ</td>
</tr>
</tbody>
</table>
<h2 id="3-其他"><a href="#3-其他" class="headerlink" title="3.其他"></a>3.其他</h2><ul>
<li><strong>省略号 </strong>：\ldots 表示与文本底线对齐的省略号，\cdots 表示与文本中线对齐的省略号。</li>
<li><code>()</code>、<code>[]</code>和<code>|</code>表示符号本身，使用 <code>\{\}</code> 来表示 <code>{}</code>。当要显示大号的括号或分隔符时，要用 <code>\left</code> 和 <code>\right</code> 命令。</li>
</ul>
<p>一些特殊的括号：</p>
<table>
<thead>
<tr>
<th>输入</th>
<th>显示</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>$$\langle表达式\rangle$$</code></td>
<td>⟨表达式⟩⟨表达式⟩</td>
</tr>
<tr>
<td><code>$$\lceil表达式\rceil$$</code></td>
<td>⌈表达式⌉⌈表达式⌉</td>
</tr>
<tr>
<td><code>$$\lfloor表达式\rfloor$$</code></td>
<td>⌊表达式⌋⌊表达式⌋</td>
</tr>
<tr>
<td><code>$$\lbrace表达式\rbrace$$</code></td>
<td>{表达式}{表达式}</td>
</tr>
</tbody>
</table>
<p>例子：<br><code>$$ f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) $$</code></p>
<p>$f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right)$</p>
<ul>
<li><strong>分数</strong>：通常使用 <code>\frac {分子} {分母}</code>命令产生一个分数\frac {分子} {分母}，分数可嵌套。</li>
</ul>
<p>便捷情况可直接输入 <code>\frac ab</code>来快速生成一个\frac ab。<br>如果分式很复杂，亦可使用 分子 \over 分母 命令，此时分数仅有一层。</p>
<p>参考：<a href="https://www.cnblogs.com/q735613050/p/7253073.html" target="_blank" rel="noopener">https://www.cnblogs.com/q735613050/p/7253073.html</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'markdown学习',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>





























]]></content>
      
        <categories>
            
            <category> markdown </category>
            
        </categories>
        
        
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[序列标注-3：CRF介绍]]></title>
      <url>/2018/01/23/ner3/</url>
      <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;想象一下，你有来自贾斯汀·比伯的一天生活的的一系列快照，你想用它所代表的活动（吃饭、睡觉、开车等）来标记每个图像。你会怎么做？</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一种方法是忽略快照的顺序性质，并构建一个图像分类器。例如，给定一个月的被标记的快照，你可能知道，黑暗拍摄的图像往往是在早上睡觉，有耀眼的色彩图像往往是跳舞，汽车的图像往往在驾驶，等等。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而，忽略这个连续的方面，你会失去很多信息。例如，如果你看到一张嘴巴的特写照片是关于唱歌还是吃饭？如果你知道前一张图片是贾斯汀·比伯吃或做饭的照片，那么这张照片更有可能是关于吃的；如果，前一张图片包含贾斯汀·比伯唱歌或跳舞，那么这张照片可能暗示他在唱歌。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，为了增加我们的标注的精度，我们应该结合它附近的照片的标签，而这正是一个条件随机场所能做的。</p>
<h2 id="1-词性标注"><a href="#1-词性标注" class="headerlink" title="1.词性标注"></a>1.词性标注</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;让我们使用更为常见的词性标注示例来进一步研究一些细节。在词性标注中，目标是用ADJECTIVE、 NOUN、PREPOSITION、VERB、ADVERB、ARTICLE等标签来标记句子（单词或记号的序列）。例如，对于句子“Bob drank coffee at Starbucks”，标注为”Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)”。我们建立一个条件随机场对句子进行词性标注。就像任何的分类器，我们首先需要选择一组特征函数。</p>
<a id="more"></a>
<h2 id="2-CRF中的特征函数"><a href="#2-CRF中的特征函数" class="headerlink" title="2.CRF中的特征函数"></a>2.CRF中的特征函数</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在CRF中，每个特征函数都是采用以下特征作为输入，以实数为输出（一般是0或者1）</p>
<ul>
<li>一个句子</li>
<li>句子的第i个字</li>
<li>当前字的标签</li>
<li>前一个字的特征</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注意：通过限制我们的特性依赖于当前和以前的标签，而不是整个句子中的任意标签，我实际上构建了一个特殊的线性链CRF，这里将忽略一般的CRF。例如，一个可能的特征函数可以衡量在前一个字为“very”的情况下，当前单词应该被标记为一个形容词。</p>
<h2 id="3-特征到概率"><a href="#3-特征到概率" class="headerlink" title="3.特征到概率"></a>3.特征到概率</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来给每个特征函数$f_j$一个权重$\lambda_j$ （我们将会讨论怎么从数据中学习到这些权重）。对于给定的一个句话，我们现在可以累加句中所有字加权后的特征来得到标签序列$l$的分数：</p>
<p>$score(l|s)=\sum_{j=1=0}^m \sum_{i=1}^n \lambda_j f_j(s,i,l_i,l_{i-1})$</p>
<p>第一个求和是对每个特征函数，里面的求和是对句子的每个字。</p>
<p>最后，我们可以将这些得分通过指数函数然后归一化转换成在0到1之间的概率$p(l|s)$</p>
<p>$p(l|s)=$ $exp[score(l|s) \over \sum_{ l’}exp[score(l’|s)] $ =$exp[\sum_{j=1}^m \sum_{i=1}^n \lambda_j f_j (s,i,l_i,l_{i-1})  ]\over \sum_{ l’}exp[\sum_{j=1}^m \sum_{i=1}^n \lambda_j f_j (s,i,l_i,l_{i-1}) ] $ </p>
<h2 id="4-特征函数举例"><a href="#4-特征函数举例" class="headerlink" title="4.特征函数举例"></a>4.特征函数举例</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;词性标注中，可以包含以下特征：</p>
<ul>
<li>$f_1(s,i,l_i,l_{i-1})=1 \: $ 如果$\: l_i=ADVERB$ 和第$i$个词以“-ly”结尾;否则，特征函数为0。如果和这个特征关联的权重$\lambda_1$是大的正数，这个特征表明我们倾向于把以“-ly”结尾的词标注为“ADVERB”。</li>
<li>$f_2(s,i,l_i,l_{i-1})=1 \: $ 如果$ i=1,\: l_i=VERB$ 和这个句子一问号结尾时。否则，特征函数为0。同样，如果该特征的权重值$\lambda_2$ 为大的正数，就偏向于将问句里的第一个词标为VERB（例如 “Is this a sentence beginning with a verb?”）</li>
<li>$f_3(s,i,l_i,l_{i-1})=1 \: $ 如果$l_{i-1}=ADJECTIVE$ 和$l_i=NOUN$。否则特征函数为0。同样的，一个正数意味着“NOUN”跟着“ADJECTIVE”后面。</li>
<li>$f_3(s,i,l_i,l_{i-1})=1 \: $如果$l_{i-1}=PERPOSITION $和 $l_1=PERPOSTION$。一个负的权重$\lambda_4$对这个特征函数意味着“prepostion”后面不太可能跟着“preposition”，所以我们应该避免这种标签的产生。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就是全部。总的来说，为了建立一个条件随机场，你只需要定义一系列的特征函数（依赖整个句子，当前位置，和附近的标签），给它们赋上对应的权重，然后对它们求和。如果需要的话，在最后把它们变成概率。</p>
<p>现在让我们回到前面并且比较一下CRF和其他类似的机器学习方法。</p>
<h2 id="5-看起来像逻辑回归"><a href="#5-看起来像逻辑回归" class="headerlink" title="5.看起来像逻辑回归"></a>5.看起来像逻辑回归</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CRF的概率形式$p(l|s)=$$exp[\sum_{j=1}^m \sum_{i=1}^n \lambda_j f_j (s,i,l_i,l_{i-1})  \over \sum_{ l’}exp[\sum_{j=1}^m \sum_{i=1}^n \lambda_j f_j (s,i,l_i,l_{i-1})] $ 看起来有些熟悉。这是因为CRF是逻辑回归的序列化版本：逻辑回归是一个用于分类的对数线性模型，CRF是一个用于序列标注的对数线性模型。</p>
<h2 id="6-看起来像隐马尔可夫模型"><a href="#6-看起来像隐马尔可夫模型" class="headerlink" title="6.看起来像隐马尔可夫模型"></a>6.看起来像隐马尔可夫模型</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回忆隐马尔可夫模型是另外一个用于词性标注的模型。CRF是计算一系列特征函数的得分，而HMM是一个用于序列标注的生成式模型。定义</p>
<ul>
<li>$p(l,s)=p(l_1)\prod p(l_i|l_{i-1}) p(w_i|l_i)) $</li>
<li>$p(l_i|l_{i-1})$ 是转移概率（例如，介词后面是名词的概率）</li>
<li>$p(w_i|l_i)$ 是发射概率（例如，“dad”是名词的概率）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CRF跟HMM比较怎么样？CRF更强大，它可以建模所有HMM能建模的，甚至更多。下面举个例子：</p>
<p>注意到HMM的对数形式为$log\:p(l,s)=log\:p(l_0)+\sum_{i}\:log\:p(l_i|l_{i-1})+\sum_i\:p(w_i|l_i)$ 。如果我们考虑这些对数概率为关联二进制的转移和发射特征的权重。这实际上是CRF的一个对数线性形式。</p>
<p>这样，我们就可以通过一些方式建立一个与HMM等价的CRF模型：</p>
<ol>
<li>对HMM中的每个转移概率$p(l_i=y|l_{i-1}=x)$ ,定义一个人转移特征$f_{x,y}(s,i,l_i,l_{i-1})=1$ if $l_i=y\: and\:l_{i-1}=x$ 。给每个特征一个权重$w_{x,y}=log\:p(l_i=y|l_{i-1}=x)$ 。</li>
<li>类似地，对每个发射概率$p(w_i=z|l_i=x)$ 定义系列特征$g_{x,y}(s,i,l_i,l_{i-1})=1\:if \: w_i=z\:and \: l_i=x$ 。给每个特征一个权重$w_{x,z}=log\:p(w_i=z|l_i=x)$ 。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，$p(l|s)$ 通过CRF这些特征计算得到的分数与HMM计算的分数是成比例的，所有每个HMM都等价某个CRF。</p>
<p>然而，因为以下两个原因，CRF可以对更丰富的标签分布建模。</p>
<ol>
<li>CRF可以定义更大的特征集合。HMM本质是局部的（因为被限制到转移特征函数和发射特征函数，这些函数要求，每个词只取决于当前的标签，每个标签只依赖前一个标签），但是CRF可以使用更多的全局特征。以词性标注的一个特征为例，如果句子以问号结尾，这个词被标注为“VERB”的概率会增加。</li>
<li>CRF可以有任意的权重。HMM的概率必须满足某种约束（例如，$0&lt;=p(w_i|l_i)&lt;=1,\sum_{w} p(w_i=w|l_i)=1$）,但是CRF中的权重不受限制（例如$log\:p(w_i|l_i)$ 可以是任意的）</li>
</ol>
<h2 id="7-权重的学习"><a href="#7-权重的学习" class="headerlink" title="7.权重的学习"></a>7.权重的学习</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们回到如何学习CRF的权重这个问题。毫无疑问的一种方式是使用梯度下降。假设我们有一系列的训练实例（被词性标注的句子）。随机初始化我们CRF模型中的权重。为了调整这些随机初始化的权重到正确的权重，对于每一个训练实例：</p>
<ol>
<li>遍历每一个特征函数，然后对$\lambda_i$ 计算每一个训练实例对数概率的梯度。</li>
</ol>
<p>${\partial} \over { \partial w_j} $$log\:p(l|s)=\sum_{j=1}^m f_i(s,j,l_j,l_{j-1})-\sum_{l’} \:p(l’|s)\sum_{j=1}^m f_i(s,j,l’_j,l’_{j-1} )$</p>
<ol>
<li>注意梯度中的第一项是特征$f_i$ 在正确标注下的贡献，第二项是特征$f_i$在当前模型中的贡献。这就是你所期望采用的梯度上升公式。</li>
<li>将$\lambda_i$ 朝着梯度下降的方向移动：$ \lambda_i =  \lambda_i+\alpha[\sum_{j=1}^m f_i(s,j,l_{j},l_{j-1})-\sum_l’p(l’|s)\sum_{j=1}^{m} f_{i}(s,j,l_{j}^{‘},l_{j-1}^{‘})]$</li>
<li>重复以上步骤直到停止条件（例如 更新低于某个阈值）。</li>
</ol>
<p>换句话说，每一个步骤都在学习我们想要的模型和当前模型的状态的差异，然后使$\lambda_i$沿着这个差异的方向移动</p>
<h2 id="8-找到最优标注"><a href="#8-找到最优标注" class="headerlink" title="8.找到最优标注"></a>8.找到最优标注</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设我们已经训练好了我们的CRF模型，现在有一个新的句子，我们怎么对它进行标注呢？</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方式是对每个可能标签序列计算$p(l|s)$ ，然后选择是这个概率最大的标签序列。然而，对于有k种标签的集合和长度为m的句子，就会有$k^m$ 种可能的标签序列，时间复杂度是指数型。一个更好的方式是意识到线性链CRF满足最优化子结构性质，可以使用动态规划算法（多项式复杂度）找到最优标签序列，与隐马尔可夫模型中的维特比算法类似。</p>
<h2 id="9-一个更有趣的应用"><a href="#9-一个更有趣的应用" class="headerlink" title="9.一个更有趣的应用"></a>9.一个更有趣的应用</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;词性标注有些无聊，已经有很多词性标注器。什么时候会在生活中使用CRF呢？</p>
<p>假设你想从推特上挖掘人们在圣诞节上到礼物类型,你怎么知道哪些词是指礼物？</p>
<p>为了收集数据，我只是简单地寻找“我想要圣诞节balabala”和“我在圣诞节得到balabala”的短语。然而，一个更复杂的CRF的变种可以使用GIFT作为词性标注的一部分（甚至添加其他标签如gift-giver和gift-receiver，获得更多的信息，谁得到来自谁的东西），像对待一个词性标注问题。特征可以基于诸如如果前面的字是一个”gift-receiver”并且这个字之前是“gave“这个词是一个礼物或者如果这个词接下来的两个词是“for Christmas”，这个词也是一个礼物。</p>
<p>翻译自<a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/" target="_blank" rel="noopener">http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '序列标注-3：CRF介绍',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>

































]]></content>
      
        <categories>
            
            <category> ner </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ner </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[linux-2：用户目录下的配置文件]]></title>
      <url>/2018/01/22/linux2/</url>
      <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Ubuntu服务器的用户目录下，有以下几个文件：</p>
<ol>
<li>.bash_history</li>
<li>.bash_logout</li>
<li>.bashrc</li>
<li>.profile</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“.”开头的文件都是隐藏文件。Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。Ken Thompson 的 sh 是第一种 Unix Shell，Windows Explorer 是一个典型的图形界面 Shell。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linux 的 Shell 种类众多，常见的有：</p>
<ul>
<li><p>Bourne Shell（/usr/bin/sh或/bin/sh）</p>
</li>
<li><p>Bourne Again Shell（/bin/bash）</p>
</li>
<li><p>C Shell（/usr/bin/csh）</p>
</li>
<li><p>K Shell（/usr/bin/ksh）</p>
</li>
<li><p>Shell for Root（/sbin/sh）</p>
<a id="more"></a>
</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bash（ Bourne Again Shell） 是一个可执行从标准输入或者文件中读取的命令的一种兼容sh命令语言解释器。Bash集成了来自K Shell和C Shell的实用功能。Bash是与Shell和IEEE POSIX规范一致的一种实现。Bash是POSIX的默认配置。Bash 也是大多数Linux 系统默认的 Shell。</p>
<p>通过以下命令查看文档</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">man bash</span><br></pre></td></tr></table></figure>
<h2 id="1-bashrc"><a href="#1-bashrc" class="headerlink" title="1. ~/.bashrc"></a>1. ~/.bashrc</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The individual per-interactive-shell startup file   # 个人交互式shell的启动文件。</span><br></pre></td></tr></table></figure>
<p>这个文件类似于windows系统的环境变量的设置，如命令别名、路径等。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PATH=&quot;/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin&quot;</span><br><span class="line">LANG=zh_CN.GBK</span><br><span class="line">export PATH LANG</span><br><span class="line">alias rm=&apos;rm -i&apos;</span><br><span class="line">alias ls=&apos;/bin/ls -F --color=tty --show-control-chars&apos;</span><br></pre></td></tr></table></figure>
<p>​    例子中定义了路径，语言，命令别名（使用rm删除命令时总是加上-i参数需要用户确认，使用ls命令列出文件列表时加上颜色显示）。每次修改.bashrc后，使用source ~/.bashrc（或者 . ~/.bashrc）就可以立刻加载修改后的设置，使之生效。一般会在.bash_profile文件中显式调用.bashrc。登陆linux启动bash时首先会去读取~/.bash_profile文件，这样~/.bashrc也就得到执行了，你的个性化设置也就生效了。</p>
<h2 id="2-profile"><a href="#2-profile" class="headerlink" title="2.~/.profile"></a>2.~/.profile</h2><p>同样是配置文件。如果~/.bash_profile 或者 ~/.bash_login存在，这个文件不会被bash读取。</p>
<h2 id="3-bash-logout"><a href="#3-bash-logout" class="headerlink" title="3.~/bash_logout"></a>3.~/bash_logout</h2><p>当login Shell 存在的时候会被执行。当离开控制台的时候，清除屏幕以增加隐私性。</p>
<h2 id="4-bash-history"><a href="#4-bash-history" class="headerlink" title="4.~/bash_history"></a>4.~/bash_history</h2><p>记录命令的历史记录。</p>
<p>在根目录下也有一个bashrc，/etc/bashrc:为每一个运行bash shell的用户执行此文件。当bash shell被打开时,该文件被读取。~/.bashrc:该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该<br>该文件被读取。而在系统加载的过程中，通过/etc/init调用/etc/inittab，调用/etc/rc.d/rc.sysinit和/etc/rc.d/init.d/functions，然后启动和检查系统功能和服务。在启动bash时，分别再调用/etc/profile和/etc/bashrc，而对于每个用户，在打开shell时使用HOME/.bashrc和​HOME/.bash_profile设定专用于自己使用的shell信息。普通用户只能更改用户目录下的配置文件，跟目录下的配置文件只有可读的权限。</p>
<p>登陆时执行文件的顺序是：</p>
<ol>
<li>/etc/profile</li>
<li>user home directory /.bash_profile</li>
<li>user home directory /.bash_login</li>
<li>user home directory /.profile</li>
</ol>
<p>参考：</p>
<p><a href="http://blog.csdn.net/yeqishi/article/details/5652870" target="_blank" rel="noopener">http://blog.csdn.net/yeqishi/article/details/5652870</a></p>
<p><a href="http://blog.csdn.net/decisiveness/article/details/51967721" target="_blank" rel="noopener">http://blog.csdn.net/decisiveness/article/details/51967721</a></p>
<p><a href="http://www.runoob.com/linux/linux-shell.html" target="_blank" rel="noopener">http://www.runoob.com/linux/linux-shell.html</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'linux-2：用户目录下的配置文件',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {

      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>







]]></content>
      
        <categories>
            
            <category> liunx </category>
            
        </categories>
        
        
        <tags>
            
            <tag> linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[linux-1:源码安装]]></title>
      <url>/2018/01/18/linux/</url>
      <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在linux系统中，源码的安装一般有3个步骤组成：配置（configure）、源码（make）、安装（make install），具体的安装方法一般作者都会给出文档，这里主要讨论配置（configure）。Configure是一个可执行脚本，它有很多选项，使用命令./configure –help输出详细的选项列表，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">-bash-3.00# ./configure --help</span><br><span class="line">Usage: configure [options][ host]</span><br><span class="line">Options: [defaults in brackets after descriptions]</span><br><span class="line">Configuration:</span><br><span class="line">--cache-file=FILE     cache test results in FILE</span><br><span class="line">--help             print this message</span><br><span class="line">--no-create         do not create output files</span><br><span class="line">--quiet, --silent     do not print `checking...&apos; messages</span><br><span class="line">--version           print the version of autoconf that created configure</span><br><span class="line">Directory and file names:</span><br><span class="line">--prefix=PREFIX       install architecture-independent files in PREFIX [/usr/local]</span><br><span class="line">--exec-prefix=EPREFIX   install architecture-dependent files in EPREFIX [same as prefix]</span><br><span class="line">--bindir=DIR         user executables in DIR [EPREFIX/bin]</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中–prefix选项就是配置安装的路径，如果不配置该选项，安装后可执行文件默认放在/usr /local/bin，库文件默认放在/usr/local/lib，配置文件默认放在/usr/local/etc，其它的资源文件放在/usr /local/share，比较分散。为了便于集中管理某个软件的各种文件，可以配置–prefix，如：./configure –prefix=/usr/local</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以把所有资源文件放在/usr/local的路径中，就不会分散了。用了—prefix选项的另一个好处是卸载软件或移植软件。当某个安装的软件不再需要时，只须简单地删除该安装目录，就可以把软件卸载得干干净净；移植软件只需拷贝整个目录到另外一个机器即可（相同的操作系统）。当然要卸载程序，也可以在原来的make目录下用一次make uninstall，但前提是make文件指定过uninstall。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尤其是在服务器上，普通用户没有sudo权限，执行./configure会提示：./configure: Permission denied。可以使用–prefix将按照的配置路径放在自己的目录下，因为像/usr/local/lib此类的目录普通用户是没有写的权限的，会造成安装失败。失败的还要一种可能的原因是当前源码中的configure文件没有可执行权限，也会导致“Permission denied”。可以使用“ll”命令查看文件的权限，然后使用chmod命令对该文件加上可执行的权限“chmod +x ./configure”。</p>
<p>参考：</p>
<p><a href="http://www.maybe520.net/blog/1945/" target="_blank" rel="noopener">http://www.maybe520.net/blog/1945/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: 'linux-1:源码安装',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>]]></content>
      
        <categories>
            
            <category> liunx </category>
            
        </categories>
        
        
        <tags>
            
            <tag> linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[序列标注-2：命名实体识别]]></title>
      <url>/2018/01/17/ner2/</url>
      <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://sighan.cs.uchicago.edu/" target="_blank" rel="noopener">SIGHAN</a>是国际计算语言学会（ACL）中文语言处理小组的简称，其英文全称为“Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics”.Bakeoff则是SIGHAN所主办的国际中文语言处理竞赛，第一届于2003年在日本札幌举行（Bakeoff 2003),第二届于2005年在韩国济州岛举行(Bakeoff 2005), 而2006年在悉尼举行的第三届（Bakeoff 2006）则在前两届的基础上加入了中文命名实体识别评测。目前SIGHAN Bakeoff已成功举办了6届，其中Bakeoff 2005的数据和结果在其主页上是完全免费和公开的，但是请注意使用的前提是非商业使用（non-commercial）:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;The data and results for the 2nd International Chinese Word Segmentation Bakeoff are now available for non-commercial use.</p>
<p>The Third SIGHAN Chinese Language Processing Bakeoff will feature two tasks:</p>
<ul>
<li>Word Segmentation</li>
<li>Named Entity Recognition</li>
</ul>
<a id="more"></a>
<h3 id="Word-Segmentation-Task"><a href="#Word-Segmentation-Task" class="headerlink" title="Word Segmentation Task"></a>Word Segmentation Task</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;The Word Segmentation task requires identification of word boundaries in running Chinese text.</p>
<p>The following resources will be available:</p>
<p>Matched training and (new) test sets from:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Source Institution</th>
<th style="text-align:center">Character Encoding</th>
<th style="text-align:center">Approximate Size (chars)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CKIP, Academia Sinica, Taiwan</td>
<td style="text-align:center">Traditional, Big5</td>
<td style="text-align:center">8.3M</td>
</tr>
<tr>
<td style="text-align:center">City University of Hong Kong</td>
<td style="text-align:center">Traditional, Big5</td>
<td style="text-align:center">2.4M</td>
</tr>
<tr>
<td style="text-align:center">Microsoft Research</td>
<td style="text-align:center">Simplified, CP936</td>
<td style="text-align:center">5M</td>
</tr>
<tr>
<td style="text-align:center">University of PennsylvaniaUniversity of Colorado, Boulder</td>
<td style="text-align:center">Simplified</td>
<td style="text-align:center">1M</td>
</tr>
</tbody>
</table>
<p>&nbsp;&nbsp;&nbsp;&nbsp;Segmentation guidelines for the following corpora are available. These were supplied to SIGHAN by each data provider, and converted into PDF by the organizer:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Corpus</th>
<th style="text-align:center">MS Word</th>
<th style="text-align:center">PDF</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Academia Sinica</td>
<td style="text-align:center"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/as_spec.doc" target="_blank" rel="noopener">516 KB</a></td>
<td style="text-align:center"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/as_spec.pdf" target="_blank" rel="noopener">336 KB</a></td>
</tr>
<tr>
<td style="text-align:center">City University of Hong Kong</td>
<td style="text-align:center"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/cityu_spec.doc" target="_blank" rel="noopener">154 KB</a></td>
<td style="text-align:center"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/cityu_spec.pdf" target="_blank" rel="noopener">237 KB</a></td>
</tr>
<tr>
<td style="text-align:center">Microsoft Research</td>
<td style="text-align:center"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/msr_spec.doc" target="_blank" rel="noopener">41 KB</a></td>
<td style="text-align:center"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/msr_spec.pdf" target="_blank" rel="noopener">70 KB</a></td>
</tr>
</tbody>
</table>
<h3 id="Named-Entity-Recognition-Task"><a href="#Named-Entity-Recognition-Task" class="headerlink" title="Named Entity Recognition Task"></a>Named Entity Recognition Task</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;The Named Entity Recognition Task requires participants to identify named entities (person, location, and organization) in running unsegmented Chinese text.</p>
<p>The following resources will be available:</p>
<p>Matched training and (new) test sets from:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Source Institution</th>
<th style="text-align:center">Character Encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">City University of Hong Kong</td>
<td style="text-align:center">Traditional, Big5</td>
</tr>
<tr>
<td style="text-align:center">Microsoft Research</td>
<td style="text-align:center">Simplified, CP936</td>
</tr>
<tr>
<td style="text-align:center">Linguistic Data Consortium</td>
<td style="text-align:center">Simplified, CP936</td>
</tr>
</tbody>
</table>
<p>&nbsp;&nbsp;&nbsp;&nbsp;You may declare that you will return results on any subset of these corpora. For example, you may decide that you will test on the Sinica Corpus and the City University corpus. The only constraint is that you <strong>must not</strong> select a corpus where you have knowingly had previous access to the testing portion of the corpus. A corollary of this is that a team may not test on the data from their own institution.</p>
<h2 id="Data-Formats"><a href="#Data-Formats" class="headerlink" title="Data Formats"></a>Data Formats</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Training data will be available for CityU and MSRA in two formats. The primary format will be similar to that of the Co-NLL NER task 2002, adapted for Chinese. The data will be presented in two-column format, where the first column consists of the character and the second is a tag. The tag is specified as follows:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Tag</th>
<th style="text-align:center">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0 (zero)</td>
<td style="text-align:center">Not part of a named entity</td>
</tr>
<tr>
<td style="text-align:center">B-PER</td>
<td style="text-align:center">Beginning character of a person name</td>
</tr>
<tr>
<td style="text-align:center">I-PER</td>
<td style="text-align:center">Non-beginning character of a person name</td>
</tr>
<tr>
<td style="text-align:center">B-ORG</td>
<td style="text-align:center">Beginning character of an organization name</td>
</tr>
<tr>
<td style="text-align:center">I-ORG</td>
<td style="text-align:center">Non-beginning character of an organization name</td>
</tr>
<tr>
<td style="text-align:center">B-LOC</td>
<td style="text-align:center">Beginning character of a location name</td>
</tr>
<tr>
<td style="text-align:center">I-LOC</td>
<td style="text-align:center">Non-beginning character of a location name</td>
</tr>
<tr>
<td style="text-align:center">B-GPE</td>
<td style="text-align:center">Beginning character of a geopolitical entity</td>
</tr>
<tr>
<td style="text-align:center">I-GPE</td>
<td style="text-align:center">Non-beginning character of a geopolitical entity</td>
</tr>
</tbody>
</table>
<h2 id="Test-data"><a href="#Test-data" class="headerlink" title="Test data"></a>Test data</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Test data will be provided one-sentence per line, unsegmented with no tags. Participants should format their results to conform to the training data format described above. Scoring will be done automatically using a variant of the Co-NLL 2003 scoring  script. Comments at the beginning of the file describe usage.</p>
<h1 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h1><p>论文题目：Chinese Named Entity Recognition with Conditional Random Fields</p>
<p>​    这篇论文使用了基本特征和辅助特征，而且加入了一个后处理的对不正确的结果进行纠正。主要是在特征的设计上面。</p>
<p><strong>1.基本特征</strong>：$C_n(n=-2,-1,0,1,2)$  $C_nC_n+1(n=-1,0)$</p>
<p>​    特征的使用是特征模板定义的，特征模板可以在一个上下文环境（窗口）中安装Unigram、Bigram、Trigram等使用特征。例如，在句子“中国和日本是邻邦“中，若当前字是”和“，Bigram模板C_2C-1/C-1C0、C0C1、C1C2将分别拓展出”中国“、”国和“、”和日“、”日本“四个特征。0.-1.-2,1,2表示相对当前字符及前后一个或者两个字符的位置。CRF模型就是根据这些特征及类别标签生成判别函数，最后在测试时为每一个token按概率最大原则打上相应的标签。</p>
<p><strong>2.辅助特征：</strong></p>
<p>1）词边界特征：</p>
<p>对人名而言，“先生“是一个重要特征。</p>
<ol>
<li><p>提取任意n元（$2&lt;=n&lt;=10\: Frequency&gt;=10$）得到列表$W_1$</p>
</li>
<li><p>使用SSR(Statistical Substring Reduction)算法得到列表$W_2$</p>
</li>
<li><p>构建一个字符列表$CH$ (在训练语料中出现频率前20的),为了收集像“的”，“了”的字符。</p>
</li>
<li><p>从列表中$W_2$移除在$CH$出现过的字符，得到$W_3$</p>
<p>用列表$W_3$作为一个字典从左向右最大匹配分词</p>
</li>
</ol>
<p>2）字符特征 </p>
<p>PSur: uni-gram characters, first characters of Person Name. (surname)</p>
<p>PC: uni-gram characters in Person Name.</p>
<p>PPre: bi-gram characters before Person Name. (prefix of Person Name)</p>
<p>PSuf: bi-gram characters after Person Name. (suffix of Person Name)</p>
<p>LC: uni-gram characters in Location Name or Geopolitical entity.</p>
<p>LSuf: uni-gram characters, the last characters of Location Name or Geopolitical Entity. (suffix of Location Name or Geopolitical Entity)</p>
<p>OC: uni-gram characters in Organization Name.</p>
<p>OSuf: uni-gram characters, the last characters of Organization Name. (suffix of Organization Name)</p>
<p>OBSuf: bi-gram characters, the last two characters of Organization Name. (suffix of Organization Name)</p>
<p><strong>3.后处理</strong></p>
<p>The post-processing tries to assign the correct tags according to n-best results for every sentence.</p>
<p>参考：</p>
<ol>
<li><a href="http://sighan.cs.uchicago.edu/bakeoff2006/" target="_blank" rel="noopener">http://sighan.cs.uchicago.edu/bakeoff2006/</a></li>
<li>Chinese Named Entity Recognition with Conditional Random Fields. Wenliang Chen</li>
</ol>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '序列标注-2：命名实体识别',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>

]]></content>
      
        <categories>
            
            <category> ner </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ner </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[序列标注-1：命名实体识别]]></title>
      <url>/2018/01/16/ner/</url>
      <content type="html"><![CDATA[<h3 id="1-命名实体研究历史"><a href="#1-命名实体研究历史" class="headerlink" title="1.命名实体研究历史"></a>1.命名实体研究历史</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;国外对于英文命名实体识别的研究开始比较早。1991年Rau在第7届IEEE人工智能应用会议上发表了“抽取和识别公司名称”的有关研究文章, 首次描述了抽取和识别公司名称的系统, 该系统主要采用启发式算法和手工编写规则的方法，1996年, 命名实体评测作为信息抽取的一个子任务被引入MUC-6 ,在其后的MUC-7的MET-2 以及IEER-99、CoNLL-2002、CoNLL-2003、IREX、LREC等一系列国际会议中, 命名实体识别都被作为其中的一项指定任务。</p>
<a id="more"></a>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于中文内在的特殊性决定了在文本处理时首先必须进行词法分析, 中文命名实体识别的难度要比英文的难度大。中文命名实体识别起步较晚, 20世纪90年代初期开始, 国内一些学者对中文命名实体(如:地名、人名、组织机构名等)识别进行了一些研究。如:孙茂松等在国内比较早开始进行中文人名识别, 他们主要采用统计的方法计算姓氏和人名用字概率。张小衡等对中文机构名称进行识别与分析, 主要采用人工规则对高校名进行了实验研究。Intel中国研究中心的Zhang等在ACL2000上演示了他们开发的一个抽取中文命名实体以及这些实体间相互关系的信息抽取系统, 该系统利用基于记忆的学习(MemoryBased Learning, MBL)算法获取规则, 用以抽取命名实体及它们之间的关系。</p>
<h3 id="2-命名实体识别研究内容"><a href="#2-命名实体识别研究内容" class="headerlink" title="2.命名实体识别研究内容"></a>2.命名实体识别研究内容</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;早期的命名实体识别工作,主要识别一般的<strong>专有名词</strong>，包括三类名词：人名、地名、机构名。这也是MUC-6 最早定义的任务要识别的名词。随着研究的进行，人们对这些名词进行更细致的划分。对于地名，可以进行细分为：国家名、省/州、城市名、街道名等。类似的人名可以细分为：政客、演员等。除了识别一般的专有名词， 人们也开始关注对于特定领域的命名实体识别。在生物医学领域，对于基因名、蛋白质名的识别已经有许多工作在开展，也取得了不错的效果。针对社交媒体文本中存在大量的电影、歌曲等， 识别电影名、歌曲名、邮件地址等实体。随着研究范围的扩大，针对不同的特定问题特定领域， 越来越多的实体类型被提出。</p>
<h3 id="3-技术方法"><a href="#3-技术方法" class="headerlink" title="3.技术方法"></a>3.技术方法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;早期的研究大多采用基于人工构造规则的方法，而现在大多使用监督的机器学习方法。监督学习方法的思想是在大量标注的文档上学习命名实体正例和负例的特征并设计捕获给定类型本质的规则。而语料库的缺乏和构造这些资源的高昂成本导致了两种可替代的学习方法，<strong>半监督学习</strong>和<strong>无监督学习</strong>。</p>
<h4 id="3-1-基于规则和词典的方法"><a href="#3-1-基于规则和词典的方法" class="headerlink" title="3.1.　基于规则和词典的方法"></a>3.1.　基于规则和词典的方法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于规则的方法多采用语言学专家手工构造规则模板, 选用特征包括统计信息、标点符号、关键字、指示<br>词和方向词、位置词(如尾字)、中心词等方法, 以模式和字符串相匹配为主要手段, 这类系统大多依赖于知<br>识库和词典的建立。一般而言, 当提取的规则能比较精确地反映语言现象时, 基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格,编制过程耗时且难以涵盖所有的语言现象, 特别容易<br>产生错误, 系统可移植性不好, 对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大, 存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p>
<h4 id="3-2-监督学习"><a href="#3-2-监督学习" class="headerlink" title="3.2  监督学习"></a>3.2  监督学习</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前，命名实体识别所使用的主流技术是监督学习。监督学习包括隐马尔科夫模型、决策树、最大熵模型、支持向量机、条件随机场等，这些方法都是命名实体识别系统的变体，这些系统都是读取大量的标注语料，存储一系列实体，并且构造基于特征的判别规则。通常提出的基本监督方法包括标注测试语料库的词，这些词在训<br>练集中被注释为实体。系统的性能依赖于同时出现在训练语料库和测试语料库中的词所占的比例，通常称之为词汇转移。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最大熵模型结构紧凑, 具有较好的通用性, 主要缺点是训练时间复杂性非常高,有时甚至导致训练代价难以承受, 另外由于需要明确的归一化计算, 导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架, 但同时存在收敛速度慢、训练时间长的问题。一般说来, 最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些, 但是隐马尔可夫模型在训练和识别时的速度要快一些, 主要是由于在利用Viterbi算法求解命名实体类别序列的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用, 如短文本命名实体识别 。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于统计的方法对特征选取的要求较高, 需要从文本中选择对该项任务有影响的各种特征, 并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性, 考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析, 从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。张祝玉等针对条件随机场的特征选取与组合进行了比较研究, 通过实验比较得出在训练时应优先选择贡献度大的特征, 同时还表明使用组合特征可以提升系统的性能。</p>
<h4 id="3-3-半监督学习"><a href="#3-3-半监督学习" class="headerlink" title="3.3.半监督学习"></a>3.3.半监督学习</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于可用标注语料库的匮乏以及大量未标注语料库的存在，研究人员提出了一种半监督学习方法，也称为弱监督学习。主要的半监督学习方法被称为“bootstrapping”，它只需要提供少量的标注数据，例如一组种子用于开始的学习。然后，系统搜索包含这些已提供数据的句子并尝试发现出现在相似上下文中实体的其他实例。接着将学习过程应用于新发现的例子以发现新的相关上下文。通过重复这一过程收集大量命名实体和大量上下文信息。半监督方法只需要较少的已标注数据，从而在大量无标注数据的条件下获得可以与监督学习方法相媲美的性能。</p>
<h3 id="4-评估指标"><a href="#4-评估指标" class="headerlink" title="4.评估指标"></a>4.评估指标</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对命名实体识别系统的发展来说，对系统的全面评估是必不可少的，许多系统被要求根据它们标注文本的能力来对系统进行排序。目前，通常采用的评估指标主要有正确率、召回率和F 值，它们的定义如下：<br>​    $正确率P= 识别出的正确实体数/ 识别出的实体数$<br>​    $召回率R= 识别出的正确实体数/ 样本中的实体数。$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两者的取值都在0 和1 之间，数值越接近1，正确率或召回率就越高。正确率和召回率有时会出现矛盾的情况，这时需要综合考虑它们的加权调和平均值，也就是F 值，其中最常用的F1 值，当F1 值较高时说明试验方法比较有效。</p>
<p>F1 值定义如下：<br>​    $   F1 =（2 \times P \times R）/（P+ R） $</p>
<p>参考：</p>
<ol>
<li>命名实体识别研究发展综述_周玉新</li>
<li>命名实体识别研究进展综述_孙镇</li>
<li>命名实体识别综述_陈基</li>
</ol>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '序列标注-1：命名实体识别',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>






































]]></content>
      
        <categories>
            
            <category> ner </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ner </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[唐诗-1：复古的伟大继承人：李白]]></title>
      <url>/2018/01/16/poetry/</url>
      <content type="html"><![CDATA[<h1 id="复古的伟大继承人：李白"><a href="#复古的伟大继承人：李白" class="headerlink" title="复古的伟大继承人：李白"></a><p align="center">复古的伟大继承人：李白</p></h1><h2 id="古风"><a href="#古风" class="headerlink" title="古风"></a><p align="center">古风</p></h2><p align="center">大雅久不作，吾衰竟谁陈？<br>王风委蔓草，战国多荆榛。<br>龙虎相啖食，兵戈逮狂秦。<br>正声何微茫，哀怨起骚人。<br>扬马激颓波，开流荡无垠。<br>废兴虽万变，宪章亦已沦。<br>自从建安来，绮丽不足珍。<br>圣代复元古，垂衣贵清真。<br>群才属休明，乘运共跃鳞。<br>文质相炳焕，众星罗秋旻。<br>我志在删述，垂辉映千春。<br>希圣如有立，绝笔于获麟。<br></p>

<a id="more"></a>
<p>注释</p>
<ol>
<li>大雅：《诗经》之一部分。此代指《诗经》。作：兴。吾衰：《论语·述而》：“子曰：甚矣，吾衰也。”陈：《礼记·王制》：“命太史陈诗以观民风。”</li>
<li>王风：《诗经·王风》，此亦代指《诗经》。委蔓草：埋没无闻。此与上句“久不作”意同。多荆榛：形容形势混乱。</li>
<li>龙虎：指战国群雄。啖食：吞食，此指吞并。兵戈：战争。逮：直到。</li>
<li>正声：雅正的诗风。骚人：指屈原。</li>
<li>扬马：指汉代文学家扬雄、司马相如。</li>
<li>宪章：本指典章制度，此指诗歌创作的法度、规范。沦：消亡。</li>
<li>建安：东汉末献帝的年号（196～219），当时文坛作家有三曹、七子等。绮丽：词采华美。</li>
<li>圣代：此指唐代。元古：上古，远古。垂衣：《易·系辞下》 ：“黄帝、尧、舜垂衣裳而天下治。” 意谓无为而治。清真：朴素自然，与绮丽相对。</li>
<li>“群才”句：文人们正逢休明盛世。属：适逢。跃鳞：比喻施展才能。</li>
<li>“文质”句：意谓词采与内容相得益彰。秋旻：秋天的天空。</li>
<li>删述：《尚书序》：“先君孔子……删《诗》为三百篇，约史记而修《春秋》，赞《易》道以黜《八索》，述职方以除《九丘》。”</li>
<li>希圣：希望达到圣人的境界。获麟：《春秋·哀公十四年》：“西狩获麟，孔子曰‘吾道穷矣’。”传说孔子修订《春秋》，至此搁笔不复述作。因为他认为骐麟出非其时而被猎获，不是好兆。以上四句意谓：李白欲追步孔子，有所述作，以期后垂名不朽。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“大雅”指的是什麽？就是《诗经》。尤其是所谓的“雅”这个字 那更是一种所谓的雅正风范。作为文学正统，它和“国风” 是不大一样的。“国风”从某个层次来讲，它还是比较偏向于一般民间，它最初是来自民间的风谣。当然，被采集到政府机构以后，其实经过了贵族阶层的整理，也受到精英知识份子的润色、雅化其实已经不能算是民间的作品，也不是各地民歌原来的面貌，它事实上是周代贵族阶层的产物了因此才会并称为“风雅”。国风的诗篇尚且如此，至于“雅” 字，那绝对是庙堂之音。 李白又说，文学正统还是要由大雅来建立。它生生不息、 绵延万代，这个基本上就是古代文人最确切不疑、天经地义的一种信念 。这么一来，连李白这样子高歌“我本楚狂人，凤歌笑孔丘”的一位诗人，很奔放、 很潇洒，甚至鼓励了很多年轻人去违抗现实束缚的一个不羁的灵魂 他实际上还是活在中国传统脉络底下的。可想而知 儒家的价值观可以说是古代文人血液里的DNA 这个价值观对他们而言是不可能打折扣的，所以李白说： 很可惜啊，在大雅之后，这个完美的、正统的、 真实的文学生命，已经长久被忽略了，而沉坠不振了。所以李白下面接著说「吾衰竟谁陈」 他说：可是我已经衰老了、衰退了、衰弱了，这样一来又有谁能够去把这样一个衰退的正统给 重新继承下来，再度加以振发？加以复活呢？ 而“吾衰竟谁陈”这句话应该会让读者立刻想到 其实李白就是以孔子为自期，“吾衰竟谁陈”根本就是套用孔子的话。 孔子说：“甚矣，吾衰也！久矣，吾不复梦见周公。” 人家梦周公可不是在说睡觉的意思，而是他想要去继承、 去发扬，去把那样一个由周公制礼作乐所形成的文化大传统，给内化成为自己的灵魂的一部分。 因此就会像庄子所说的：“其寐也魂交” 一旦全心全意的追求，深入到潜意识的内里，我们所渴望追求的对象就会在梦中 出现，所以如果连做梦都没有梦见周公，那就表示离周公伟大的礼乐精神已经有了距离。那这就是孔子的感慨，感慨说，那是不是我衰老了，是不是我的精神血性，不足以把这样的大生命、 大传统给继承下来？所以李白一开始的“大雅久不作，吾衰竟谁陈”。就表明了他是以孔子自许的，他要像孔子一样把那个由周公所建构的而成的文化传统的核心 那最源源不断的生机的来源化为己任。就像孔子承担了这个责任 一样，现在李白也说我要承担下来，甚至李白认为除了他自己以外 没有别人可以承担下来。「吾衰竟谁陈」就是这个意思，颇为当仁不让、捨我其谁 就李白这个人物而言，我们读他很多的作品以后就会发现，他事实上还是一个非常正统的传统的知识份子，这个知识份子还是以儒家的价值观，作为他生命真切不疑的最高指导原则 这是没有谁能够例外的，杜甫也不例外，苏东坡也不例外，陶渊明也不例外，曹雪芹也不例外 所以一开始李白就觉得当仁不让、舍我其谁，他要以 孔子自许，在诗歌发展起起伏伏的这一条曲折的河流中 李白自我定位在孔子的地位上，他要做文学史上的孔子。 那位不但继承周公的礼乐文化，同时也为后来的人点燃灵根般，成为中国文化传统的一位 至圣先师。 宋代人就说：“天不生仲尼，则万古如长夜”， 他们认为假如没有孔子的话，中国文化就会陷入到无限的黑暗里，就这点来说 孔子简直不但是这个传统文化的核心主轴，甚至还是唯一的一颗恒星 他带来一切这个文化的光亮，所以就这点来说李白会以孔子 自许，你就该知道那是多麽大的自信，<br>以及多麽大的自我期望！</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来各位请看，就在这一段 李白用了和陈子昂同样的操作方式，所谓“大雅久不作” 是不是就等于“王风委蔓草”，是不是就等于“正声何微茫”，也就是等于 “宪章亦已沦”，那个消失不见的价值。他用了四个文句来说明 而那四个文句所指涉的不同语词，其实都是同一个意义 你看，“大雅”就等于“王风”，就等于“正声”，等于“宪章”，都是一种永恒 不变的价值，而“久不作”就是“委蔓草”，就是“何微茫”，就是“亦已沦”， 也就是它们都被否定了，都消沉不见了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这首诗的下面又更清楚了，李白说：“自从建安来，绮丽不足珍”，这十个字 可以说是只要提到李白的复古理论，一定都会涉及到的经典名言对李白而言，固然《诗经》的正统已经沦丧，如他前面所反覆强调的，可是 《诗经》之后，确实还有一个在“委蔓草”、“何微茫”、“亦已沦”的衰颓状态中独一无二的中流砥柱，这个中流砥柱就是“建安”。所以李白说建安在《诗经》这个宪章沦落了四百年之久以后啊 它终于恢复了正统。可是很可惜，建安之后又走上了另外一条岔路，那条岔路是 以绮丽作为追求的目标，而这个绮丽却又是被李白这一类的复古诗人认为最不重要的文学追求。所以李白说“绮丽不足珍”，这麽一来建安和绮丽 就变成一个二元的对立组，而且这个二元的对立组和陈子昂所说的如出一辙，陈子昂不也说“僕尝暇时观齐、梁间诗，彩丽竞繁 而兴寄都绝”吗？可以说李白这首诗就是对陈子昂的直接回应 只是陈子昂的是书信的散文版，那麽李白是诗歌的韵文版，事实上 意思完全一样，<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;李白进一步说：“圣代复元古，垂衣贵清真”， 清、真这两个字，已经不是老庄“垂拱而治，清静无为” 政治上面的不扰民的意思，而是指诗歌的一种对纯粹情志的追求。不再以绮丽的雕虫小技来覆盖文学的原貌 。不要有太繁缛的那些装扮，那些彩丽的文字，脂粉气太浓了 覆盖在文字上面，遮蔽了真性情，真心灵，你都可以洗掉了，恢复它清雅的面貌吧！而这裡所说的清、真 确确实实在李白的许多其他地方也反覆地出现过，例如像《古风五十九首》里面第三十五首也提到 “一曲斐然子，雕虫丧天真”。 后来在一首晚年的长诗里面也提到：“清水出芙蓉，天然去雕饰” 其中“清”和“真”都代表一种由衷的、真实的、 天然的特质，基本上就是李白复古思想，以及他创作上最重要的核心指标由此可见，任何一个人要能成为一个大生命。他都绝对不会只顾及到自己的名利地位，他要想的是 把传统里面最好的东西，任何点点滴滴有价值的养分，他都希望 可以传承下去，而我们作为历史的一部分，实际上也都应该用这样的一种历史意识，透过古往今来的宏大视野来看待一切问题，不要只活在当下，目光如豆地 聚焦在一个该死的自我上，而是应该要恢复“人作为一个人”的最高尊严 那就是，你能够了解过去，也能够期许未来，<br>然后把自我扩充为整个时代的成员之一，薪火相传 这么一来，人才会拥有真正的灵魂。 </p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以你看，接下来李白说：盛唐是一个“众星罗秋旻，群才属休明”的时代，是一个人才济济，像无数闪耀的星星在天空中星罗棋布 大家都要来共同参与历史阶段裡这一个昂扬的、盛明的大好时代。 那么我，也就是李白，该做什麽事情呢？你看他又回了到个人身上，给自己一个宏大的责任、宏大的期许 到此为止，《古风》的这第一首诗歌，其中的诗歌主张已经非常清楚了 李白在其它地方也一再做出类似的表达，例如 《古风》的第三十首，他还说：“玄风变太古，道丧无时还。” 尤其是我们再来参考唐代 有一位孟棨，他在《本事诗》中又记载了李白有一段著名的名言 李白说：“梁陈以来，艳薄斯极”，“沉休文”（就是沉约）又尚以声律”，就是追求格律于是他说：“将复古道，非我而谁？”所谓的 “梁陈以来，艳薄斯极”，岂非就是陈子昂所说的“齐梁间诗，彩丽竞繁”吗？ 那所谓的“沉休文又尚以声律”，这句中的沉休文，也就是南朝齐梁诗坛上的沉约 他在文学史上面最重要的贡献，就是他是奠定格律的先锋。 如此一来，绮丽的这一脉发展就被贬低了，也被否定了。 李白的“将复古道，非我而谁”，也就等于是“大雅久不作，吾衰竟谁陈”。 李白要当仁不让地以诗歌中的孔子自居要去恢复、振拔文学的大生命，追求理想的永恒不朽。最后的“我志在删述，垂辉映千春。 希圣如有立，绝笔于获麟。” 这四句话其实是在回应前面的第一联，李白不就是用孔子来自许，他在诗歌结束的地方又回到 孔子身上，起结，也就是开头和结尾，都以孔子作为坐标 就可想而知，李白气魄是很大的，他不甘于只做一个小小的诗人而已 他觉得即使要做一个诗人，都应该要有孔子的胸襟和目标，给自己一个宏大的眼光。 所以他说：“我志在删述，垂辉映千春。” 你看，他的志向是什麽？他的志向是，要像孔子一样地删诗书，然后述而不作这里当然不是指学术思想上面的，不是指那个文化上面的事业。 他指的是，要在诗歌的历史里面，达到像孔子在整个中国文化里面所达到的成就，这样一个给自己最高的期许。 他说，这麽一来，他的生命所散发的光芒就可以辉映著千年的历史千春，就是千秋，就是千年，也就是永恒。 所以李白是不争一时的，他是要争千秋的。 因此他尽其在我的努力，感谢盛唐这个大时代带给他们的灵魂养分，让自己壮大，让自己努力地散发光芒 这个光芒如果足够，就可以照耀千古、辉映后代。 所以李白说：那个志气是很大的，他也不知道自己究竟能不能做得到 但是，如同司马迁所说的：「虽不能至，心嚮往之。」 要有这样的嚮往之情，人的灵魂才会维持在一个高度，不会逐渐地低落、下坠而不自知。 假如我没有这样一种仰望的志向在护持著、 维繫著一种精神高度，人就会越来越活在庸俗生活的浅水沙滩里然后就斤斤计较于小小的得失，志气就会越来越衰颓，终究就被消磨殆尽。参考一下白居易所说的：“蜗牛角上争何事？石火光中寄此身。” 确实，当我们眼界大的时候就会知道，我们所在的世界真的只是一个小小的蜗牛角。而原来我们就在一个好狭窄的地方拥挤著，你争我夺，费力追求一些恐怕临死回顾的时候，根本就不觉得重要的东西。但这就是人性。 一旦不加以警觉，人们不知不觉就会走上“在蜗牛角你争我夺”这样的一条路。 所以，我们才需要时时刻刻给自己一个大视野、大提醒，重点就在于这里。 所以李白说：“希圣如有立，绝笔于获麟。” 他说希圣，就是希望能达到圣人所达到的境界，也就是指孔子所展现的崇高境界。 如果这个期望能够达到的话，让他也能够在这个文明里面有所成就、 有所奠定，能够立足在诗歌的历史高峰上“垂辉映千春”，那麽李白就会像孔子 一样“绝笔于获麟”，也就是等到获麟事件发生之后才会停笔 意思就是说：不到死，他绝不会停下笔来。这就清楚宣告了李白终其一生的坚持和努力。 如此一来，各位想想，又岂能够说李白是一个反对儒家的叛逆者呢？</p>
<p>参考：</p>
<ol>
<li>欧丽娟，台湾大学</li>
<li><a href="http://so.gushiwen.org/shiwenv_5835f8a8117b.aspx" target="_blank" rel="noopener">http://so.gushiwen.org/shiwenv_5835f8a8117b.aspx</a></li>
</ol>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '唐诗-1：复古的伟大继承人：李白',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[序列标注]]></title>
      <url>/2018/01/16/sequence-laneling/</url>
      <content type="html"><![CDATA[<h2 id="1-什么是序列标注？"><a href="#1-什么是序列标注？" class="headerlink" title="1.什么是序列标注？"></a>1.什么是序列标注？</h2><p> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是我们在NLP任务中一直面临的一个元问题，我们希望在一个序列中为每个元素分配一个标签。对我们来说，一个序列通常是一个句子，一个词是一个元素。我们试图分配的元素通常是词性、句法块标签（是名词短语、动词短语等的一部分），命名实体标记（这是一个人吗？）凡此种种，不一而足。信息提取系统（比如从电子邮件中提取会议时间和地点）也可以被视为序列标记问题。</p>
<h3 id="1-1-任务："><a href="#1-1-任务：" class="headerlink" title="1.1.任务："></a>1.1.任务：</h3><p>$f:{X} {sequence} \to {Y}  sequence $</p>
<p>$X:X_1 \:  X_2\: X_3\:…X_L$</p>
<p>$Y:Y_1 \:  Y_2 \:  Y_3 \: …Y_L$</p>
<p>目标：$Given\:X,predcit\: Y$</p>
<a id="more"></a>
<h3 id="1-2-有两种序列标注："><a href="#1-2-有两种序列标注：" class="headerlink" title="1.2 .有两种序列标注："></a>1.2 .有两种序列标注：</h3><font face="Times New Roman"><br><br>1. raw labeling:Raw labeling is something like POS tagging where <strong>each element gets a single tag</strong><br><br>2.  joint segmentation and labeling: Joint segmentation and labeling is where whole segments get the same label.</font>

<p>   ​</p>
<p>   举个例子，在命名实体识别中，像</p>
<p>   “Yesterday , George Bush gave a speech. ”</p>
<p>   这里我们想给“George Bush”整个标注“PERSON”而不是单个的字。如果两个字被命名，我们需要在哪隔开。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决这个问题最容易的方法是把他们转换为raw labeling problems。标准的方法是通过“B-X”，“I-X”或者“O”对每个字进行“BIO”编码，其他的编码方式也是可以的。如下所示：</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Yesterday/O  ,/O    George/B-PER    Bush/I-PER    gave/O    a/O   speech/O   ./O</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注意要避免产生不可能的序列，如”I-X” 只可能出现在”B-X” 或者 “I-X”后面。</p>
<h2 id="2-标签编码表示"><a href="#2-标签编码表示" class="headerlink" title="2.标签编码表示"></a>2.标签编码表示</h2><h3 id="2-1-IO-编码"><a href="#2-1-IO-编码" class="headerlink" title="2.1. IO 编码"></a>2.1. IO 编码</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的编码是IO编码，标签要么是实体X的（I_X）要么是无实体（O）。这种编码是有缺陷的，因为它不能表示彼此相邻的两个实体，因为没有边界标记。</p>
<h3 id="2-2-BIO-Encoding"><a href="#2-2-BIO-Encoding" class="headerlink" title="2.2 .BIO Encoding"></a>2.2 .BIO Encoding</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;行业标注编码是“BIO”编码，它划分实体标签要么是实体的开始部分“B_X”和实体的延续“I_X”</p>
<h3 id="2-3-BMEWO-Encoding"><a href="#2-3-BMEWO-Encoding" class="headerlink" title="2.3.BMEWO Encoding"></a>2.3.BMEWO Encoding</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BMEWO编码进一步区分实体，增加了结束标签E_X，M和I是一样的，尤其增加了一个完整的单实体标签。BMEWO编码最早介绍在<a href="http://citeseer.ist.psu.edu/old/699859.html" target="_blank" rel="noopener">Andrew Borthwick的 NYU thesis</a> 和1998年相关的论文最大熵命名实体识别中。</p>
<h2 id="3-序列标注"><a href="#3-序列标注" class="headerlink" title="3.序列标注"></a>3.序列标注</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;类似的，分词、词性标注、语块识别、<a href="http://book.paddlepaddle.org/07.label_semantic_roles/index.cn.html" target="_blank" rel="noopener">语义角色标注</a>等任务都可通过序列标注来解决。序列标注可以分为Sequence Classification、Segment Classification和Temporal Classification三类<a href="http://models.paddlepaddle.org/2017/04/21/sequence-tagging-for-ner-README.html#参考文献" target="_blank" rel="noopener">Supervised Sequence Labelling with Recurrent Neural Networks</a>]，这里只考虑Segment Classification，即对输入序列中的每个元素在输出序列中给出对应的标签。对于NER任务，由于需要标识边界，一般采用<a href="http://book.paddlepaddle.org/07.label_semantic_roles/" target="_blank" rel="noopener">BIO标注方法</a>定义的标签集。在raw labeling中，我们面临的问题是给序列中的每个字打上一个标签。也许最容易的方法是独立的预测每个标签。然后可以看做一个多分类的任务（每个标签是不同的分类）。我们可以使用各种分类器来解决这个问题。尽管它很简单，但这种方法对许多问题是非常<a href="http://arxiv.org/abs/cs/0111003" target="_blank" rel="noopener">有效</a>。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直觉上，我们通常认为这些问题中的标签不是独立的。以词性标注为例，不可能有一个动词跟着限定词。因此，我们想使用局部序列信息来提高我们的性能。传统的方法是用<a href="http://nlp.stanford.edu/fsnlp/" target="_blank" rel="noopener">隐马尔科夫</a>。在这里我们有两个概率分布：一个是转移概率（限定词后面跟着动词的可能性）；二是发射概率（“the”是限定词的可能性）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用隐马尔可夫的潜在问题是我们知道发射概率$ P(word|tag)$,但是我们想知道模型$P(tag|word)$ 后者更可取，因为他包含了许多重叠特征（大写、单词标识、前缀，后缀，主干等）。局部的解决是用最大熵马尔科夫模型<a href="http://www.cs.iastate.edu/~honavar/memm-icml2000.pdf" target="_blank" rel="noopener">MEMM</a> ,即使用最大熵模型对模型$P(tag|word)$ 建模，而其他的地方跟隐马尔可夫模型保持一样。最大熵马尔科夫模型比隐马尔可夫模型的训练复杂一点，但是效果好得多。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最大熵马尔科夫潜在的问题是当模型训练的时候，它们训练出来与先前的正确标签的有冲突。这是因为我们创建一个与时间$t+1$ 中的标签相对于的分类示例时，我们包含的特征依赖于时间$t$的标签，它们在训练的时刻总是对的，但是在测试的时刻是错误的。这导致了著名的“标签偏移”问题。CRF是解决这个问题的答案。CRF经常表现比MEMM更好。<a href="http://mallet.cs.umass.edu" target="_blank" rel="noopener">CRF implement</a></p>
<p>参考资料：</p>
<p><a href="https://nlpers.blogspot.com.au/2006/11/getting-started-in-sequence-labeling.html" target="_blank" rel="noopener">https://nlpers.blogspot.com.au/2006/11/getting-started-in-sequence-labeling.html</a></p>
<p><a href="https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/" target="_blank" rel="noopener">https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '序列标注',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>


]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[训练集 验证集 测试集]]></title>
      <url>/2018/01/13/%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86/</url>
      <content type="html"><![CDATA[<p>在机器学习中划分数据的过程中总是有几个疑问？</p>
<p><strong>(1).Why only three partitions? (training, validation, test)?</strong></p>
<p><strong>(2).What is the difference between test set and validation set?</strong></p>
<a id="more"></a>
<p><font face="Times New Roman">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In  <strong>machine</strong> learning, the study and construction of algorithms that can learn from and make predictions on data is a common task. Such algorithms work by making data-driven predictions or decisions,through building a mathematical model from input data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The data used to build the final model usually comes from multiple datasets. In particular, three data sets are commonly used in <strong>different stages</strong> of the creation of the model.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The model is initially <strong>fit</strong> on a training dataset that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks of the model. The model (e.g. a neural net or a naive Bayes classifier is trained on the training dataset using a supervised learning method (e.g. gradient descent or stochastic gradient descent). In practice, the training dataset often consist of pairs of an input vector and the corresponding <em>answer</em> vector or scalar, which is commonly denoted as the <em>target</em>. The current model is run with the training dataset and produces a result, which is then compared with the <em>target</em>, for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both Feature_selection and parameter estimation.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Successively, the fitted model is used to predict the responses for the observations in a second dataset called the <strong>validation dataset</strong>.The validation dataset provides an  <strong>unbiased evaluation</strong>  of a model fit on the training dataset while tuning the model’s  <strong>hyperparameters</strong>  (e.g. the number of hidden units in a neural network) by early stopping: stop training when the error on the validation dataset increases, as this is a sign of overfitting to the training dataset. This simple procedure is complicated in practice by the fact that the validation dataset’s error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finally, the <strong>test dataset</strong> is a dataset used to provide an unbiased evaluation of a <em>final</em> model fit on the training dataset.</font> </p>
<h2 id="1-Training-dataset"><a href="#1-Training-dataset" class="headerlink" title="1.Training dataset"></a>1.Training dataset</h2><p><font face="Times New Roman">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A training dataset is a dataset of examples used for learning, that is to fit the parameters (e.g., weights) of, for example, a classifier).<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Most approaches that search through training data for empirical relationships tend to <strong>overfit</strong>  the data, meaning that they can identify apparent relationships in the training data that do not hold in general.</font></p>
<h2 id="2-Test-dataset"><a href="#2-Test-dataset" class="headerlink" title="2.Test dataset"></a>2.Test dataset</h2><p><font face="Times New Roman">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A test dataset is a dataset that is  <strong>independent</strong>  of the training dataset, but that follows the same <strong>probability distribution </strong> as the training dataset. If a model fit to the training dataset also fits the test dataset well, minimal overfitting has taken place . A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A test set is therefore a set of examples used only to assess the performance (i.e.  <strong>generalization</strong> ) of a fully specified classifier.</font></p>
<h2 id="3-Validation-dataset"><a href="#3-Validation-dataset" class="headerlink" title="3.Validation dataset"></a>3.Validation dataset</h2><p><font face="Times New Roman">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A validation dataset is a set of examples used to tune the <strong>hyperparameters</strong>  (i.e. the architecture) of a classifier. In artificial neural networks, a hyperparameter is, for example, the number of hidden units. It, as well as the testing set (as mentioned above), should follow the same probability distribution as the training dataset.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>In order to avoid overfitting</strong>  , when any classification parameter needs to be adjusted, it is necessary to have a validation dataset in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training dataset is used to train the candidate algorithms, the validation dataset is used to compare their performances and decide which one to take and, finally, the test dataset is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on. The validation dataset functions as a hybrid: it is training data used by testing, but neither as part of the low-level training nor as part of the final testing.</font></p>
<h2 id="4-意义"><a href="#4-意义" class="headerlink" title="4.意义"></a>4.意义</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般来说，机器学习中包含两种参数。一是普通参数和超参数。对应到神经网络中，普通参数为权重，超参数为隐藏层单元的个数，迭代的次数等。我们通常会将数据集划分为三部分，训练集、验证集、测试集。比例为8:1:1。三个集合无交集是同分布的。训练集用来学习普通的参数。验证集用来验证学习到的模型的准确率，调整模型的超参数，在神经网络中可以是隐藏单元的个数或者确定网络的结构（model selection）。测试集是对验证集中挑选出的模型进行最终的性能检测。<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 例如在神经网络的训练中，验证集是没有参与训练的过程的。但是，在超参数的选择中，我们根据验证集的结果调整迭代的次数等，从这个角度上看，验证集也是参与了调参的过程。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，如果是在比赛中，官方提供的数据只有一个训练集和无标签的测试集。由于官方一般给的训练集不是很多，一般不用再划分一个测试集。</p>
<p>参考资料：<a href="https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets" target="_blank" rel="noopener">维基百科</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: 'window.location.pathname', // 可选。默认为 location.href
  title: '训练集 验证集 测试集',
  owner: 'goingcoder',
  repo: 'goingcoder.github.io',
  oauth: {
      client_id: 'de24b44123b8efbb4747',
      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',

  },
})
gitment.render('container')
</script>

]]></content>
      
        
    </entry>
    
  
  
</search>
