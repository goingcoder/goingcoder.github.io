<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>离弦的博客</title>
  
  <subtitle>我若为王,舍我其谁</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://goingcoder.github.io/"/>
  <updated>2018-07-08T07:05:06.573Z</updated>
  <id>https://goingcoder.github.io/</id>
  
  <author>
    <name>goingcoder</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Contextual String Embeddings for Sequence Labeling笔记</title>
    <link href="https://goingcoder.github.io/2018/07/08/ner6/"/>
    <id>https://goingcoder.github.io/2018/07/08/ner6/</id>
    <published>2018-07-08T05:40:45.000Z</published>
    <updated>2018-07-08T07:05:06.573Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    使用递归神经网络进行语言建模的最新进展表明基于字符的语言建模是可行的。这篇论文提出利用一个预训练的字符语言模型的内部状态，以产生一种新型的字嵌入，称之为上下文字符串向量。我们提出的embedding有明显的优点：（1）在没有任何明确的词汇概念的情况下进行训练，从根本上将单词作为字符序列建模。（2）由其周围语境上下文化，这意味着相同的单词的embedding将根据其不同的语境的不同而不同。我们对先前的embedding进行比较评估，发现我们的embedding对于下游任务非常有用。在四个经典序列标记任务中，我们始终优于当前最优的结果。</p><a id="more"></a><p>​    在一个大规模未标注语料上进行训练，用来辅助学习和泛化放的的word embedding是一个关键的部分。目前最优的方法中连接了三种embedding：</p><ol><li>word embedding(Pennington et al., 2014; Mikolov et al., 2013)：在大规模语料上预训练的，为捕捉潜在句法和语义相似性。</li><li>Character-level features(Ma and Hovy, 2016; Lample et al., 2016)：不是预训练，在任务数据上训练，为捕捉特点任务的词根特征（subword features）。</li><li>Contextualized word embeddings(Peters et al., 2017; Peters et al., 2018)：捕捉词在上下文中的语义，解决词汇的多义性和语境依赖性。</li></ol><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ol><li><p>在一个大规模无标注语料上预训练</p></li><li><p>捕获词在上下文的意思，因此可以对多义词根据他们的用法产生不同的embedding</p></li><li><p>对词和上下文作为一个字符的序列，可以更好的处理生僻词以及拼写错误的的单词和对词根（如前缀和后缀）的建模。</p></li></ol><p><img src="/2018/07/08/ner6/p1.png" alt=""></p><p>​    对没一句话作为一个字符序列传递给双向字符级的语言模型，从语言模型中提取每一个词的embedding，这个embedding可以用于BiLSTM-CRF序列标注模型的中。</p><p><img src="/2018/07/08/ner6/p2.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文做了几个尝试（different stackings of embeddings）。</p><ol><li><p>PROPOSED : 只使用预训练的contextual string embeddings。就是上图所示</p></li><li><p>PROPOSED+WORD：加入预训练的word embedding<br> $$</p><p> $$</p><p> $$<br> w_i=\begin{bmatrix} w^{ChatLM}_i\\w^{Glove}_i  \ \end{bmatrix}<br> $$</p></li><li><p>PROPOSED+CHAR :加入char embedding</p></li><li><p>PROPOSED+WORD+CHAR</p></li><li><p>PROPOSED+ALL : Finally, we evaluate a setup that uses all four embeddings. Since Peters et al. (2018)<br> embeddings are only distributed for English, we use this setup only on the English tasks.</p></li></ol><p>实验结果</p><p><img src="/2018/07/08/ner6/p3.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p>字符级语言模型独立于token和固定的词典。</p></li><li><p>可以产生更强的字符特征，用于下游的NLP任务，比如NER。</p></li><li>这个模型有一个更小的字典（distinct characters vs. distinct words），在一个GPU上只需要训练一周，而基于词的语言模型需要在32个GPU上训练五个星期。(Peters et al. (2017))</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><strong>Contextual String Embeddings for Sequence Labeling</strong> – Alan Akbik, Duncan Blythe and Roland Vollgraf.</li></ul><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'Contextual String Embeddings for Sequence Labeling笔记',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {      client_id: 'de24b44123b8efbb4747',      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;​    使用递归神经网络进行语言建模的最新进展表明基于字符的语言建模是可行的。这篇论文提出利用一个预训练的字符语言模型的内部状态，以产生一种新型的字嵌入，称之为上下文字符串向量。我们提出的embedding有明显的优点：（1）在没有任何明确的词汇概念的情况下进行训练，从根本上将单词作为字符序列建模。（2）由其周围语境上下文化，这意味着相同的单词的embedding将根据其不同的语境的不同而不同。我们对先前的embedding进行比较评估，发现我们的embedding对于下游任务非常有用。在四个经典序列标记任务中，我们始终优于当前最优的结果。&lt;/p&gt;
    
    </summary>
    
      <category term="ner" scheme="https://goingcoder.github.io/categories/ner/"/>
    
    
      <category term="ner" scheme="https://goingcoder.github.io/tags/ner/"/>
    
  </entry>
  
  <entry>
    <title>Deep contextualized word representations论文笔记</title>
    <link href="https://goingcoder.github.io/2018/07/08/ner5/"/>
    <id>https://goingcoder.github.io/2018/07/08/ner5/</id>
    <published>2018-07-08T01:22:44.000Z</published>
    <updated>2018-07-08T05:41:24.957Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    预训练的词向量是很多神经语言理解模型的关键部分，然而学习高质量的表示是很有挑战性的。一个好的词向量应该包含：（1）能够建模词的复杂字符特征（比如语法和语义），（2）能够在不同语境有不同反映（如多义词）。这篇论文中，引入一种新类型的深度上下文词表示，可以应对这些挑战，并且可以容易与当前存在的模型进行整合，有效提升了一系列有挑战的语言理解问题的最优性能。</p><a id="more"></a><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>​    ELMO是一个语言模型中中间层表示的特点任务的组合。对于每一个token$t_k$,一个L层的双向语言模型计算$2L+1$表征。<br>$$<br>R_k={ x^{LM}_k,h^{LM}_{k,j}|j=1,\dots ,L} \ ={h^{LM}_{k,j}|j=1,\dots ,L}<br>$$<br>$h^{LM}_{k,0}​$是token层，$h^{LM}_{k，j}​$是双向LSTM层<br>$$<br>ELMo^{task}_k=E(R_k;\theta^task)=\gamma^{task}\sum^L_{j=0}s^{task}_jh^{LM}_{k.j}<br>$$<br>​    最简单的情况下，ELMO只选择顶层，即$E(R_k)=h^{LM}_{k,L}$,如TagLM（Peters et al., 2017 ）和 CoVe（McCann et al., 2017） 。$s^{task}$是softmax归一化权重。ELMo的训练过程与一般语言模型相似，在使用时需要对各层输出进行加权。 </p><h2 id="将ELMo模型加入到有监督的NLP任务中"><a href="#将ELMo模型加入到有监督的NLP任务中" class="headerlink" title="将ELMo模型加入到有监督的NLP任务中"></a>将ELMo模型加入到有监督的NLP任务中</h2><p>三种方法</p><ul><li>先固定biLM中权重，然后拼接$ELMo^{task}_k$和$x_k$为$[x_k;ELMo^{task}_k]$,再将这个向量丢到任务RNN中</li><li>在task RNN输出的时候使用$[h_k;ELMo^{LM}_k]$替换$h_k$。</li><li>在ELMo中加入dropout以及给loss增加$\lambda||w||^2_2$</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>​    在NER任务上，ELMo加强的BiLSTM-CRF的f1值为92.22%。</p><p><img src="/2018/07/08/ner5/p1.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>感觉和17年ACL的TagLM很像，只是在LM上加了一些改动，对不同的层的加入一些权重。主要的问题还是在训练语言模型上。</li><li>词向量应该随着上下文的改变而改变</li><li>不同层的双向RNN可以编码不同类型的信息。ELMo第一层输出包含更多的句法信息，而第二层输出包含更多的语义信息。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations[J]. arXiv preprint arXiv:1802.05365, 2018. </p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'Deep contextualized word representations论文笔记',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {      client_id: 'de24b44123b8efbb4747',      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',  },})gitment.render('container')</script><p>​    </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;​    预训练的词向量是很多神经语言理解模型的关键部分，然而学习高质量的表示是很有挑战性的。一个好的词向量应该包含：（1）能够建模词的复杂字符特征（比如语法和语义），（2）能够在不同语境有不同反映（如多义词）。这篇论文中，引入一种新类型的深度上下文词表示，可以应对这些挑战，并且可以容易与当前存在的模型进行整合，有效提升了一系列有挑战的语言理解问题的最优性能。&lt;/p&gt;
    
    </summary>
    
      <category term="ner" scheme="https://goingcoder.github.io/categories/ner/"/>
    
    
      <category term="ner" scheme="https://goingcoder.github.io/tags/ner/"/>
    
  </entry>
  
  <entry>
    <title>Semi-supervised sequence tagging with bidirectional language models</title>
    <link href="https://goingcoder.github.io/2018/07/06/ner4/"/>
    <id>https://goingcoder.github.io/2018/07/06/ner4/</id>
    <published>2018-07-06T08:23:33.000Z</published>
    <updated>2018-07-06T10:36:51.952Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    对于NLP任务，从无标签文本中学习预训练的embedding已经是神经网络的一个标准部分。在这篇论文中，提出了将一个双向的语言模型的context embedding引入序列标注任务中，在NER和chunking两个标准数据集上进行了实验，证明了与其他的某些使用迁移学习和使用附加标注数据和特定任务字典的联合学习相比，效果都要好，超过了以往所有的模型。</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul><li><p>主要贡献是表明在LM embedding中捕获的上下文敏感表示在监督序列标记设置中是有用的。</p></li><li><p>双向的LM embedding比前向的LM embedding效果要好。</p><a id="more"></a></li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>​    一个语言模型是计算一个序列$(t_1,t_2,…,t_N)$的概率<br>$$<br>p(t_1,t_2,\dots,t_N)=\prod_{k=1}^Np(t_k|t_1,t_2,\dots,t_k-1)<br>$$</p><h3 id="TagLM-language-model-augmented-sequence-tagger"><a href="#TagLM-language-model-augmented-sequence-tagger" class="headerlink" title="TagLM(language-model-augmented sequence tagger)"></a>TagLM(language-model-augmented sequence tagger)</h3><p><img src="/2018/07/06/ner4/p1.PNG" alt="1"></p><p>​    图的左边是标准的LSTM-CRF模型，这里LSTM使用了两层。图的右边是语言模型。语言模型的加入是在第一层双向LSTM输出之后进行拼接的，因为这个位置效果最好。作者做了三个尝试，将LM embeddings分别拼接在第一层RNN的输入、第一层RNN的输出、第二层RNN的输出。实验中，第二种方案的表现最好。 正是上图展示的那样。</p><h3 id="未实验的其他LM-embedding加入方式"><a href="#未实验的其他LM-embedding加入方式" class="headerlink" title="未实验的其他LM embedding加入方式"></a>未实验的其他LM embedding加入方式</h3><ul><li>在第一层RNN拼接之后加入一个非线性映射$ f([ h_{k,1};h^{LM}_k])$</li><li>对 LM embedding加入attention机制</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>​    在CoNLL2003 NER任务和CoNLL Chunking 任务上进行了实验。</p><p><img src="/2018/07/06/ner4/p2.PNG" alt="2"></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>###1.How to use LM embedding?</p><p>作者在不同的位置上对LM embedding进行拼接</p><ul><li><p>在第一层RNN的输入时，<br>  $$</p><p>  $$</p><p>  $$<br>  x_k=[c_k;,w_k;h^{LM}_k]<br>  $$</p></li><li><p>在第一层RNN输出时<br>  $$<br>  h_{k,1}=[h_{k,1};h^{LM}_k]<br>  $$</p></li><li><p>在第二层RNN输出时<br>  $$<br>  h_{k,2}=[h_{k,2};h^{LM}_k]<br>  $$<br>  实验结果</p><p>  <img src="/2018/07/06/ner4/p3.PNG" alt="3"></p><p>  <strong>结果表明，在第一层RNN输出时拼接效果做好</strong></p><h3 id="2-Does-it-matter-which-language-model-to-use"><a href="#2-Does-it-matter-which-language-model-to-use" class="headerlink" title="2.Does it matter which language model to use?"></a>2.Does it matter which language model to use?</h3><p>  不同语言模型的影响。</p><p>  <img src="/2018/07/06/ner4/p4.PNG" alt="4"></p></li><li><p>语言模型的大小是重要的。</p></li><li><p>双向语言模型比前向语言模型要好</p></li></ul><p>###3.Importance of task specific RNN</p><p>​    不使用RNN只使用全连接层，$F_1$为88.71，低于baseline。</p><h3 id="4-Dataset-size"><a href="#4-Dataset-size" class="headerlink" title="4.Dataset size"></a>4.Dataset size</h3><p>作者的方法对训练集大小的依赖很小，在使用更大的训练集时，语言模型会有一个明显的提升。</p><h3 id="5-Number-of-parameters"><a href="#5-Number-of-parameters" class="headerlink" title="5.Number of parameters"></a>5.Number of parameters</h3><p>没大看懂对比</p><h3 id="6-Does-the-LM-transfer-across-domains"><a href="#6-Does-the-LM-transfer-across-domains" class="headerlink" title="6.Does the LM transfer across domains?"></a>6.Does the LM transfer across domains?</h3><p>​    这篇论文证明语言模型是可以跨领域的。将新闻领域训练出来的语言模型，用到科学论文的任务上，也有效果也有提升。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>LM embedding 与word embedding类似语义信息，LM embedding感觉更像序列级别的，在加入LM embedding 到RNN中时，在与第一层RNN拼接时效果也是最好的。word embedding也是包含上下文信息的，感觉两个embedding包含的信息有部分的重叠，虽然有一个点的提升。</li><li>对人们比较关心的几个问题都做了附加的分析，让人心服口服，值得学习。</li><li>即使只是对基础模型的一个细小改动，增加一个类型信息（本文是语言模型的上下文信息），如果把实验做的充分，结果显著，也是可以发出来文章的。</li><li>感觉得presentation者得天下……</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. <em>ACL</em>. <a href="http://doi.org/10.18653/v1/P17-1194" target="_blank" rel="noopener">http://doi.org/10.18653/v1/P17-1194</a> </p><p>[2] <a href="http://oyeblog.com/2017/paper_0_LM/" target="_blank" rel="noopener">http://oyeblog.com/2017/paper_0_LM/</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'Semi-supervised sequence tagging with bidirectional language models',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {      client_id: 'de24b44123b8efbb4747',      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;​    对于NLP任务，从无标签文本中学习预训练的embedding已经是神经网络的一个标准部分。在这篇论文中，提出了将一个双向的语言模型的context embedding引入序列标注任务中，在NER和chunking两个标准数据集上进行了实验，证明了与其他的某些使用迁移学习和使用附加标注数据和特定任务字典的联合学习相比，效果都要好，超过了以往所有的模型。&lt;/p&gt;
&lt;h2 id=&quot;贡献&quot;&gt;&lt;a href=&quot;#贡献&quot; class=&quot;headerlink&quot; title=&quot;贡献&quot;&gt;&lt;/a&gt;贡献&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;主要贡献是表明在LM embedding中捕获的上下文敏感表示在监督序列标记设置中是有用的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;双向的LM embedding比前向的LM embedding效果要好。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="ner" scheme="https://goingcoder.github.io/categories/ner/"/>
    
    
      <category term="ner" scheme="https://goingcoder.github.io/tags/ner/"/>
    
  </entry>
  
  <entry>
    <title>CNN卷积神经网络对MNIST数据分类</title>
    <link href="https://goingcoder.github.io/2018/05/17/tf10/"/>
    <id>https://goingcoder.github.io/2018/05/17/tf10/</id>
    <published>2018-05-17T06:31:32.000Z</published>
    <updated>2018-05-17T07:47:56.021Z</updated>
    
    <content type="html"><![CDATA[<p>​    Convolutional Neural Networks (CNN) 是神经网络处理图片信息的一大利器. 因为利用卷积神经网络在图像和语音识别方面能够给出更优预测结果, 这一种技术也被广泛的传播可应用. 卷积神经网络最常被应用的方面是计算机的图像识别, 不过因为不断地创新, 它也被应用在视频分析, 自然语言处理, 药物发现, 等等. 近期最火的 Alpha Go, 让计算机看懂围棋, 同样也是有运用到这门技术。</p><p>​    这里将建立一个卷积神经网络，它可以把MNIST手写字符的识别准确率提高到99%，读者可能需要一些卷积神经网络的基础知识才能更好地理解本节的内容。</p><p>​    程序的开头依旧是导入Tensorflow：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br></pre></td></tr></table></figure><p>​    接下来载入MNIST数据集，并建立占位符。占位符x的含义为训练图像，y_为对应训练图像的标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>​    由于要使用的是卷积神经网络对图像进行分类，所以不能再使用784维的向量表示输入的x，而是将其还原为28*28的图片形式。[-1,28,28,1]中的-1表示形状第一维的大小是根据x自动确定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">  x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>​    x_image就是输入的训练图像，接下来。我们堆训练图像进行卷积操作，第一层卷积的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"><span class="comment"># 第一层卷积层</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br></pre></td></tr></table></figure><p>​    先定义四个函数，函数weight_variable可以返回一个给定形状的变量并自动以截断正态分布初始化，bias_variable同样返回一个给定形状的变量，初始化时所有值是0.1，可分别用这两个函数创建卷积的核与偏置。h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)是真正进行卷积运算，卷积计算后选用ReLU作为激活函数。h_pool1 = max_pool_2x2(h_conv1)是调用函数max_pool_2x2(h_conv1)进行一次池化操作。卷积、激活函数、池化，可以说是一个卷积层的“标配”,通常一个卷积层都会包含这三个步骤，有时会去掉最后的池化操作。</p><p>​    tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。 </p><p>​    对第一个卷积操作后产生的h_pool1再做一次卷积计算，使用的代码与上面类似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure><p>两层卷积之后是全连接层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">   W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">   b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">   h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">   h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">   <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">   keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">   h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure><p>​    在全连接中加入了Dropout，它是防止神经网络过拟合的一种手段。在每一步训练时，以一定的概率“去掉”网络中的某些连接，但这种去除不是永久性的，只是在当前步骤中去除，并且每一步去除的连接都是随机选择的。在这个程序中，选择的Dropout概率是0.5,也就是说训练每一个连接都有50%的概率被去除。在测试时保留所有连接。</p><p>​    最后，在加入一层全连接层，把上一步得到的h_fc1_drop转换为10个类别的打分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">   W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">   b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">   y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure><p>​    y_conv相当于Softmax模型中的Logit，当然可以使用Softmax函数将其转换为10个类别的概率，在定义交叉熵损失。但其实Tensorflow提供了一个更直接的cross_entropy = tf.reduce_mean(    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))函数，它可以直接对Logit定义交叉熵损失，写法为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line"><span class="comment"># 同样定义train_step</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><p>定义测试的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义测试的准确率</span></span><br><span class="line">   correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure><p>​    在验证集上计算模型准确率并输出，方便监控训练的进度，也可以据此调整模型的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br></pre></td></tr></table></figure><p>​    训练结束后，打印在全体测试集上的准确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure><p>​    得到的准确率结果应该在99%左右。与Softmax回归模型相比，使用两层卷积神经网络模型借助了卷积的威力，准确率非常大的提升。</p><p>完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读入数据</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># x为训练图像的占位符、y_为训练图像标签的占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="comment"># 将单张图片从784维向量重新还原为28x28的矩阵图片</span></span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># print(x_image.shape)</span></span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 第一层卷积层</span></span><br><span class="line">    W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line"></span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">    print(h_pool1.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层卷积层</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"></span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">    print(h_pool2.shape)</span><br><span class="line">    <span class="comment"># os._exit(0)</span></span><br><span class="line">    <span class="comment"># 全连接层，输出为1024维的向量</span></span><br><span class="line">    W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">    <span class="comment"># 使用Dropout，keep_prob是一个占位符，训练时为0.5，测试时为1</span></span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把1024维的向量转换成10维，对应10个类别</span></span><br><span class="line">    W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们不采用先Softmax再计算交叉熵的方法，而是直接用tf.nn.softmax_cross_entropy_with_logits直接计算</span></span><br><span class="line">    <span class="comment"># cross_entropy = tf.reduce_mean(</span></span><br><span class="line">    <span class="comment">#     tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(</span><br><span class="line">        tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同样定义train_step</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试的准确率</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建Session和变量初始化</span></span><br><span class="line">    sess = tf.InteractiveSession()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练20000步</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">        batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">        <span class="comment"># 每100步报告一次在验证集上的准确度</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">                x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">            print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练结束后报告在测试集上的准确度</span></span><br><span class="line">    print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/v_july_v/article/details/51812459" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/51812459</a></p><p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p><p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="noopener">https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/</a></p><p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    Convolutional Neural Networks (CNN) 是神经网络处理图片信息的一大利器. 因为利用卷积神经网络在图像和语音识别方面能够给出更优预测结果, 这一种技术也被广泛的传播可应用. 卷积神经网络最常被应用的方面是计算机的图像识别, 不过因为
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow入门笔记1</title>
    <link href="https://goingcoder.github.io/2018/05/14/tf9/"/>
    <id>https://goingcoder.github.io/2018/05/14/tf9/</id>
    <published>2018-05-14T07:29:59.000Z</published>
    <updated>2018-05-14T11:57:59.382Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、MNIST数据集介绍"><a href="#1、MNIST数据集介绍" class="headerlink" title="1、MNIST数据集介绍"></a>1、MNIST数据集介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍MNIST数据集。如果所示，MNIST数据集主要由一些手写数字的图片和相应的标签组成，图片一共有10类，分别对应0~9，共10个阿拉伯数字。如下图：</p><p><img src="/2018/05/14/tf9/image1.png" alt="1"></p><p>MNIST 数据集可在 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> 获取, 它包含了四个部分:</p><ul><li><strong>Training set images</strong>: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</li><li><strong>Training set labels</strong>: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</li><li><strong>Test set images</strong>: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</li><li><strong>Test set labels</strong>: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MNIST 数据集来自美国国家标准与技术研究所, <strong>National Institute of Standards and Technology (NIST)</strong>. 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据。 60,000 个训练样本和 10,000 个测试样本. </p><a id="more"></a><h2 id="2、下载数据集"><a href="#2、下载数据集" class="headerlink" title="2、下载数据集"></a>2、下载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从tensorflow.examples.tutorials.mnist引入模块。这是TensorFlow为了教学MNIST而提前编制的程序</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># 从MNIST_data/中读取MNIST数据。这条语句在数据不存在时，会自动执行下载</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在执行语句mnist = input_data.read_data_sets(“MNIST_data/“, one_hot=True)时，Tensorflow会检测数据是否存在。当数据不存在时，系统自动将数据下载到MNIST_data/文件夹中。当执行完语句后，可以看到MNIST_data/文件夹下多了4个上述文件。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;成功加载数据后，得到了一个mnist对象，可以通过mnist对象的数学访问到MNIST数据集。</p><table><thead><tr><th>属性名</th><th>内容</th><th>大小</th></tr></thead><tbody><tr><td>mnist.train,images</td><td>训练图像</td><td>(55000,784)</td></tr><tr><td>mnist.train.label</td><td>训练标签</td><td>(55000,10)</td></tr><tr><td>mnist.validation.images</td><td>验证图像</td><td>（5000,784）</td></tr><tr><td>mnist.validation.labels</td><td>验证标签</td><td>（5000,10）</td></tr><tr><td>mnist.test.images</td><td>测试图像</td><td>（10000,784）</td></tr><tr><td>mnist.test.labels</td><td>测试标签</td><td>（10000,10）</td></tr></tbody></table><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运行下面代码可以查看各个变量的形状大小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看训练数据的大小</span></span><br><span class="line">print(mnist.train.images.shape)  <span class="comment"># (55000, 784)</span></span><br><span class="line">print(mnist.train.labels.shape)  <span class="comment"># (55000, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看验证数据的大小</span></span><br><span class="line">print(mnist.validation.images.shape)  <span class="comment"># (5000, 784)</span></span><br><span class="line">print(mnist.validation.labels.shape)  <span class="comment"># (5000, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看测试数据的大小</span></span><br><span class="line">print(mnist.test.images.shape)  <span class="comment"># (10000, 784)</span></span><br><span class="line">print(mnist.test.labels.shape)  <span class="comment"># (10000, 10)</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 原始的MNIST数据集中包含了60000张训练图片和10000张测试图片。而在tensorflow中，又将原先的60000张训练图片重新划分成了新的55000张训练图片和5000张验证图片。所以在mnist对象中，数据一共分为三部分：mnist.train是训练图片数据，mnist.validation是验证图片数据，mnist.test是测试图片数据，这正好对应了机器学习中额训练集、验证集和测试集、一般来说，会在训练集上训练模型，通过在验证集上调整参数，最后通过测试集确定模型的性能。</p><p>MNIST数据集保存为图片</p><p>为了加深理解，将MNIST数据集读取出来，并保存为图片文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取MNIST数据集。如果不存在会事先下载。</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 我们把原始图片保存在MNIST_data/raw/文件夹下</span></span><br><span class="line"><span class="comment"># 如果没有这个文件夹会自动创建</span></span><br><span class="line">save_dir = <span class="string">'MNIST_data/raw/'</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(save_dir) <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">    os.makedirs(save_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存前20张图片</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    <span class="comment"># 请注意，mnist.train.images[i, :]就表示第i张图片（序号从0开始）</span></span><br><span class="line">    image_array = mnist.train.images[i, :]</span><br><span class="line">    <span class="comment"># TensorFlow中的MNIST图片是一个784维的向量，我们重新把它还原为28x28维的图像。</span></span><br><span class="line">    image_array = image_array.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    <span class="comment"># 保存文件的格式为 mnist_train_0.jpg, mnist_train_1.jpg, ... ,mnist_train_19.jpg</span></span><br><span class="line">    filename = save_dir + <span class="string">'mnist_train_%d.jpg'</span> % i</span><br><span class="line">    <span class="comment"># 将image_array保存为图片</span></span><br><span class="line">    <span class="comment"># 先用scipy.misc.toimage转换为图像，再调用save直接保存。</span></span><br><span class="line">    scipy.misc.toimage(image_array, cmin=<span class="number">0.0</span>, cmax=<span class="number">1.0</span>).save(filename)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Please check: %s '</span> % save_dir)</span><br></pre></td></tr></table></figure><h2 id="3、图像标签的独热表示"><a href="#3、图像标签的独热表示" class="headerlink" title="3、图像标签的独热表示"></a>3、图像标签的独热表示</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变量mnist.train.labels表示训练数据的标签，它的形状是(55000,10)。原始的图像标签是数字0-9，我们完全可以用一个数字来存储图像标签，但为什么这里每个训练标签是一个10维的向量呢？其实，这个10维的向量是原先类别的独热(one-hot)表示。</p><h2 id="4、Softmax回归识别MNIST"><a href="#4、Softmax回归识别MNIST" class="headerlink" title="4、Softmax回归识别MNIST"></a>4、Softmax回归识别MNIST</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;softmax回归是一个线性的多分类模型，实际上它是从Logistic回归模型转化而来的。区别是Logistic回归模型是一个两分类模型，而Softmax模型为多分类模型。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在手写体识别问题中，一共有10个类别(0~9)，我们希望对输入的图像计算它属于每个类别的概率。如属于9的概率为70%，属于1的概率为10%等。最后模型预测的结果就是概率最大的那个类别。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先来了解什么是Softmax函数。Softmax函数的主要功能是将各个类别的“打分“转化成合理的概率值。例如，一个样本可能属于三个类别：第一个类别的打分为a，第二个类别的打分为b，第三个类别的打分为c。打分越高代表属于这个类别的概率越高，但是打分本身不代表概率，因为打分的值可以使负数，也可以很大。但概率要求值必须在0~1，并且三类的概率加起来等于1.那么，如何将(a,b,c)转换成合理的概率值呢？方法就是使用Softmax函数。例如，对(a,b,c)使用softmax函数后，相异的值会变成$({e^a \over e^a+e^b+e^c},{e^b \over e^a+e^b+e^c},{e^c \over e^a+e^b+e^c})$，也就是说，第一类的概率用$e^a \over e^a+e^b+e^c$来表示，第二类用$e^b \over e^a+e^b+e^c$来表示，第三类可以用$e^c \over e^a+e^b+e^c$来表示。显然，这三个数都在0~1之间，并且加起来正好等于1，是合理的概率表示。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设$x$是单个样本的特征，$W,b$是Softmax模型的参数。在MNIST数据集中，$x$就代表输入图片，它是一个784维的向量，而$W$是一个矩阵，它的形状为(784,10),$b$是一个10维的向量，10代表的是类别数。Softmax模型的第一步是通过下面的公司计算各个类别的Logit：<br>$$<br>Logit=W^Tx+b<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logit同样是一个10维的向量，它的实际上可以看出样本对于于各个类别的“打分”。接下来使用Softmax函数将他转换成各个类别的概率值：<br>$$<br>y=Softmax(Logit)<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Softmax模型输出的y代表各个类别的概率，还可以直接用下面的式子来表示整个Softmax模型：<br>$$<br>y=Softmax(W^Tx+b)<br>$$</p><p>##5、Softmax实现</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先导入tensorflow模型,下面是约定俗成的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来导入MNIST数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;构建模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建x，x是一个占位符（placeholder），代表待识别的图片</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line"><span class="comment"># W是Softmax模型的参数，将一个784维的输入转换为一个10维的输出</span></span><br><span class="line"><span class="comment"># 在TensorFlow中，变量的参数用tf.Variable表示</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># b是又一个Softmax模型的参数，我们一般叫做“偏置项”（bias）。</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># y=softmax(Wx + b)，y表示模型的输出</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line"><span class="comment"># y_是实际的图像标签，同样以占位符表示。</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里定义了一下占位符(placeholder)和变量(Variable).在tensorflow中无论是占位符还是变量，它们实际上都是“Tensor”。从Tensorflow的名字中就可以看出Tensor在整个系统处于核心地位。Tensor并不是具体的数值，它只是一些我们”希望“Tensorflow系统计算的“节点”。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里的占位符和变量是不同类型的Tensor。先来讲解占位符。占位符不依赖于其它的Tensor，它的值由用户自行传到给Tensorflow，通常用来存储样本数据和标签。如在这里定义了x = tf.placeholder(tf.float32, [None, 784])，它是用来存储训练图片数据的占位符，它的形状为[None, 784]，None表示这一维的大小是任意的，也就是说可以传递任意章训练图片给这个占位符，每张图片用784维的向量表示。同样的，y_ = tf.placeholder(tf.float32, [None, 10])也是一个占位符，它存储训练图片的实际标签。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再来看什么是变量，变量是指在计算过程中可以改变的值，每次计算后变量的值会被保存下来，通常用变量来存储模型的参数。如：W = tf.Variable(tf.zeros([784, 10]))。创建变量时通常要指定某些初始值。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除了变量和占位符之外，还创建了一个y</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个y就是一个依赖x，W，b的Tensor。如果要求Tensorflow计算y的值，那么系统首先会获取$x,W,b$的值，再去计算y的值</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y实际上定义了一个Softmax回归模型，在此可以尝试写出y的形状。假设输入x的形状为(N,784)，其中N表示输入的训练图像的数目。W的形状为（784,10），b的形状为（10），那么Wx+b的形状是(N,10)。Softmax函数不改变结果的形状，所以得到y的形状为(N,10)。也就是说，y的每一行是10维的向量，表示模型预测的样本对应到各个类别的概率。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型的输出是y，而实际的标签为y_,它们应当越相似越好。在Softmax回归模型中，通常使用“交叉熵”损失来衡量相似性。损失越小，模型的输出就和实际标签越接近，模型的预测也就越准确。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Tensorflow中，这样定义交叉熵损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据y, y_构造交叉熵损失</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;构造玩损失之后，下面一步是如何优化损失，让损失减小。这里使用梯度下降优化损失，定义为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有了损失，我们就可以用随机梯度下降针对模型的参数（W和b）进行优化</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><p>​    Tensorfloe默认会对所有变量计算梯度。这里只定义两个变量$W$和$b$，因此程序将会使用梯度下降法对$W,b$计算梯度并更新它们的值。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在优化之前，必须要创建一个会话(Session),并在会话中对变量进行初始化操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个Session。只有在Session中才能运行优化步骤train_step。</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 运行之前必须要初始化所有变量，分配内存。</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了会话，就可以对变量进行优化了，优化的程序如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 在mnist.train中取100个训练数据</span></span><br><span class="line">    <span class="comment"># batch_xs是形状为(100, 784)的图像数据，batch_ys是形如(100, 10)的实际标签</span></span><br><span class="line">    <span class="comment"># batch_xs, batch_ys对应着两个占位符x和y_</span></span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 在Session中运行train_step，运行时要传入占位符的值</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与变量不同的是，占位符的值不会被保存，每次可以给占位符传递不同的值。</p><p>​    训练完成后，可以检测模型训练的结果，对于的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正确的预测结果</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算预测准确率，它们都是Tensor</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"><span class="comment"># 在Session中运行Tensor可以得到Tensor的值</span></span><br><span class="line"><span class="comment"># 这里是获取最终模型的正确率</span></span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))  <span class="comment"># 0.9052</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型预测y的形状是(N,10)，而实际标签y_的形状是(N,10)，其中N为输入模型1的样本数。tf.argmax(y,1)、tf.argmax(y_,1)的功能是取出数组最大值得下标，可以用来将独热表示以及模型输出转换为数字标签。假设传入四个样本，它们的独热表示y_为（需要通过sess.run(y_)才能获取此Tensor的值，下同）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]]</span><br></pre></td></tr></table></figure><p>tf.argmax(y_,1)就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>​    也就是说，取出每一行最大值对应的下标位置，它们是输入样本的实际标签。假设此时模型的预测输出y为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>],</span><br><span class="line"> [<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>],</span><br><span class="line"> [<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>],</span><br><span class="line"> [<span class="number">0.91</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>,<span class="number">0.01</span>]]</span><br></pre></td></tr></table></figure><p>tf.argmax(y_,1)就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>​    得到预测的标签和实际标签，接下来通过tf.equal函数来比较它们是否相等，并将结果保存到correct_prediction中。在上述例子中，correct_prediction就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">True</span>,<span class="keyword">False</span>,<span class="keyword">True</span>,<span class="keyword">False</span>]</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即第一个样本和最后一个样本预测是正确的，另外两个样本预测错误。可以用tf.cast(correct_prediction,tf.float32)将比较值转换成float32型的变量，此时True会被转换成1，False会被转换为0.在上述例子中，tf.cast(correct_prediction, tf.float32)的结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">1.</span>]</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，用tf.reduce_mean可以计算数组中的所有元素的平均值，相当于得到了模型的预测准确率，如[1.,0.,0.,1.]的平均值为0,5，即50%的分类准确率。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在程序softmax_regression.py中，传入占位符的值是feed_dict={x: mnist.test.images, y_: mnist.test.labels}。也就是说，使用全体测试样本进行测试，测试图片一共10000张，运行的结果为0.9052，即90.52%的准确率。因为softmax回归是一个比较简单的模型，这里的预测准确率并不高。</p><p>完整代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 导入MNIST教学的模块</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建x，x是一个占位符（placeholder），代表待识别的图片</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line"><span class="comment"># W是Softmax模型的参数，将一个784维的输入转换为一个10维的输出</span></span><br><span class="line"><span class="comment"># 在TensorFlow中，变量的参数用tf.Variable表示</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="comment"># b是又一个Softmax模型的参数，我们一般叫做“偏置项”（bias）。</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># y=softmax(Wx + b)，y表示模型的输出</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line"><span class="comment"># y_是实际的图像标签，同样以占位符表示。</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 至此，我们得到了两个重要的Tensor：y和y_。</span></span><br><span class="line"><span class="comment"># y是模型的输出，y_是实际的图像标签，不要忘了y_是独热表示的</span></span><br><span class="line"><span class="comment"># 下面我们就会根据y和y_构造损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据y, y_构造交叉熵损失</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))</span><br><span class="line"><span class="comment"># 有了损失，我们就可以用随机梯度下降针对模型的参数（W和b）进行优化</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Session。只有在Session中才能运行优化步骤train_step。</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"><span class="comment"># 运行之前必须要初始化所有变量，分配内存。</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">print(<span class="string">'start training...'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行1000步梯度下降</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 在mnist.train中取100个训练数据</span></span><br><span class="line">    <span class="comment"># batch_xs是形状为(100, 784)的图像数据，batch_ys是形如(100, 10)的实际标签</span></span><br><span class="line">    <span class="comment"># batch_xs, batch_ys对应着两个占位符x和y_</span></span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 在Session中运行train_step，运行时要传入占位符的值</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确的预测结果</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算预测准确率，它们都是Tensor</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"><span class="comment"># 在Session中运行Tensor可以得到Tensor的值</span></span><br><span class="line"><span class="comment"># 这里是获取最终模型的正确率</span></span><br><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id:  'window.location.pathname', // 可选。默认为 location.href  title:  'tensorflow入门笔记1',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、MNIST数据集介绍&quot;&gt;&lt;a href=&quot;#1、MNIST数据集介绍&quot; class=&quot;headerlink&quot; title=&quot;1、MNIST数据集介绍&quot;&gt;&lt;/a&gt;1、MNIST数据集介绍&lt;/h2&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;首先介绍MNIST数据集。如果所示，MNIST数据集主要由一些手写数字的图片和相应的标签组成，图片一共有10类，分别对应0~9，共10个阿拉伯数字。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2018/05/14/tf9/image1.png&quot; alt=&quot;1&quot;&gt;&lt;/p&gt;
&lt;p&gt;MNIST 数据集可在 &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt; 获取, 它包含了四个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training set images&lt;/strong&gt;: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training set labels&lt;/strong&gt;: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test set images&lt;/strong&gt;: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test set labels&lt;/strong&gt;: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;MNIST 数据集来自美国国家标准与技术研究所, &lt;strong&gt;National Institute of Standards and Technology (NIST)&lt;/strong&gt;. 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据。 60,000 个训练样本和 10,000 个测试样本. &lt;/p&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>embedding</title>
    <link href="https://goingcoder.github.io/2018/05/11/embedding/"/>
    <id>https://goingcoder.github.io/2018/05/11/embedding/</id>
    <published>2018-05-11T13:30:51.000Z</published>
    <updated>2018-05-11T13:31:16.384Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搜狐新闻数据-SogouCS"><a href="#搜狐新闻数据-SogouCS" class="headerlink" title="搜狐新闻数据(SogouCS)"></a>搜狐新闻数据(SogouCS)</h2><h5 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h5><p>来自搜狐新闻2012年6月—7月期间国内，国际，体育，社会，娱乐等18个频道的新闻数据，提供URL和正文信息</p><h5 id="格式说明："><a href="#格式说明：" class="headerlink" title="格式说明："></a>格式说明：</h5><p>数据格式为</p><doc><br><br><url>页面URL</url><br><br><docno>页面ID</docno><br><br><contenttitle>页面标题</contenttitle><br><br><content>页面内容</content><br><br></doc><p>注意：content字段去除了HTML标签，保存的是新闻正文文本</p><p>完整版(648MB)：<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.tar.gz" target="_blank" rel="noopener">tar.gz格式</a>，<a href="http://www.sogou.com/labs/resource/ftp.php?dir=/Data/SogouCS/news_sohusite_xml.full.zip" target="_blank" rel="noopener">zip格式</a> </p><h2 id="1、构建中文语料库"><a href="#1、构建中文语料库" class="headerlink" title="1、构建中文语料库"></a>1、构建中文语料库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜狐新闻 2.1G</span></span><br><span class="line">tar -zxvf news_sohusite_xml.full.tar.gz </span><br><span class="line">cat news_sohusite_xml.dat | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_sohusite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_sohusite.txt &gt; news_sohusite_cutword.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全网新闻 1.8G</span></span><br><span class="line">tar -zxvf news_tensites_xml.full.tar.gz </span><br><span class="line">cat news_tensites_xml.full.tar.gz | iconv -f gb18030 -t utf<span class="number">-8</span> | grep <span class="string">"&lt;content&gt;"</span> &gt; news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;content&gt;//g'</span> news_tensite.txt</span><br><span class="line">sed -i <span class="string">'s/&lt;\/content&gt;//g'</span> news_tensite.txt</span><br><span class="line">python -m jieba -d <span class="string">' '</span> news_tensite.txt &gt; news_tensite_cutword.txt</span><br></pre></td></tr></table></figure><p>第一条命令解压之后会生成news_sohusite_xml.dat文件</p><h2 id="2、利用gensim库进行训练"><a href="#2、利用gensim库进行训练" class="headerlink" title="2、利用gensim库进行训练"></a>2、利用gensim库进行训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment">#  Loading corpus...</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    input_file = <span class="string">"news_sohu_text.txt"</span></span><br><span class="line">    vector_file = <span class="string">"news_sohu_embedding"</span></span><br><span class="line">    <span class="comment"># 保存字典文件</span></span><br><span class="line">    vocab_file = <span class="string">"vocabulary_file"</span></span><br><span class="line">    sentences = word2vec.Text8Corpus(input_file)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Training embedding...</span></span><br><span class="line"><span class="string">    embedding_dim : 100</span></span><br><span class="line"><span class="string">    window_size ：5</span></span><br><span class="line"><span class="string">    minimum_count of word ：5</span></span><br><span class="line"><span class="string">    epoch of model（iter）：10</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 记录各种优先级的东西; 激活日志记录</span></span><br><span class="line">    logging.basicConfig(format=<span class="string">'%(asctime)s: %(levelname)s: %(message)s'</span>, level=logging.INFO)</span><br><span class="line">     <span class="comment"># sg=0 使用cbow训练, sg=1对低频词较为敏感，默认是sg=0</span></span><br><span class="line">    model = word2vec.Word2Vec(sentences, size=<span class="number">100</span>, window=<span class="number">5</span>, min_count=<span class="number">6</span>, iter=<span class="number">10</span>, workers=multiprocessing.cpu_count())</span><br><span class="line">    <span class="comment">#  Saving word_embedding</span></span><br><span class="line">    model.wv.save_word2vec_format(vector_file, fvocab=vocab_file, binary=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>​    input_file是输入文件，如果是训练词向量，必须是分词好的文件，词与词之间空格隔开。如果训练的是字向量，字与字之间同样用空格隔开。一个句子是一个列表，所有的句子是二维列表。</p><h2 id="3、加载训练好的embedding并测试"><a href="#3、加载训练好的embedding并测试" class="headerlink" title="3、加载训练好的embedding并测试"></a>3、加载训练好的embedding并测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型，embedding_file是训练好的向量文件</span></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=<span class="keyword">False</span>)</span><br><span class="line">temp1 = model.most_similar(<span class="string">'我'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">temp2 = model.most_similar(<span class="string">'好'</span>)  <span class="comment"># 测试相关词</span></span><br><span class="line">t = model.similarity(<span class="string">"他"</span>, <span class="string">"她"</span>)  <span class="comment"># 测试相似度</span></span><br><span class="line"></span><br><span class="line">print(temp1)</span><br><span class="line">print(temp2)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure><p>我在搜狐新闻训练的向量测试的结果：</p><p><code>[(&#39;咱&#39;, 0.8646953105926514), (&#39;他&#39;, 0.857693076133728), (&#39;你&#39;, 0.7627511024475098), (&#39;她&#39;, 0.7525184750556946), (&#39;俺&#39;, 0.7336732149124146), (&#39;它&#39;, 0.7071460485458374), (&#39;牠&#39;, 0.46839216351509094), (&#39;谁&#39;, 0.4390422999858856), (&#39;您&#39;, 0.40126579999923706), (&#39;尤&#39;, 0.3958999514579773)]</code></p><p><code>[(&#39;懂&#39;, 0.4896905720233917), (&#39;棒&#39;, 0.4554690718650818), (&#39;是&#39;, 0.4444722533226013), (&#39;佳&#39;, 0.44040754437446594), (&#39;让&#39;, 0.43987756967544556), (&#39;快&#39;, 0.4291210174560547), (&#39;朋&#39;, 0.42199385166168213), (&#39;错&#39;, 0.4218539297580719), (&#39;多&#39;, 0.42170557379722595), (&#39;乖&#39;,</code>0.4164968729019165)]<code>``</code>0.813737169426`</p><h2 id="4、训练注意"><a href="#4、训练注意" class="headerlink" title="4、训练注意"></a>4、训练注意</h2><p>Word2vec 有多个影响训练速度和质量的参数。</p><p>其中之一是用来修剪内部字典树的。在一个数以亿计的预料中出现一到两次的单词非常有可能是噪音或不需要被关注的。另外，也没有足够的数据对他们进行有意义的训练。因此，最好的办法就是直接将他们忽略掉。<br> <code>model = Word2Vec(sentences, min_count=10) # default value is 5</code></p><ol><li>对于设定 <code>min_count</code> 的值，合理的范围是0 - 100，可以根据数据集的规模进行调整。</li><li><p>另一个参数是神经网络 NN 层单元数，它也对应了训练算法的自由程度。</p><p><code>model = Word2Vec(sentences, size=200) # default value is 100</code><br>更大的 <code>size</code> 值需要更多的训练数据，但也同时可以得到更准确的模型。合理的取值范围是几十到几百。</p></li></ol><p>3.最后一个主要参数是训练并行粒度，用来加速训练。<br> <code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization</code><br> 该参数只有在机器已安装 Cython 情况下才会起到作用。如没有 Cython，则只能单核运行。</p><h2 id="5、评估"><a href="#5、评估" class="headerlink" title="5、评估"></a>5、评估</h2><p>Word2vec 训练是一个非监督任务，很难客观地评估结果。评估要<strong>依赖于后续的实际应用场景</strong>。Google 公布了一个包含 20,000 语法语义的测试样例，形式为 “A is to B as C is to D”。</p><p>需要注意的是，<strong>如在此测试样例上展示良好性能并不意味着在其它应用场景依然有效，反之亦然</strong>。</p><p>参考：</p><p>1.<a href="https://www.cnblogs.com/jkmiao/p/7007763.html" target="_blank" rel="noopener">https://www.cnblogs.com/jkmiao/p/7007763.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;搜狐新闻数据-SogouCS&quot;&gt;&lt;a href=&quot;#搜狐新闻数据-SogouCS&quot; class=&quot;headerlink&quot; title=&quot;搜狐新闻数据(SogouCS)&quot;&gt;&lt;/a&gt;搜狐新闻数据(SogouCS)&lt;/h2&gt;&lt;h5 id=&quot;介绍：&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="embedding" scheme="https://goingcoder.github.io/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>动态规划</title>
    <link href="https://goingcoder.github.io/2018/05/07/dp1/"/>
    <id>https://goingcoder.github.io/2018/05/07/dp1/</id>
    <published>2018-05-07T15:32:16.000Z</published>
    <updated>2018-05-13T15:07:52.961Z</updated>
    
    <content type="html"><![CDATA[<p>​    <strong>动态规划中递推式的求解方法不是动态规划的本质。</strong> 动态规划的本质，是对问题<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 动态规划是通过<strong>拆分问题，</strong>定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。 本题下的其他答案，大多都是在说递推的求解方法，但<strong>如何拆分问题</strong>，才是动态规划的核心。 而<strong>拆分问题</strong>，靠的就是<strong>状态的定义</strong>和<strong>状态转移方程的定义</strong>。 要解决这个问题，我们首先要<strong>定义这个问题</strong>和这个问题的子问题。 有人可能会问了，题目都已经在这了，我们还需定义这个问题吗？需要，原因就是问题在字面上看，不容易找不出子问题，而没有子问题，这个题目就没办法解决 。状态转移方程，就是定义了问题和子问题之间的关系。 可以看出，状态转移方程就是带有条件的递推式。 </p><h2 id="一-阶乘-Factorial"><a href="#一-阶乘-Factorial" class="headerlink" title="一.阶乘(Factorial)"></a>一.阶乘(Factorial)</h2><p>$1\times 2\times3\times\dots\times N$,整数1到$N$的连乘积。$N$阶乘$N!$</p><p>分析：$N!$源自$(N-1)!$，如此就递回分割问题了。</p><p><img src="/2018/05/07/dp1/./dp1/wps63B6.tmp.jpg" alt="img"></p><p>阵列的每一格对应每一个问题。设定第一格的答案，再以回圈依序计算其余答案。</p><p><img src="/2018/05/07/dp1/D:/hexo\source\_posts/wps3581.jpg" alt="img"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> f[N];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f[i] = f[i<span class="number">-1</span>] * i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">factorial</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> f = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=N; ++i)</span><br><span class="line">        f *= i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h3><p>总共$N$个问题，每个问题花费$O(1)$时间，总共花费$O(N)$时间</p><h3 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h3><p>求$1!$到$N!$:总共$N$个问题，用一条$N$格阵列存储全部问题的答案，空间复杂度为$O(N)$</p><p>只求$N!$:用一个变量累计成绩，空间复杂度为$O(1)$</p><p>Dynamic Programming: recurrence</p><p>Dynamic Programming = Divide and Conquer + Memoization</p><p>​    动态规划是分治法的延伸。当递推分割出来的问题，一而再、再而三出现，就运用记忆法储存这些问题的答案，避免重复求解，以空间换取时间。动态规划的过程，就是反复地读取数据、计算数据、储存数据。</p><p><img src="/2018/05/07/dp1/wpsB825.jpg" alt="img"></p><ol><li><p>把原问题递归分割成许多更小的问题。（recurrence）</p><p>子问题与原问题的求解方式皆类似。（optimal sub-structure）</p><p>子问题会一而再、再而三的出现。（overlapping sub-problems）</p></li><li><p>设计计算过程：</p><p> 确认每个问题需要哪些子问题来计算答案。（recurrence）</p><p> 确认总共有哪些问题。（state space）</p><p>把问题一一对应到表格。（lookup table）</p><p>决定问题的计算顺序。（computational sequence）</p></li></ol><p>​       确认初始值、计算范围（initial states / boundary）   </p><ol><li><p>实作，主要有两种方式：</p><p> Top-down</p><p> Bottom-up</p></li></ol><h3 id="1-recurrence"><a href="#1-recurrence" class="headerlink" title="1. recurrence"></a>1. recurrence</h3><p>​    递归分割问题时，当子问题与原问题完全相同，只有数值范围不同，我们称此现象为recurrence，再度出现、一而再出现之意。</p><p>【注： recursion 和 recurrence ，中文都翻译为“递归”，然而两者意义大不相同，读者切莫混淆】</p><p>​    此处以爬楼梯问题当做范例。先前于递归法章节，已经谈过如何求踏法，而此处要谈如何求踏法数目。</p><p><img src="/2018/05/07/dp1/wpsB76A.jpg" alt="img"></p><p>​    踏上第五阶，只能从第四阶或从第三阶踏过去，因此爬到五阶源自两个子问题：爬到四阶与爬到三阶。</p><p><img src="/2018/05/07/dp1/a1.png" alt="a"></p><p>​    爬到五阶的踏法数目，就是综合爬到四阶与爬到三阶的踏法数目。写成数学式子是$f(5)=f(4)+f(3)$，其中$f$表示爬到某阶的踏法数目。</p><p>​    依样画葫芦，得到<br>$$<br>f(4)=f(3)+f(2)\\<br>f(3)=f(2)+f(1)<br>$$<br>​    爬到两阶与爬到一阶无法再分割，没有子问题，直接得到<br>$$<br>f(2)=2\\<br>f(1)=1<br>$$<br>整理成一道简明扼要的递归公式：<br>$$<br>f(n)= \begin{cases}<br>1&amp;if   n=1 \\<br>2  &amp; if   n =2 \\<br>f(n-1)+f(n-2) &amp; if  n \ge3  and   n\le 5<br>\end{cases}<br>$$<br>爬到任何一阶的踏法数目，都可以借由这道递归公式求得。<img src="/2018/05/07/dp1/D:/hexo\source\_posts/wpsE565.tmp.png" alt="img">带入实际数值，递归计算即可。</p><p>​    为什么分割问题之后，就容易计算答案呢？因为分割问题时，同时也分类了这个问题所有可能答案。分类使得答案的规律变得单纯，于是更容易求得答案。</p><p><img src="/2018/05/07/dp1/wps2ACB.jpg" alt="img"></p><h3 id="2-、state-space"><a href="#2-、state-space" class="headerlink" title="2 、state space"></a>2 、state space</h3><p>想要计算第五阶的踏法数目。</p><p>全部的问题是“爬到一节”、“爬到二阶”、“爬到三阶”、“爬到四阶”、“爬到五阶”。</p><p><img src="/2018/05/07/dp1/wps2204.jpg" alt="img"></p><p>至于爬到零阶、爬到负一阶。爬到负二阶以及爬到六阶、爬到七阶没有必要计算。</p><h3 id="3、lookup-table"><a href="#3、lookup-table" class="headerlink" title="3、lookup table"></a>3、lookup table</h3><p>​    建立六格的阵列，存储五个问题的答案。</p><p>​    表格的第零格不使用，第一格式爬到一阶的答案，第二格是爬到二阶的答案，以此类推。</p><p><img src="/2018/05/07/dp1/wps75F.jpg" alt="img"></p><p>​    如果只计算爬完五阶，也可以建立三个变量交替使用。</p><h3 id="4、computational-sequence"><a href="#4、computational-sequence" class="headerlink" title="4、computational sequence"></a>4、computational sequence</h3><p>​    因为每个问题都依赖阶数少一阶、阶数少两阶这两个问题，所以必须由阶数小的问题开始计算。</p><p>​    计算顺序是爬到一阶、爬到二阶、……、爬到五阶。设定初始值。</p><h3 id="5、-initial-states-boundary"><a href="#5、-initial-states-boundary" class="headerlink" title="5、 initial states / boundary"></a>5、 initial states / boundary</h3><p>​    最先计算的问题时爬到一阶与爬到二阶，必须预先将答案填入表格。写入方程式，才能计算其他问题。心算求得爬到一阶的答案是1，爬到二阶的答案是2。最后计算的问题时原问题是原问题爬到五阶。</p><p>​    为了让表格更顺畅。为了让程式更漂亮，可以加入爬到零阶的答案，对应到表格的第零格。爬到零阶的答案，可以运用爬到一阶的答案与爬到两阶的答案，刻意逆推而得。</p><p><img src="/2018/05/07/dp1/wps589E.jpg" alt="img"></p><p>​    最后可以把初始值，尚待计算的部分、不需计算的部分，整理成一道递归公式：<br>$$<br>f(n)= \begin{cases}<br>0 &amp; if  n<0\\ 0="" 1="" &="" if="" \="" n="0\\" 1&if="" \\="" f(n-1)+f(n-2)="" \ge2="" and="" n\le="" 5\\="">5<br>\end{cases}<br>$$</0\\></p><h3 id="6、实现"><a href="#6、实现" class="headerlink" title="6、实现"></a>6、实现</h3><p>​    直接用递归实作，而不使用记忆体存储各个问题的答案，是最直接的方式，也是最慢的方式。时间复杂度是$O(f(n))$。问题一而再、再而三的出现，不断呼叫同样的函数求解，效率不彰。刚接触DP的新手常犯这种错误。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    正确的DP是一边计算，一边将计算出来的数值存入表格，以便不必重算。这里整理了两种实现方式，各有优缺点。</p><p>1)Top-down</p><p>2)Bottom-up</p><p><img src="/2018/05/07/dp1/wps58E0.jpg" alt="img"></p><p>1)Top-down</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 表格，储存全部问题的答案。</span></span><br><span class="line"><span class="keyword">bool</span> solve[<span class="number">6</span>];  <span class="comment">// 记录问题是否已经計算完毕</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> table[n] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 如果已經計算過，就直接讀取表格的答案。</span></span><br><span class="line">    <span class="keyword">if</span> (solve[n]) <span class="keyword">return</span> table[n];</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 如果不曾計算過，就計算一遍，儲存答案。</span></span><br><span class="line">    table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>); <span class="comment">// 將答案存入表格</span></span><br><span class="line">    solve[n] = <span class="literal">true</span>;            <span class="comment">// 已經計算完畢</span></span><br><span class="line">    <span class="keyword">return</span> table[n];</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        solve[i] = <span class="literal">false</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];   <span class="comment">// 合并solve跟table，简化代码。</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span> || n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="comment">// 用 0 代表该问题还未计算答案</span></span><br><span class="line"><span class="comment">//  if (table[n] != 0) </span></span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line"><span class="keyword">if</span> (table[n]) </span><br><span class="line"><span class="keyword">return</span> table[n];</span><br><span class="line">    <span class="keyword">return</span> table[n] = f(n<span class="number">-1</span>) + f(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stairs_climbing</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">     table[i] = <span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; (n &gt;= <span class="number">0</span> &amp;&amp; n &lt;= <span class="number">5</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"爬到"</span> &lt;&lt; n &lt;&lt; <span class="string">"阶，"</span> &lt;&lt; f(n) &lt;&lt; <span class="string">"种踏法"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    这个实现方式的好处是不必斤斤计较计算顺序，因为代码中的递归结构会迫使最小的问题先被计算。这个实现方式的另一个好处是只计算必要的问题，而不必计算所有可能的问题。</p><p>​    这个实现的坏处是代码采用递归结构，不断调用函数，执行效率差。这个实现方式的另一个坏处是无法自由地控制计算顺序，因而无法妥善运用记忆体，浪费了可回收再利用的记忆体。</p><p>2）Bottom-up</p><p>​    指定一个计算顺序，然后由最小问题开始计算。特色是代码通常只有几个递归。这个实现方式的好处与坏处与前一个方式恰好互补。</p><p>​    首先建立表格。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">5</span> + <span class="number">1</span>];</span><br></pre></td></tr></table></figure><p>心算爬到零阶的答案、爬到一阶的答案，填入报个当中，作为初始值。分别天道表格的第零格、第一格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">table[0] = 1;</span><br><span class="line">table[1] = 1;</span><br></pre></td></tr></table></figure><p>​    尚待计算的部分就是爬到两阶的答案、……、爬到五阶的答案。通常是使用递归，按照计算顺序来计算。</p><p>计算过程的实现方式，有两种迥异的风格。一种是往回取值，是常见的实现方式。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">往回取值</span><br><span class="line"><span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    table[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">        table[i] = table[i<span class="number">-1</span>] + table[i<span class="number">-2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另一种是往后补值，是罕见的实现方式。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">往后补值<span class="keyword">int</span> table[<span class="number">6</span>];</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dynamic_programming</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// [Initial]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++) table[i] = <span class="number">0</span>;</span><br><span class="line">    table[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//  table[1] = 1;   // 剛好可以被算到</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// [Compute]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=<span class="number">5</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span> &lt;= <span class="number">5</span>) table[i+<span class="number">1</span>] += table[i];</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">2</span> &lt;= <span class="number">5</span>) table[i+<span class="number">2</span>] += table[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7、总结"><a href="#7、总结" class="headerlink" title="7、总结"></a>7、总结</h3><p>​    第一，先找到原问题和其子问题之间的关系，写成递归公式。如此一来，即可利用递归公式，用子子问题的答案，求出子问题的答案；用子问题的答案，求出原问题的答案。</p><p>​    第二。确认可能出现的问题全部总共有哪些；用子问题的答案，求出原问题的答案。</p><p>​    第三。有了递归公式之后，就必须安排出一套计算顺序。大问题的答案，总是以小问题的答案来求得的，所以，小问题的答案必须是先算的，否则大问题的答案从何而来呢？一个好的安排方式，不但使代码易写，还可重复利用记忆体空间。</p><p>​    第四。记得先将最小，最先被计算的问题，心算出答案，储存如表格，内建与代码之中。一道递归公式必须拥有初始值，才有计算其他项。</p><p>​    第五。实现DP的代码时，会建立一个表格，表格存入所有大小问题的答案。安排好每个问题的答案再表格的哪个位置，这样计算时才能知道该在哪里取值。</p><p>​    切勿存取超出表格的原始，产生溢位情形，导致答案算错。计算过程当中，一旦某个问题的答案出错，就会如骨牌效应般一个影响一个，造成很难除错。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    &lt;strong&gt;动态规划中递推式的求解方法不是动态规划的本质。&lt;/strong&gt; 动态规划的本质，是对问题&lt;strong&gt;状态的定义&lt;/strong&gt;和&lt;strong&gt;状态转移方程的定义&lt;/strong&gt;。 动态规划是通过&lt;strong&gt;拆分问题，&lt;/strong
      
    
    </summary>
    
    
      <category term="dynamic programming" scheme="https://goingcoder.github.io/tags/dynamic-programming/"/>
    
  </entry>
  
  <entry>
    <title>tf8</title>
    <link href="https://goingcoder.github.io/2018/04/08/tf8/"/>
    <id>https://goingcoder.github.io/2018/04/08/tf8/</id>
    <published>2018-04-08T07:39:40.000Z</published>
    <updated>2018-04-09T07:29:54.381Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>​    某些任务需要能够更好的处理<strong>序列</strong>的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个<strong>序列</strong>。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络(Recurrent Neural Network)</strong>。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个<strong>循环神经网络</strong>。</p><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>​    RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。那么，什么是语言模型呢？我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p><p>我昨天上学迟到了，老师批评了__。</p><p>​    我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p><p>​    <strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p><p>​    <strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p><p>​    使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p><p>我 昨天 上学 迟到 了 ，老师 批评 了 。</p><p>​    如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p><p>​    现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p><p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p><h2 id="循环神经网络是啥"><a href="#循环神经网络是啥" class="headerlink" title="循环神经网络是啥"></a>循环神经网络是啥</h2><p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p><h3 id="基本循环神经网络"><a href="#基本循环神经网络" class="headerlink" title="基本循环神经网络"></a>基本循环神经网络</h3><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：</p><p><img src="/2018/04/08/tf8/image1.jpg" alt="1"></p><p>​    纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为<strong>循环神经网络</strong>实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的<strong>全连接神经网络</strong>。x是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的<strong>权重矩阵</strong>（读者可以回到第三篇文章<a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">零基础入门深度学习(3) - 神经网络和反向传播算法</a>，看看我们是怎样用矩阵来表示<strong>全连接神经网络</strong>的计算的）；o也是一个向量，它表示<strong>输出层</strong>的值；V是隐藏层到输出层的<strong>权重矩阵</strong>。那么，现在我们来看看W是什么。<strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。<strong>输出层是一个全连接层，它的每个节点都和隐藏层的每个节点相连，隐藏层是循环层。</strong></p><p>如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：</p><p><img src="/2018/04/08/tf8/image2.jpg" alt="2"></p><p>​    </p><p>现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是，$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：<br>$$<br>o_t=g(Vs_t)…..(式1)……(1)\\<br>s_t=f(Ux_t+Ws_{t-1})…..(式2)……(2)<br>$$<br>​    <strong>式1</strong>是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是隐藏层的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p><p>从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p><p>如果反复把<strong>式2</strong>带入到<strong>式1</strong>，我们将得到：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t)     ……….(3)\\<br>&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(4)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))……….(5)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))……….(6)\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+…))))……….(7)<br>\end{align}<br>$$<br>从上面可以看出，<strong>循环神经网络</strong>的输出值$o_t$，是受前面历次输入值$x_t、x_{t-1}、x_{t-2}、x_{t-3}$…影响的，这就是为什么<strong>循环神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p><h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>​    对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p><p>我的手机坏了，我打算____一部新手机。</p><p>​    可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p><p>在上一小节中的<strong>基本循环神经网络</strong>是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：</p><p><img src="/2018/04/08/tf8/image3.png" alt="3"></p><p>​    当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中$y_2$的计算。</p><p>​    从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_2$取决于$A_2$和$A’_2$。其计算方法为：<br>$$<br>\mathrm{y}_2=g(VA_2+V’A_2’)<br>$$<br>$A_2$和$A’_2$则分别计算：<br>$$<br>\begin{align}<br>A_2&amp;=f(WA_1+U\mathrm{x}_2)………(8)\\<br>A_2’&amp;=f(W’A_3’+U’\mathrm{x}_2)…….(9)\\<br>\end{align}<br>$$<br>​    现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值$s_t$与$s_t-1$有关；反向计算时，隐藏层的值$s’_t$与$s’_{t-1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出双向循环神经网络的计算方法：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V’\mathrm{s}_t’)…….(10)\\<br>\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})…….(11)\\<br>\mathrm{s}_t’&amp;=f(U’\mathrm{x}_t+W’\mathrm{s}_{t+1}’)……(12)\\<br>\end{align}<br>$$</p><p>​    从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p><h3 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h3><p>前面我们介绍的<strong>循环神经网络</strong>只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p><p><img src="/2018/04/08/tf8/image4.png" alt="4"></p><p>我们把第i个隐藏层的值表示为$s_t^{(i)}$、$s_t^{‘(i)}$，则<strong>深度循环神经网络</strong>的计算方式可以表示为：<br>$$<br>\begin{align}<br>\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V’^{(i)}\mathrm{s}_t’^{(i)})…….(13)\\<br>\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})……….(14)\\<br>\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\mathrm{s}_t’^{(i-1)}+W’^{(i)}\mathrm{s}_{t+1}’)……..(15)\\<br>…(16)\\<br>\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})……..(17)\\<br>\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\mathrm{x}_t+W’^{(1)}\mathrm{s}_{t+1}’)………(18)\\<br>\end{align}<br>$$</p><h2 id="循环神经网络的训练"><a href="#循环神经网络的训练" class="headerlink" title="循环神经网络的训练"></a>循环神经网络的训练</h2><h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p><ol><li>前向计算每个神经元的输出值；</li><li>反向计算每个神经元的<strong>误差项</strong>$\delta_j$值，它是误差函数E对神经元j的<strong>加权输入</strong>$net_j$的偏导数；</li><li>计算每个权重的梯度。</li></ol><p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p><p>循环层如下图所示：</p><p><img src="/2018/04/08/tf8/image5.png" alt="5"></p><h4 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h4><p>使用前面的<strong>式2</strong>对循环层进行前向计算：<br>$$<br>\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})<br>$$<br>注意，上面的$s_t$、$x_t$、$s_{t-1}$都是向量，用<strong>黑体字母</strong>表示；而U、V是<strong>矩阵</strong>，用大写字母表示。<strong>向量的下标</strong>表示<strong>时刻</strong>，例如，表示在t时刻向量s的值。</p><p>​    我们假设输入向量x的维度是m，输出向量s的维度是n，则矩阵U的维度是$n<em> m$，矩阵W的维度是$n</em>n$。下面是上式展开成矩阵的样子，看起来更直观一些：<br>$$<br>\begin{align}<br>\begin{bmatrix}<br>s_1^t\\<br>s_2^t\\<br>.\.\\<br>s_n^t\\<br>\end{bmatrix}=f(<br>\begin{bmatrix}<br>u_{11} u_{12} … u_{1m}\\<br>u_{21} u_{22} … u_{2m}\\<br>.\.\\<br>u_{n1} u_{n2} … u_{nm}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>.\.\\<br>x_m\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>w_{11} w_{12} … w_{1n}\\<br>w_{21} w_{22} … w_{2n}\\<br>.\.\\<br>w_{n1} w_{n2} … w_{nn}\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>s_1^{t-1}\\<br>s_2^{t-1}\\<br>.\.\\<br>s_n^{t-1}\\<br>\end{bmatrix})…..(19)<br>\end{align}<br>$$<br>​    在这里我们用<strong>手写体字母</strong>表示向量的一个<strong>元素</strong>，它的下标表示它是这个向量的第几个元素，它的上标表示第几个<strong>时刻</strong>。例如，$s_t^{j}$表示向量s的第j个元素在t时刻的值。$u_{ji}$表示<strong>输入层</strong>第i个神经元到<strong>循环层</strong>第j个神经元的权重。$w_{ji}$表示<strong>循环层</strong>第t-1时刻的第i个神经元到<strong>循环层</strong>第t个时刻的第j个神经元的权重。</p><h4 id="误差项的计算"><a href="#误差项的计算" class="headerlink" title="误差项的计算"></a>误差项的计算</h4><p>​    BTPP算法将第l层t时刻的<strong>误差项</strong>$\delta_t^l $值沿两个方向传播，一个方向是其传递到上一层网络，得到，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始时刻，得到，这部分只和权重矩阵W有关。</p><p>我们用向量表示神经元在t时刻的<strong>加权输入</strong>，因为：</p><p><img src="/2018/04/08/tf8/image6.png" alt="6"></p><p>其中，$X_t$为输入，A为模型处理部分，$h_t$为输出。</p><p>为了更容易地说明递归神经网络，我们把上图展开，得到： </p><p><img src="/2018/04/08/tf8/image7.png" alt="7"></p><p>​    这样的一条链状神经网络代表了一个递归神经网络，可以认为它是对相同神经网络的多重复制，每一时刻的神经网络会传递信息给下一时刻。如何理解它呢？假设有这样一个语言模型，我们要根据句子中已出现的词预测当前词是什么，递归神经网络的工作原理如下： </p><p><img src="/2018/04/08/tf8/image8.png" alt="8"></p><p>其中，W为各类权重，x表示输入，y表示输出，h表示隐层处理状态。</p><p>递归神经网络因为具有一定的记忆功能，可以被用来解决很多问题，例如：语音识别、语言模型、机器翻译等。但是它并不能很好地处理长时依赖问题。</p><p><strong>长时依赖问题</strong> </p><p>​    RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p><p>​    有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p><p><img src="/2018/04/08/tf8/image9.png" alt="9"></p><p>​                        不太长的相关信息和位置间隔</p><p>​    但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。</p><p>​    不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p><p><img src="/2018/04/08/tf8/image10.png" alt="10"></p><p>​    在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="https://link.jianshu.com?t=http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。</p><p>​    隐含内容和输出结果是相同的内容。</p><p>​    TensorFlow 实现 RNN Cell 的位置在 python/ops/rnn_cell_impl.py，首先其实现了一个 RNNCell 类，继承了 Layer 类，其内部有三个比较重要的方法，state_size()、output_size()、<strong>call</strong>() 方法，其中 state_size() 和 output_size() 方法设置为类属性，可以当做属性来调用，实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""size(s) of state(s) used by this cell.</span></span><br><span class="line"><span class="string">It can be represented by an Integer, a TensorShape or a tuple of Integers</span></span><br><span class="line"><span class="string">or TensorShapes.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="string">"""Integer or TensorShape: size of outputs produced by this cell."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">"Abstract method"</span>)</span><br></pre></td></tr></table></figure><p>分别代表 Cell 的状态和输出维度，和 Cell 中的神经元数量有关，但这里两个方法都没有实现，意思是说我们必须要实现一个子类继承 RNNCell 类并实现这两个方法。</p><p>另外对于 <strong>call</strong>() 方法，实际上就是当初始化的对象直接被调用的时候触发的方法，实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> scope <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(scope,</span><br><span class="line">                               custom_getter=self._rnn_get_variable) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state, scope=scope)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> vs.variable_scope(vs.get_variable_scope(),</span><br><span class="line">                               custom_getter=self._rnn_get_variable):</span><br><span class="line">            <span class="keyword">return</span> super(RNNCell, self).__call__(inputs, state)</span><br></pre></td></tr></table></figure><p>​    实际上是调用了父类 Layer 的 <strong>call</strong>() 方法，但父类中 <strong>call</strong>() 方法中又调用了 call() 方法，而 Layer 类的 call() 方法的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure><p>父类的 call() 方法实现非常简单，所以要实现其真正的功能，只需要在继承 RNNCell 类的子类中实现 call() 方法即可。</p><p>接下来我们看下 RNN Cell 的最基本的实现，叫做 BasicRNNCell，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicRNNCell</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">  <span class="string">"""The most basic RNN cell.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    num_units: int, The number of units in the RNN cell.</span></span><br><span class="line"><span class="string">    activation: Nonlinearity to use.  Default: `tanh`.</span></span><br><span class="line"><span class="string">    reuse: (optional) Python boolean describing whether to reuse variables</span></span><br><span class="line"><span class="string">     in an existing scope.  If not `True`, and the existing scope already has</span></span><br><span class="line"><span class="string">     the given variables, an error is raised.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, activation=None, reuse=None)</span>:</span></span><br><span class="line">    super(BasicRNNCell, self).__init__(_reuse=reuse)</span><br><span class="line">    self._num_units = num_units</span><br><span class="line">    self._activation = activation <span class="keyword">or</span> math_ops.tanh</span><br><span class="line">    self._linear = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">    <span class="string">"""Most basic RNN: output = new_state = act(W * input + U * state + B)."""</span></span><br><span class="line">    <span class="keyword">if</span> self._linear <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">      self._linear = _Linear([inputs, state], self._num_units, <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    output = self._activation(self._linear([inputs, state]))</span><br><span class="line">    <span class="keyword">return</span> output, output</span><br></pre></td></tr></table></figure><p>可以看到在初始化的时候，最重要的一个参数是 num_units，意思就是这个 Cell 中神经元的个数，另外还有一个参数 activation 即默认使用的激活函数，默认使用的 tanh，reuse 代表该 Cell 是否可以被重新使用。</p><p>在 state_size()、output_size() 方法里，其返回的内容都是 num_units，即神经元的个数，接下来 call() 方法中，传入的参数为 inputs 和 state，即输入的 x 和 上一次的隐含状态，首先实例化了一个 _Linear 类，这个类实际上就是做线性变换的类，将二者传递过来，然后直接调用，就实现了 w * [inputs, state] + b 的线性变换，其中 _Linear 类的 <strong>call</strong>() 方法实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, args)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._is_sequence:</span><br><span class="line">        args = [args]</span><br><span class="line">    <span class="keyword">if</span> len(args) == <span class="number">1</span>:</span><br><span class="line">        res = math_ops.matmul(args[<span class="number">0</span>], self._weights)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res = math_ops.matmul(array_ops.concat(args, <span class="number">1</span>), self._weights)</span><br><span class="line">    <span class="keyword">if</span> self._build_bias:</span><br><span class="line">        res = nn_ops.bias_add(res, self._biases)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>​    很明显这里传递了 [inputs, state] 作为 <strong>call</strong>() 方法的 args，会执行 concat() 和 matmul() 方法，然后接着再执行 bias_add() 方法，这样就实现了线性变换。</p><p>​    最后回到 BasicRNNCell 的 call() 方法中，在 _linear() 方法外面又包括了一层 _activation() 方法，即对线性变换应用一次 tanh 激活函数处理，作为输出结果。</p><p>​    最后返回的结果是 output 和 output，第一个代表 output，第二个代表隐状态，其值也等于 output。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环神经网络&quot;&gt;&lt;a href=&quot;#循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络&quot;&gt;&lt;/a&gt;循环神经网络&lt;/h1&gt;&lt;p&gt;​    某些任务需要能够更好的处理&lt;strong&gt;序列&lt;/strong&gt;的信息，即前面的输入和后面的输入是
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tf7</title>
    <link href="https://goingcoder.github.io/2018/04/07/tf7/"/>
    <id>https://goingcoder.github.io/2018/04/07/tf7/</id>
    <published>2018-04-07T13:58:04.000Z</published>
    <updated>2018-04-07T14:11:19.896Z</updated>
    
    <content type="html"><![CDATA[<p>过拟合</p><p>解决办法：</p><p>​    方法一: 增加数据量, 大部分过拟合产生的原因是因为数据量太少了. 如果我们有成千上万的数据, 红线也会慢慢被拉直, 变得没那么扭曲 . </p><p>​    方法二:运用正规化. L1, l2 regularization等等, 这些方法适用于大多数的机器学习, 包括神经网络. 他们的做法大同小异, 我们简化机器学习的关键公式为 y=Wx 。 W为机器需要学习到的各种参数。在过拟合中, W 的值往往变化得特别大或特别小。为了不让W变化太大, 我们在计算误差上做些手脚. 原始的 cost 误差是这样计算, cost = 预测值-真实值的平方. 如果 W 变得太大, 我们就让 cost 也跟着变大, 变成一种惩罚机制. 所以我们把 W 自己考虑进来. 这里 abs 是绝对值. 这一种形式的<strong>正规化</strong>, 叫做 l1 正规化。L2 正规化和 l1 类似, 只是绝对值换成了平方. 其他的l3, l4 也都是换成了立方和4次方等等. 形式类似. 用这些方法,我们就能保证让学出来的线条不会过于扭曲.<br>$$<br>l1,l2…regularization\\<br>y=Wx\\<br>L1:cost=(Wx-real \, y)^2+abs(W)\\<br>L2:cost=(Wx-real \, y)^2+(W)^2<br>$$<br>​    还有一种专门用在神经网络的正规化的方法, 叫作 dropout。在训练的时候, 我们随机忽略掉一些神经元和神经联结 , 是这个神经网络变得”不完整”. 用一个不完整的神经网络训练一次。到第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象其实每次训练的时候, 我们都让每一次预测结果都不会依赖于其中某部分特定的神经元. 像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数。Dropout 的做法是从根本上让神经网络没机会过度依赖。</p><p>​    举个Regression (回归)的例子。</p><p><img src="/2018/04/07/tf7/image1.png" alt="1"></p><p>​    第三条曲线存在overfitting问题，尽管它经过了所有的训练点，但是不能很好的反应数据的趋势，预测能力严重不足。 TensorFlow提供了强大的dropout方法来解决overfitting问题。</p><h2 id="建立-dropout-层"><a href="#建立-dropout-层" class="headerlink" title="建立 dropout 层"></a>建立 dropout 层</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过拟合&lt;/p&gt;
&lt;p&gt;解决办法：&lt;/p&gt;
&lt;p&gt;​    方法一: 增加数据量, 大部分过拟合产生的原因是因为数据量太少了. 如果我们有成千上万的数据, 红线也会慢慢被拉直, 变得没那么扭曲 . &lt;/p&gt;
&lt;p&gt;​    方法二:运用正规化. L1, l2 regulari
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorboard可视化</title>
    <link href="https://goingcoder.github.io/2018/04/06/tf6/"/>
    <id>https://goingcoder.github.io/2018/04/06/tf6/</id>
    <published>2018-04-06T13:05:29.000Z</published>
    <updated>2018-04-06T13:56:38.481Z</updated>
    
    <content type="html"><![CDATA[<p>​    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了更方便 TensorFlow 程序的理解、调试与优化，有一套叫做 TensorBoard 的可视化工具。你可以用 TensorBoard 来展现你的 TensorFlow 图像，绘制图像生成的定量指标图以及附加数据。TensorBoard 通过读取 TensorFlow 的事件文件来运行。TensorFlow 的事件文件包括了你会在 TensorFlow 运行中涉及到的主要数据。。显示的神经网络差不多是这样的：</p><a id="more"></a><p><img src="/2018/04/06/tf6/image1.PNG" alt="1"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时我们也可以展开看每个layer中的一些具体的结构：</p><p><img src="/2018/04/06/tf6/image2.PNG" alt="1"></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化器optimizer</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer'</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">            Weights = tf.Variable(tf.random_normal([insize,outsize]), name=<span class="string">'W'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">            biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>, name=<span class="string">'b'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">            Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line">y = np.random.random(y_data.shape)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</span><br><span class="line">    xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">'x_input'</span>)</span><br><span class="line">    ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">'y_input'</span>)</span><br><span class="line"><span class="comment"># add hidden layer</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line"><span class="comment"># add output layer</span></span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 这里对矩阵按行求和没有什么作用，因为数据每行只有一个实例，还是它本身</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                          reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"logs/"</span>, sess.graph)</span><br><span class="line">    sess.run(init)</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;执行完成会在logs文件夹下生成一个事件文件。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;打开终端，切换到py文件的目录，输入：tensorboard –logdir=./logs</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意：</strong>文件的路径一定要加“.” ,否则会找不到那个事件文件</p><p>在浏览器打开终端显示的链接：<a href="http://DESKTOP-RKATBUI:6006" target="_blank" rel="noopener">http://DESKTOP-RKATBUI:6006</a></p><p>即可看到tensorboard的界面,如下。</p><p><img src="/2018/04/06/tf6/image3.PNG" alt="3"></p><p>参考：</p><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'tensorboard可视化',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;为了更方便 TensorFlow 程序的理解、调试与优化，有一套叫做 TensorBoard 的可视化工具。你可以用 TensorBoard 来展现你的 TensorFlow 图像，绘制图像生成的定量指标图以及附加数据。TensorBoard 通过读取 TensorFlow 的事件文件来运行。TensorFlow 的事件文件包括了你会在 TensorFlow 运行中涉及到的主要数据。。显示的神经网络差不多是这样的：&lt;/p&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow增加层和数据可视化</title>
    <link href="https://goingcoder.github.io/2018/04/06/tf5/"/>
    <id>https://goingcoder.github.io/2018/04/06/tf5/</id>
    <published>2018-04-06T11:12:20.000Z</published>
    <updated>2018-04-06T13:04:03.324Z</updated>
    
    <content type="html"><![CDATA[<p>添加层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span></span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([insize,outsize]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><a id="more"></a><p>完整的一个小例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([insize,outsize]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line">y = np.random.random(y_data.shape)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 这里对矩阵按行求和没有什么作用，因为数据每行只有一个实例，还是它本身</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                      reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br></pre></td></tr></table></figure><p>数据可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,insize,outsize,activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([insize,outsize]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>,outsize]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs,Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line">y = np.random.random(y_data.shape)</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># 这里对矩阵按行求和没有什么作用，因为数据每行只有一个实例，还是它本身</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                      reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ax.scatter(x_data, y_data)</span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                ax.lines.remove(lines[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)</span><br><span class="line">            lines = ax.plot(x_data, prediction_value, <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">            <span class="comment"># ax.lines.remove(lines[0])</span></span><br><span class="line">            plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>参考：<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'tensorflow增加层和数据可视化',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;添加层&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;add_layer&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(inputs,insize,outsize,activation_function=None)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Weights = tf.Variable(tf.random_normal([insize,outsize]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    biases = tf.Variable(tf.zeros([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,outsize]) + &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Wx_plus_b = tf.matmul(inputs,Weights) + biases&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; activation_function &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        outputs = Wx_plus_b&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        outputs = activation_function(Wx_plus_b)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; outputs&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tf.placeholder 与 tf.Variable区别</title>
    <link href="https://goingcoder.github.io/2018/04/06/tf4/"/>
    <id>https://goingcoder.github.io/2018/04/06/tf4/</id>
    <published>2018-04-06T10:37:35.000Z</published>
    <updated>2018-04-06T11:02:33.442Z</updated>
    
    <content type="html"><![CDATA[<p>二者的主要区别在于：</p><p><strong>tf.Variable</strong>：主要在于一些可训练变量（trainable variables），比如模型的权重（weights，W）或者偏置值（bias）；</p><p><strong>声明时，必须提供初始值；<em>**</em></strong></p><p>名称的真实含义，在于变量，也即在真实训练时，其值是会改变的，自然事先需要指定初始值； </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable( tf.truncated_normal([IMAGE_PIXELS,                                hidden1_units],stddev=<span class="number">1.</span>/math.sqrt(float(IMAGE_PIXELS)), name=<span class="string">'weights'</span>))</span><br><span class="line">biases = tf.Variable(tf.zeros([hidden1_units]), name=<span class="string">'biases'</span>)</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>tf.placeholder</strong>：用于得到传递进来的真实的训练样本：</p><p><strong>不必指定初始值</strong>，可在运行时，通过 Session.run 的函数的 feed_dict 参数指定；这也是其命名的原因所在，仅仅作为一种占位符；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">images_placeholder = tf.placeholder(tf.float32, shape=[batch_size, IMAGE_PIXELS])</span><br><span class="line">labels_placeholder = tf.placeholder(tf.int32, shape=[batch_size])</span><br></pre></td></tr></table></figure><p>如下则是二者真实的使用场景：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(FLAGS.max_steps):</span><br><span class="line">    feed_dict = &#123;</span><br><span class="line">        images_placeholder = images_feed,</span><br><span class="line">        labels_placeholder = labels_feed</span><br><span class="line">    &#125;</span><br><span class="line">    _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)</span><br></pre></td></tr></table></figure><p>当执行这些操作时，tf.Variable 的值将会改变，也即被修改，这也是其名称的来源（variable，变量）。</p><p>那么，什么时候该用tf.placeholder，什么时候该使用tf.Variable之类直接定义参数呢？</p><p>答案是，<strong>tf.Variable适合一些需要初始化或被训练而变化的权重或参数，而tf.placeholder适合通常不会改变的被训练的数据集。</strong></p><p>小例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">output = tf.multiply(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(output,  feed_dict=&#123;input1: <span class="number">7.</span>, input2: <span class="number">2.</span>&#125;))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">14.0</span></span><br></pre></td></tr></table></figure><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'tf.placeholder 与 tf.Variable区别',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;二者的主要区别在于：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tf.Variable&lt;/strong&gt;：主要在于一些可训练变量（trainable variables），比如模型的权重（weights，W）或者偏置值（bias）；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;声明时，必须提供初始值；&lt;em&gt;**&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;名称的真实含义，在于变量，也即在真实训练时，其值是会改变的，自然事先需要指定初始值； &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;weights = tf.Variable( tf.truncated_normal([IMAGE_PIXELS, 		                               hidden1_units],stddev=&lt;span class=&quot;number&quot;&gt;1.&lt;/span&gt;/math.sqrt(float(IMAGE_PIXELS)), name=&lt;span class=&quot;string&quot;&gt;&#39;weights&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;biases = tf.Variable(tf.zeros([hidden1_units]), name=&lt;span class=&quot;string&quot;&gt;&#39;biases&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-变量</title>
    <link href="https://goingcoder.github.io/2018/04/04/tf3/"/>
    <id>https://goingcoder.github.io/2018/04/04/tf3/</id>
    <published>2018-04-04T09:44:13.000Z</published>
    <updated>2018-04-04T10:11:27.738Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;训练模型时，需要使用<strong>变量(Variables)</strong>保存和更新参数。Variables是包含张量(tensor)的内存缓冲。变量必须要先被<strong>初始化(initialize)</strong>，而且可以在训练时和训练后<strong>保存(save)</strong>到磁盘中。之后可以再<strong>恢复(restore)</strong>保存的变量值来训练和测试模型。 </p><h1 id="1-创建-Creation"><a href="#1-创建-Creation" class="headerlink" title="1.创建(Creation)"></a>1.创建(Creation)</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建Variable，需将一个tensor传递给Variable()构造函数。可以使用TensorFlow提供的许多ops(操作)初始化张量，参考<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html" target="_blank" rel="noopener">constants or random values</a>。这些ops都要求指定tensor的<strong>shape(形状)</strong>。比如</p><blockquote><h4 id="Create-two-variables"><a href="#Create-two-variables" class="headerlink" title="Create two variables."></a>Create two variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), </span><br><span class="line">&gt; name=”weights”) </span><br><span class="line">&gt; biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=”biases”)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote></blockquote><p>&nbsp;<a id="more"></a></p><p>&nbsp;调用tf.Variable()函数在graph中增加以下几个ops:<br>- 一个<code>Variable</code> op ,负责保存变量值。<br>- 一个<code>initializer</code> op,负责将变量设为初始值，这实际是<code>tf.assign</code> op。<br>- 初始值的op，比如<code>zeros</code> op 。</p><p><code>tf.Variable()</code>返回一个<code>tf.Variable</code>类的实例。</p><p>使用tf.Variable()时，如果系统检测到重名，会做自动处理，不会报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w_1 = tf.Variable(<span class="number">3</span>,name=<span class="string">"w"</span>)</span><br><span class="line">w_2 = tf.Variable(<span class="number">1</span>,name=<span class="string">"w"</span>)</span><br><span class="line">print(w_1.name)</span><br><span class="line">print(w_2.name)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w:<span class="number">0</span></span><br><span class="line">w_1:<span class="number">0</span></span><br></pre></td></tr></table></figure><h1 id="2-设备安置-Device-placement"><a href="#2-设备安置-Device-placement" class="headerlink" title="2.设备安置(Device placement)"></a>2.设备安置(Device placement)</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 with tf.device(…): block，将一个变量安置在一个设备上。</p><p>Pin a variable to CPU.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(“/cpu:<span class="number">0</span>”): </span><br><span class="line">v = tf.Variable(…)</span><br></pre></td></tr></table></figure><p>Pin a variable to GPU.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(“/gpu:<span class="number">0</span>”): </span><br><span class="line">v = tf.Variable(…)</span><br></pre></td></tr></table></figure><p>Pin a variable to a particular parameter server task.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(“/job:ps/task:<span class="number">7</span>”): </span><br><span class="line">v = tf.Variable(…)</span><br></pre></td></tr></table></figure><p>改变变量的一些ops,比如<code>v.assign()</code>和<code>tf.train.Optimizer</code>需要与变量在同一个设备上。</p><h1 id="3-初始化-Initialization"><a href="#3-初始化-Initialization" class="headerlink" title="3.初始化(Initialization)"></a>3.初始化(Initialization)</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在运行模型中其他操作之前，必须先对变量进行初始化。最简单的初始化方法是添加一个对所有变量进行初始化的op,然后再使用model前运行此op。</p><h2 id="3-1全局初始化"><a href="#3-1全局初始化" class="headerlink" title="3.1全局初始化"></a>3.1全局初始化</h2><p>使用<code>tf.global_variables_initializer()</code>添加一个op来运行初始化。要在完全构建完模型后，在一个对话(Session)中运行它。</p><blockquote><h4 id="Create-two-variables-1"><a href="#Create-two-variables-1" class="headerlink" title="Create two variables."></a>Create two variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=”weights”) </span><br><span class="line">&gt; biases = tf.Variable(tf.zeros([<span class="number">200</span>]), name=”biases”) </span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Add-an-op-to-initialize-the-variables"><a href="#Add-an-op-to-initialize-the-variables" class="headerlink" title="Add an op to initialize the variables."></a>Add an op to initialize the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; init_op = tf.global_variables_initializer()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Later-when-launching-the-model"><a href="#Later-when-launching-the-model" class="headerlink" title="Later, when launching the model"></a>Later, when launching the model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">&gt; <span class="comment"># Run the init operation. </span></span><br><span class="line">&gt; sess.run(init_op) </span><br><span class="line">&gt; … </span><br><span class="line">&gt; <span class="comment"># Use the model </span></span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote></blockquote><h2 id="3-2-用其他变量值创建变量"><a href="#3-2-用其他变量值创建变量" class="headerlink" title="3.2 用其他变量值创建变量"></a>3.2 用其他变量值创建变量</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用变量A的值初始化另一个变量B，需使用变量A的属性(property)<code>initialized_value()</code>。可以直接使用变量A的初始值，也可以用之计算新的值。</p><p>Create a variable with a random value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">200</span>], stddev=<span class="number">0.35</span>), name=”weights”)</span><br></pre></td></tr></table></figure><p>Create another variable with the same value as ‘weights’.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w2 = tf.Variable(weights.initialized_value(), name=”w2”)</span><br></pre></td></tr></table></figure><p>Create another variable with twice the value of ‘weights’</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_twice = tf.Variable(weights.initialized_value() * <span class="number">2.0</span>, name=”w_twice”)</span><br></pre></td></tr></table></figure><h1 id="4-保存和恢复Saving-and-Restoring"><a href="#4-保存和恢复Saving-and-Restoring" class="headerlink" title="4.保存和恢复Saving and Restoring"></a>4.保存和恢复Saving and Restoring</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最简单的方法是用<code>tf.train.Saver</code>对象，此构造函数在graph中为所有变量(or a specified list)添加save和restore ops。saver对象提供运行这些ops的方法，并指定读写checkpoint files的路径。</p><h2 id="4-1-checkpoint文件"><a href="#4-1-checkpoint文件" class="headerlink" title="4.1 checkpoint文件"></a>4.1 checkpoint文件</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变量存储在一个<strong>二进制</strong>文件中，包含从变量名称到张量值的映射。</p><p>创建checkpoint files时，可以选择性地选择变量名称来保存。默认情况，它使用每个Variable的<code>Variable.name</code>属性。</p><p>可以使用<a href="https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/python/tools/inspect_checkpoint.py" target="_blank" rel="noopener">inspect_checkpoint</a>库查看checkpoint file中的变量，还有<code>print_tensrs_in_checkpoint_file</code>函数。</p><h1 id="4-2保存变量"><a href="#4-2保存变量" class="headerlink" title="4.2保存变量"></a>4.2保存变量</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用<code>tf.train.Saver()</code>创建一个<code>Saver</code>对象来管理模型中所有变量。</p><blockquote><h4 id="Create-some-variables"><a href="#Create-some-variables" class="headerlink" title="Create some variables."></a>Create some variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; v1 = tf.Variable(…, name=”v1”) </span><br><span class="line">&gt; v2 = tf.Variable(…, name=”v2”) </span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Add-an-op-to-initialize-the-variables-1"><a href="#Add-an-op-to-initialize-the-variables-1" class="headerlink" title="Add an op to initialize the variables."></a>Add an op to initialize the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; init_op = tf.global_variables_initializer()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Add-ops-to-save-and-restore-all-the-variables"><a href="#Add-ops-to-save-and-restore-all-the-variables" class="headerlink" title="Add ops to save and restore all the variables."></a>Add ops to save and restore all the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver = tf.train.Saver()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Later-launch-the-model-initialize-the-variables-do-some-work-save-the-variables-to-disk"><a href="#Later-launch-the-model-initialize-the-variables-do-some-work-save-the-variables-to-disk" class="headerlink" title="Later, launch the model, initialize the variables, do some work, save the variables to disk."></a>Later, launch the model, initialize the variables, do some work, save the variables to disk.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">&gt; sess.run(init_op)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Do-some-work-with-the-model"><a href="#Do-some-work-with-the-model" class="headerlink" title="Do some work with the model."></a>Do some work with the model.</h4><p>..</p><h4 id="Save-the-variables-to-disk"><a href="#Save-the-variables-to-disk" class="headerlink" title="Save the variables to disk."></a>Save the variables to disk.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; save_path = saver.save(sess, “/tmp/model.ckpt”) </span><br><span class="line">&gt; print(“Model saved <span class="keyword">in</span> file: %s” % save_path)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote></blockquote><p>先初始化变量，再操作模型，最后保存变量。</p><h2 id="4-3恢复变量"><a href="#4-3恢复变量" class="headerlink" title="4.3恢复变量"></a>4.3恢复变量</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用同样的Saver对象恢复变量，恢复变量时，就不用先初始化变量了。</p><blockquote><h4 id="Create-some-variables-1"><a href="#Create-some-variables-1" class="headerlink" title="Create some variables."></a>Create some variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; v1 = tf.Variable(…, name=”v1”) </span><br><span class="line">&gt; v2 = tf.Variable(…, name=”v2”) </span><br><span class="line">&gt; …</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Add-ops-to-save-and-restore-all-the-variables-1"><a href="#Add-ops-to-save-and-restore-all-the-variables-1" class="headerlink" title="Add ops to save and restore all the variables."></a>Add ops to save and restore all the variables.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver = tf.train.Saver()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Later-launch-the-model-use-the-saver-to-restore-variables-from-disk-and"><a href="#Later-launch-the-model-use-the-saver-to-restore-variables-from-disk-and" class="headerlink" title="Later, launch the model, use the saver to restore variables from disk, and"></a>Later, launch the model, use the saver to restore variables from disk, and</h4><h4 id="do-some-work-with-the-model"><a href="#do-some-work-with-the-model" class="headerlink" title="do some work with the model."></a>do some work with the model.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Restore-variables-from-disk"><a href="#Restore-variables-from-disk" class="headerlink" title="Restore variables from disk."></a>Restore variables from disk.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver.restore(sess, “/tmp/model.ckpt”) </span><br><span class="line">&gt; print(“Model restored.”)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Do-some-work-with-the-model-1"><a href="#Do-some-work-with-the-model-1" class="headerlink" title="Do some work with the model"></a>Do some work with the model</h4><p>…</p></blockquote><p>无初始化操作，先恢复变量，再操模型。</p><h1 id="4-4选择保存和恢复的变量"><a href="#4-4选择保存和恢复的变量" class="headerlink" title="4.4选择保存和恢复的变量"></a>4.4选择保存和恢复的变量</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果不给<code>tf.train.Saver</code>传递任何参数，Saver会在graph中处理所有变量。每个变量会存在他们创建时的name下。</p><p>通过给<code>tf.train.Saver</code>传递一个Python字典，可以指定保存变量的name。key是要在checkpoint file中使用的name, values指要管理的变量。</p><h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h3><ul><li>可以创建多个saver，分别保存不同的变量集合。</li><li>如果在对话开始时，只恢复了部分变量，就要对其他变量运行initializer op。参考<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#variables_initializer" target="_blank" rel="noopener">tf.variables_initializer() </a></li></ul><blockquote><h4 id="Create-some-variables-2"><a href="#Create-some-variables-2" class="headerlink" title="Create some variables."></a>Create some variables.</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; v1 = tf.Variable(…, name=”v1”) </span><br><span class="line">&gt; v2 = tf.Variable(…, name=”v2”) </span><br><span class="line">&gt; … </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Add-ops-to-save-and-restore-only-‘v2’-using-the-name-“my-v2”"><a href="#Add-ops-to-save-and-restore-only-‘v2’-using-the-name-“my-v2”" class="headerlink" title="Add ops to save and restore only ‘v2’ using the name “my_v2”"></a>Add ops to save and restore only ‘v2’ using the name “my_v2”</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; saver = tf.train.Saver(&#123;“my_v2”: v2&#125;) </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><h4 id="Use-the-saver-object-normally-after-that"><a href="#Use-the-saver-object-normally-after-that" class="headerlink" title="Use the saver object normally after that."></a>Use the saver object normally after that.</h4></blockquote><p>一个小例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line">print(state.name)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer() <span class="comment"># must have if define variable</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="https://blog.csdn.net/muyiyushan/article/details/65442052" target="_blank" rel="noopener">https://blog.csdn.net/muyiyushan/article/details/65442052</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'tensorflow-变量',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;训练模型时，需要使用&lt;strong&gt;变量(Variables)&lt;/strong&gt;保存和更新参数。Variables是包含张量(tensor)的内存缓冲。变量必须要先被&lt;strong&gt;初始化(initialize)&lt;/strong&gt;，而且可以在训练时和训练后&lt;strong&gt;保存(save)&lt;/strong&gt;到磁盘中。之后可以再&lt;strong&gt;恢复(restore)&lt;/strong&gt;保存的变量值来训练和测试模型。 &lt;/p&gt;
&lt;h1 id=&quot;1-创建-Creation&quot;&gt;&lt;a href=&quot;#1-创建-Creation&quot; class=&quot;headerlink&quot; title=&quot;1.创建(Creation)&quot;&gt;&lt;/a&gt;1.创建(Creation)&lt;/h1&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;创建Variable，需将一个tensor传递给Variable()构造函数。可以使用TensorFlow提供的许多ops(操作)初始化张量，参考&lt;a href=&quot;https://www.tensorflow.org/versions/r0.12/api_docs/python/constant_op.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;constants or random values&lt;/a&gt;。这些ops都要求指定tensor的&lt;strong&gt;shape(形状)&lt;/strong&gt;。比如&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4 id=&quot;Create-two-variables&quot;&gt;&lt;a href=&quot;#Create-two-variables&quot; class=&quot;headerlink&quot; title=&quot;Create two variables.&quot;&gt;&lt;/a&gt;Create two variables.&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; weights = tf.Variable(tf.random_normal([&lt;span class=&quot;number&quot;&gt;784&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;200&lt;/span&gt;], stddev=&lt;span class=&quot;number&quot;&gt;0.35&lt;/span&gt;), &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; name=”weights”) &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt; biases = tf.Variable(tf.zeros([&lt;span class=&quot;number&quot;&gt;200&lt;/span&gt;]), name=”biases”)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-会话（session）</title>
    <link href="https://goingcoder.github.io/2018/04/04/tf2/"/>
    <id>https://goingcoder.github.io/2018/04/04/tf2/</id>
    <published>2018-04-04T08:21:05.000Z</published>
    <updated>2018-04-04T09:42:00.218Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tensorflow中的会话是来执行定义好的运算的。会话拥有并管理Tensorflow程序运行时的所有资源。当计算完成之后需要关闭会话来帮助系统回收资源，否则可能出现资源泄露的问题。 Tensorflow中使用会话的模式一般有两种，第一种模式需要明确调用会话生成函数和关闭会话函数，流程如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">matrix1 =  tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># 输出：[[12]]</span></span><br></pre></td></tr></table></figure><p>​    <a id="more"></a></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用这种模式时，所有计算完成后，需要明确调用Session.close()函数关闭会话并释放资源。然而当程序因为异常而退出时，关闭会话的函数可能就不会被执行而导致资源泄露。为了解决异常退出导致资源泄露的问题，Tensorflow可以通过Python的上下文管理器来使用会话。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">matrix1 =  tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method2,sess会自动关闭</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result2 = sess.run(product)</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过Python的上下文管理器的机制，只要所有的计算放在with的内部就可以。当上下文管理器退出时会自动释放所有资源。</p><h2 id="上下文管理器"><a href="#上下文管理器" class="headerlink" title="上下文管理器"></a>上下文管理器</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在使用Python编程中，可以会经常碰到这种情况：有一个特殊的语句块，在执行这个语句块之前需要先执行一些准备动作；当语句块执行完成后，需要继续执行一些收尾动作。</p><p>例如：当需要操作文件或数据库的时候，首先需要获取文件句柄或者数据库连接对象，当执行完相应的操作后，需要执行释放文件句柄或者关闭数据库连接的动作。</p><p>又如，当多线程程序需要访问临界资源的时候，线程首先需要获取互斥锁，当执行完成并准备退出临界区的时候，需要释放互斥锁。</p><p>​    对于这些情况，Python中提供了<strong>上下文管理器（Context Manager）</strong>的概念，可以通过上下文管理器来定义/控制代码块执行前的准备动作，以及执行后的收尾动作。</p><h2 id="上下文管理协议"><a href="#上下文管理协议" class="headerlink" title="上下文管理协议"></a>上下文管理协议</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么在Python中怎么实现一个上下文管理器呢？这里，又要提到两个”魔术方法”__enter__和__exit__，下面就是关于这两个方法的具体介绍。</p><ul><li><p><strong>enter</strong>(self) Defines what the context manager should do at the beginning of the block created by the with statement. Note that the return value of <strong>enter</strong> is bound to the target of the with statement, or the name after the as.</p></li><li><p><strong>exit</strong>(self, exception_type, exception_value, traceback) Defines what the context manager should do after its block has been executed (or terminates). It can be used to handle exceptions, perform cleanup, or do something always done immediately after the action in the block. If the block executes successfully, exception_type, exception_value, and traceback will be None. Otherwise, you can choose to handle the exception or let the user handle it; if you want to handle it, make sure <strong>exit</strong> returns True after all is said and done. If you don’t want the exception to be handled by the context manager, just let it happen.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;也就是说，当我们需要创建一个上下文管理器类型的时候，就需要实现<strong>enter</strong>和<strong>exit</strong>方法，这对方法就称为<strong>上下文管理协议（Context Manager Protocol）</strong>，定义了一种运行时上下文环境。</p></li></ul><h2 id="with语句"><a href="#with语句" class="headerlink" title="with语句"></a>with语句</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Python中，可以通过with语句来方便的使用上下文管理器，with语句可以在代码块运行前进入一个运行时上下文（执行__enter__方法），并在代码块结束后退出该上下文（执行__exit__方法）。</p><p>with语句的语法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> context_expr [<span class="keyword">as</span> var]:</span><br><span class="line">    with_suite</span><br></pre></td></tr></table></figure><ul><li>context_expr是支持上下文管理协议的对象，也就是上下文管理器对象，负责维护上下文环境</li><li>as var是一个可选部分，通过变量方式保存上下文管理器对象</li><li>with_suite就是需要放在上下文环境中执行的语句块</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Python的内置类型中，很多类型都是支持上下文管理协议的，例如file，thread.LockType，threading.Lock等等。这里我们就以file类型为例，看看with语句的使用。</p><h3 id="with语句简化文件操作"><a href="#with语句简化文件操作" class="headerlink" title="with语句简化文件操作"></a>with语句简化文件操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要写一个文件的时候，一般都会通过下面的方式。代码中使用了try-finally语句块，即使出现异常，也能保证关闭文件句柄。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">logger = open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    logger.write(<span class="string">'Hello '</span>)</span><br><span class="line">    logger.write(<span class="string">'World'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    logger.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> logger.closed</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实，Python的内置file类型是支持上下文管理协议的，可以直接通过内建函数dir()来查看file支持的方法和属性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> dir(file)</span><br><span class="line">[<span class="string">'__class__'</span>, <span class="string">'__delattr__'</span>, <span class="string">'__doc__'</span>, <span class="string">'__enter__'</span>, <span class="string">'__exit__'</span>, <span class="string">'__format__'</span>, <span class="string">'</span></span><br><span class="line"><span class="string">__getattribute__'</span>, <span class="string">'__hash__'</span>, <span class="string">'__init__'</span>, <span class="string">'__iter__'</span>, <span class="string">'__new__'</span>, <span class="string">'__reduce__'</span>,</span><br><span class="line"><span class="string">'__reduce_ex__'</span>, <span class="string">'__repr__'</span>, <span class="string">'__setattr__'</span>, <span class="string">'__sizeof__'</span>, <span class="string">'__str__'</span>, <span class="string">'__subclass</span></span><br><span class="line"><span class="string">hook__'</span>, <span class="string">'close'</span>, <span class="string">'closed'</span>, <span class="string">'encoding'</span>, <span class="string">'errors'</span>, <span class="string">'fileno'</span>, <span class="string">'flush'</span>, <span class="string">'isatty'</span>, <span class="string">'</span></span><br><span class="line"><span class="string">mode'</span>, <span class="string">'name'</span>, <span class="string">'newlines'</span>, <span class="string">'next'</span>, <span class="string">'read'</span>, <span class="string">'readinto'</span>, <span class="string">'readline'</span>, <span class="string">'readlines'</span>,</span><br><span class="line"><span class="string">'seek'</span>, <span class="string">'softspace'</span>, <span class="string">'tell'</span>, <span class="string">'truncate'</span>, <span class="string">'write'</span>, <span class="string">'writelines'</span>, <span class="string">'xreadlines'</span>]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，可以通过with语句来简化上面的代码，代码的效果是一样的，但是使用with语句的代码更加的简洁：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> logger:</span><br><span class="line">    logger.write(<span class="string">'Hello '</span>)</span><br><span class="line">    logger.write(<span class="string">'World'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">print</span> logger.closed</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="https://www.cnblogs.com/wilber2013/p/4638967.html" target="_blank" rel="noopener">https://www.cnblogs.com/wilber2013/p/4638967.html</a></p><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'tensorflow-会话（session）',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Tensorflow中的会话是来执行定义好的运算的。会话拥有并管理Tensorflow程序运行时的所有资源。当计算完成之后需要关闭会话来帮助系统回收资源，否则可能出现资源泄露的问题。 Tensorflow中使用会话的模式一般有两种，第一种模式需要明确调用会话生成函数和关闭会话函数，流程如下：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;matrix1 =  tf.constant([[&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;matrix2 = tf.constant([[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                       [&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;product = tf.matmul(matrix1, matrix2)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# method 1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sess = tf.Session()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;result = sess.run(product)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(result)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sess.close()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输出：[[12]]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;​&lt;/p&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tf1</title>
    <link href="https://goingcoder.github.io/2018/04/04/tf1/"/>
    <id>https://goingcoder.github.io/2018/04/04/tf1/</id>
    <published>2018-04-04T07:02:55.000Z</published>
    <updated>2018-04-04T08:18:20.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TensorFlow-简介"><a href="#TensorFlow-简介" class="headerlink" title="TensorFlow 简介"></a>TensorFlow 简介</h2><p>​    TensorFlow™ 是一个使用数据流图进行数值计算的开源软件库。图中的节点代表数学运算， 而图中的边则代表在这些节点之间传递的多维数组（张量）。这种灵活的架构可让您使用一个 API 将计算工作部署到桌面设备、服务器或者移动设备中的一个或多个 CPU 或 GPU。 TensorFlow 最初是由 Google 机器智能研究部门的 Google Brain 团队中的研究人员和工程师开发的，用于进行机器学习和深度神经网络研究， 但它是一个非常基础的系统，因此也可以应用于众多其他领域。</p><p>​    TensorFlow是开源数学计算引擎，由Google创造，用Apache 2.0协议发布。TF的API是Python的，但底层是C++。和Theano不同，TF兼顾了工业和研究，在RankBrain、DeepDream等项目中使用。TF可以在单个CPU或GPU，移动设备以及大规模分布式系统中使用。</p><a id="more"></a><p>TF的计算是用<strong>图</strong>表示的：</p><ul><li>节点：节点进行计算，有一个或者多个输入输出。节点间的数据叫张量：多维实数数组。</li><li>边缘：定义数据、分支、循环和覆盖的图，也可以进行高级操作，例如等待某个计算完成。</li><li>操作：取一个输入值，得出一个输出值，例如，加减乘除。</li></ul><p>使用 TensorFlow, 你必须明白 TensorFlow:</p><ul><li>使用图 (graph) 来表示计算任务.</li><li>在被称之为 <code>会话 (Session)</code> 的上下文 (context) 中执行图.</li><li>使用 tensor 表示数据，每个 Tensor 是一个类型化的多维数组</li><li>通过 <code>变量 (Variable)</code> 维护状态.</li><li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据。</li></ul><p>​    TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op.</p><p>一个简单例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># create data,100维的列表</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">print(x_data.shape)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### create tensorflow structure start ###</span></span><br><span class="line"><span class="comment"># 创建一个给定类型的数组，将其填充在一个均匀分布的随机样本[0, 1)中</span></span><br><span class="line">Weights = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">biases = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">y = Weights * x_data + biases</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment">### create tensorflow structure end ###</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)          <span class="comment"># Very important</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, sess.run(Weights), sess.run(biases))</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/basic_usage.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/basic_usage.html</a></p><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'linux-1:源码安装',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TensorFlow-简介&quot;&gt;&lt;a href=&quot;#TensorFlow-简介&quot; class=&quot;headerlink&quot; title=&quot;TensorFlow 简介&quot;&gt;&lt;/a&gt;TensorFlow 简介&lt;/h2&gt;&lt;p&gt;​    TensorFlow™ 是一个使用数据流图进行数值计算的开源软件库。图中的节点代表数学运算， 而图中的边则代表在这些节点之间传递的多维数组（张量）。这种灵活的架构可让您使用一个 API 将计算工作部署到桌面设备、服务器或者移动设备中的一个或多个 CPU 或 GPU。 TensorFlow 最初是由 Google 机器智能研究部门的 Google Brain 团队中的研究人员和工程师开发的，用于进行机器学习和深度神经网络研究， 但它是一个非常基础的系统，因此也可以应用于众多其他领域。&lt;/p&gt;
&lt;p&gt;​    TensorFlow是开源数学计算引擎，由Google创造，用Apache 2.0协议发布。TF的API是Python的，但底层是C++。和Theano不同，TF兼顾了工业和研究，在RankBrain、DeepDream等项目中使用。TF可以在单个CPU或GPU，移动设备以及大规模分布式系统中使用。&lt;/p&gt;
    
    </summary>
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="https://goingcoder.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>dl2</title>
    <link href="https://goingcoder.github.io/2018/04/02/dl2/"/>
    <id>https://goingcoder.github.io/2018/04/02/dl2/</id>
    <published>2018-04-02T13:32:40.000Z</published>
    <updated>2018-04-04T08:15:55.053Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：</p><p><img src="/2018/04/02/dl2/image1.png" alt="1"></p><p>​    计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量$x$，权重向量$w$是(偏置项是$w_0$)，激活函数是sigmoid函数，则其输出$y$：<br>$$<br>y=sigmioid(w^T  x )<br>$$<br>sigmoid函数的定义如下：<br>$$<br>sigmoid(x)={1\over 1+e^{-x}}<br>$$<br>将其代入前面的式子，得到<br>$$<br>y={1\over 1+e^{-w^Tx}}<br>$$<br>sigmoid函数是一个非线性函数，值域是(0,1)。函数图像如下图所示</p><p><img src="/2018/04/02/dl2/image2.jpg" alt="2"></p><p>sigmoid函数的导数是：<br>$$<br>y=sigmoid(x)…………(1)\\<br>y’=y(1-y)……………(2)<br>$$</p><h2 id="神经网络是啥"><a href="#神经网络是啥" class="headerlink" title="神经网络是啥"></a>神经网络是啥</h2><p><img src="/2018/04/02/dl2/image3.jpeg" alt="3"></p><p>​    神经网络其实就是按照<strong>一定规则</strong>连接起来的多个<strong>神经元</strong>。上图展示了一个<strong>全连接(full connected, FC)</strong>神经网络，通过观察上面的图，我们可以发现它的规则包括：</p><ul><li><p>神经元按照<strong>层</strong>来布局。最左边的层叫做<strong>输入层</strong>，负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>，因为它们对于外部来说是不可见的。</p></li><li><p>同一层的神经元之间没有连接。</p></li><li><p>第N层的每个神经元和第N-1层的<strong>所有</strong>神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。</p></li><li><p>每个连接都有一个<strong>权值</strong>。</p><p>上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p></li></ul><h2 id="计算神经网络的输出"><a href="#计算神经网络的输出" class="headerlink" title="计算神经网络的输出"></a>计算神经网络的输出</h2><p>神经网络实际上就是一个输入向量$x$到输出向量$y$的函数，即：<br>$$<br>y=f_{network}(x)<br>$$<br>​    根据输入计算神经网络的输出，需要首先将输入向量的每个元素的值赋给神经网络的输入层的对应神经元，然后根据<strong>式1</strong>依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量。</p><p>接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p><p><img src="/2018/04/02/dl2/image4.png" alt="4"></p><p>​    如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$w_{41},w_{42},w_{43}$。那么，我们怎样计算节点4的输出值呢$a_4$？</p><p>​    为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是<strong>输入层</strong>的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。我们要求<strong>输入向量的维度和输入层神经元个数相同</strong>，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把$x_1$赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p><p>​    一旦我们有了节点1、2、3的输出值，我们就可以根据<strong>式1</strong>计算节点4的输出值$a_4$<br>$$<br>a_4=sigmoid(w^Tx)………………..(3)\\<br>=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})…………..(4)<br>$$</p><p>上式的$w_{4b}$是节点4的<strong>偏置项</strong>，图中没有画出来。而$w_{41},w_{42},w_{43}$分别为节点1、2、3到节点4连接的权重，在给权重$w_{ji}$编号时，我们把目标节点的编号$j$放在前面，把源节点的编号$i$放在后面。</p><p>​    同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$：<br>$$<br>y_1=sigmoid(w^Ta)……………(5)\\<br>=sigmoid(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})……………(6)<br>$$<br>​    同理，我们还可以计算出的值$y_2$。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$时，神经网络的输出向量$\vec{y}=\begin{bmatrix}y_1\\y_2\end{bmatrix} $这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong></p><h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。</p><p>首先我们把隐藏层4个节点的计算依次排列出来：<br>$$<br>a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\<br>a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\<br>a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\<br>a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\<br>$$<br>接着，定义网络的输入向量$x$和隐藏层每个节点的权重向量$w$。令<br>$$<br>\begin{align}<br>\vec{x}&amp;=\begin{bmatrix}x_1\\x_2\\x_3\\1\end{bmatrix}……..(7)\\<br>\vec{w}_4&amp;=[w_{41},w_{42},w_{43},w_{4b}……………….(8)]\\<br>\vec{w}_5&amp;=[w_{51},w_{52},w_{53},w_{5b}]…………………..(9)\\<br>\vec{w}_6&amp;=[w_{61},w_{62},w_{63},w_{6b}]……………………..(10)\\<br>\vec{w}_7&amp;=[w_{71},w_{72},w_{73},w_{7b}]………………(11)\\<br>f&amp;=sigmoid ………………..(12)<br>\end{align}<br>$$</p><p>代入到前面的一组式子，得到<br>$$<br>\begin{align}<br>a_4&amp;=f(\vec{w_4}\centerdot\vec{x})…………..(13)\\<br>a_5&amp;=f(\vec{w_5}\centerdot\vec{x})…………(14)\\<br>a_6&amp;=f(\vec{w_6}\centerdot\vec{x})…………(15)\\<br>a_7&amp;=f(\vec{w_7}\centerdot\vec{x})…………(16)<br>\end{align}<br>$$<br>​    现在，我们把上述计算$a_4,a_5,a_6,a_7$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令<br>$$<br>\vec{a}=<br>\begin{bmatrix}<br>a_4 \\<br>a_5 \\<br>a_6 \\<br>a_7 \\<br>\end{bmatrix},\qquad W=<br>\begin{bmatrix}<br>\vec{w}_4 \\<br>\vec{w}_5 \\<br>\vec{w}_6 \\<br>\vec{w}_7 \\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>w_{41},w_{42},w_{43},w_{4b} \\<br>w_{51},w_{52},w_{53},w_{5b} \\<br>w_{61},w_{62},w_{63},w_{6b} \\<br>w_{71},w_{72},w_{73},w_{7b} \\<br>\end{bmatrix}<br>,\qquad f(<br>\begin{bmatrix}<br>x_1\\<br>x_2\\<br>x_3\\<br>.\\<br>.\\<br>.\\<br>\end{bmatrix})=<br>\begin{bmatrix}<br>f(x_1)\\<br>f(x_2)\\<br>f(x_3)\\<br>.\\<br>.\\<br>.\\<br>\end{bmatrix}<br>$$<br>带入前面的一组式子，得到<br>$$<br>\vec{a}=f(W\centerdot\vec{x})\qquad (式2)<br>$$</p><p>​    在<strong>式2</strong>中，$f$是激活函数，在本例中是sigmoid函数；是某一层的权重矩阵；$\vec x$是某层的输入向量；$\vec a$是某层的输出向量。<strong>式2</strong>说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p><p>​    每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$W_1,W_2,W_3,W_4$，每个隐藏层的输出分别是$\vec a_1,\vec a_2,\vec a_3$，神经网络的输入为$\vec x$，神经网络的输入为$\vec y$，如下图所示：</p><p><img src="/2018/04/02/dl2/image5.png" alt="5"></p><p>则每一层的输出向量的计算可以表示为：<br>$$<br>\begin{align}<br>&amp;\vec{a}_1=f(W_1\centerdot\vec{x})………(17)\\<br>&amp;\vec{a}_2=f(W_2\centerdot\vec{a}_1)………(18)\\<br>&amp;\vec{a}_3=f(W_3\centerdot\vec{a}_2)……..(19)\\<br>&amp;\vec{y}=f(W_4\centerdot\vec{a}_3)………..(20)\\<br>\end{align}<br>$$<br>这就是神经网络输出值的计算方法。</p><h2 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h2><p>​    现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p><p>​    接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p><h3 id="反向传播算法-Back-Propagation"><a href="#反向传播算法-Back-Propagation" class="headerlink" title="反向传播算法(Back Propagation)"></a>反向传播算法(Back Propagation)</h3><p>​    我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。</p><p>我们假设每个训练样本为$(\vec x,\vec t)$，其中向量$\vec x$是训练样本的特征，而$\vec t$是样本的目标值。</p><p><img src="/2018/04/02/dl2/image4.png" alt="6"></p><p>​    首先，我们根据上一节介绍的算法，用样本的特征$\vec x$，计算出神经网络中每个隐藏层节点的输出$a_i$，以及输出层每个节点的输出$y_i$。</p><p>​    然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$：</p><ul><li>对于输出层节点$i$，<br>$$<br>\delta_i=y_i(1-y_i)(t_i-y_i)\qquad(式3)<br>$$<br>​</li></ul><p>其中，$\delta_i$是节点的误差项，$y_i$是节点的<strong>输出值</strong>，$t_i$是样本对应于节点$i$的<strong>目标值</strong>。举个例子，根据上图，对于输出层节点8来说，它的输出值是$y_i$，而样本的目标值是$t_i$，带入上面的公式得到节点8的误差项$\delta_8$应该是：<br>$$<br>\delta_8=y_1(1-y_1)(t_1-y_1)<br>$$</p><ul><li>对于隐藏层节点，</li></ul><p>$$<br>\delta_i=a_i(1-a_i)\sum_{k\in{outputs}}w_{ki}\delta_k\qquad(式4)<br>$$</p><p>其中，是$a_i$节点$i$的输出值，$w_{ki}$是节点$i$到它的下一层节点$k$的连接的权重，$\delta_k$是节点$i$的下一层节点的误差项。例如，对于隐藏层节点4来说，计算方法如下：<br>$$<br>\delta_4=a_4(1-a_4)(w_{84}\delta_8+w_{94}\delta_9)<br>$$<br>最后，更新每个连接上的权值：<br>$$<br>w_{ji}\gets w_{ji}+\eta\delta_jx_{ji}\qquad(式5)<br>$$<br>​    其中，$w_{ji}$是节点$i$到节点$j$的权重，$\eta$是一个成为<strong>学习速率</strong>的常数，$\delta_j$是节点$j$的误差项，$x_{ji}$是节点$i$传递给节点$j$的输入。例如，权重$w_{84}$的更新方法如下：<br>$$<br>w_{84}\gets w_{84}+\eta\delta_8 a_4<br>$$<br>类似的，权重$w_{41}$的更新方法如下<br>$$<br>w_{41}\gets w_{41}+\eta\delta_4 x_1<br>$$<br>偏置项的输入值永远为1。例如，节点4的偏置项$w_{4b}$应该按照下面的方法计算：<br>$$<br>w_{4b}\gets w_{4b}+\eta\delta_4<br>$$<br>​    我们已经介绍了神经网络每个节点误差项的计算和权重更新方法。显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据<strong>式5</strong>来更新所有的权重。</p><h3 id="反向传播算法的推导"><a href="#反向传播算法的推导" class="headerlink" title="反向传播算法的推导"></a>反向传播算法的推导</h3><p>反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt提出感知器算法将近30年之后才被发明和普及的。对此，Bengio这样回应道：</p><blockquote><p>很多看似显而易见的想法只有在事后才变得显而易见。</p></blockquote><p>接下来，我们用链式求导法则来推导反向传播算法，也就是上一小节的<strong>式3</strong>、<strong>式4</strong>、<strong>式5</strong>。</p><p><strong>前方高能预警——接下来是数学公式重灾区，读者可以酌情阅读，不必强求。</strong></p><p>按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用<strong>随机梯度下降</strong>优化算法去求目标函数最小值时的参数值。</p><p>我们取网络所有输出层节点的误差平方和作为目标函数：<br>$$<br>E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2<br>$$<br>​    其中，$E_d$表示是样本$d$的误差。</p><p><img src="/2018/04/02/dl2/image4.png" alt="7"></p><p>观察上图，我们发现权重$w_{ji}$仅能通过影响节点$j$的输入值影响网络的其它部分，设$net_j$是节点$j$的<strong>加权输入</strong>，即<br>$$<br>\begin{align}<br>net_j&amp;=\vec{w_j}\centerdot\vec{x_j}………(21)\\<br>&amp;=\sum_{i}{w_{ji}}x_{ji}……..(22)<br>\end{align}<br>$$<br>$E_d$是$net_j$的函数，而$net_j$是$w_{ji}$的函数。根据链式求导法则，可以得到：<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{w_{ji}}}&amp;=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{net_j}}{\partial{w_{ji}}}…..(23)\\<br>&amp;=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{\sum_{i}{w_{ji}}x_{ji}}}{\partial{w_{ji}}}…..(24)\\<br>&amp;=\frac{\partial{E_d}}{\partial{net_j}}x_{ji}…….(24)<br>\end{align}<br>$$<br>上式中，$x_{ji}$是节点$i$传递给节点$j$的输入值，也就是节点$i$的输出值。</p><p>对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分<strong>输出层</strong>和<strong>隐藏层</strong>两种情况。</p><h4 id="输出层权值训练"><a href="#输出层权值训练" class="headerlink" title="输出层权值训练"></a>输出层权值训练</h4><p>对于<strong>输出层</strong>来说，$net_j$仅能通过节点$j$的输出值$y_i$来影响网络其它部分，也就是说$E_d$是$y_i$的函数，而$y_i$是$net_j$的函数，其中$y_i=sigmoid(net_j)$。所以我们可以再次使用链式求导法则：<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{net_j}}&amp;=\frac{\partial{E_d}}{\partial{y_j}}\frac{\partial{y_j}}{\partial{net_j}}……(26)\\<br>\end{align}<br>$$<br>考虑上式第一项:<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{y_j}}&amp;=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2…..(27)\\<br>&amp;=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2……(28)\\<br>&amp;=-(t_j-y_j)<br>\end{align}<br>$$<br>考虑上式第二项：<br>$$<br>\begin{align}<br>\frac{\partial{y_j}}{\partial{net_j}}&amp;=\frac{\partial sigmoid(net_j)}{\partial{net_j}}…..(30)\\<br>&amp;=y_j(1-y_j)  …….(31)\\<br>\end{align}<br>$$<br>将第一项和第二项带入，得到：<br>$$<br>\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)<br>$$<br>如果令$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$,也就是一个节点的误差项是网络误差对这个节点输入的偏导数的相反数。带入上式，得到：<br>$$<br>\delta_j=(t_j-y_j)y_j(1-y_j)<br>$$<br>上式就是<strong>式3</strong>。</p><p>将上述推导带入随机梯度下降公式，得到：<br>$$<br>\begin{align}<br>w_{ji}&amp;\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}…….(32)\\<br>&amp;=w_{ji}+\eta(t_j-y_j)y_j(1-y_j)x_{ji}………(33)\\<br>&amp;=w_{ji}+\eta\delta_jx_{ji}……..(34)<br>\end{align}<br>$$<br>上式就是<strong>式5</strong>。</p><h4 id="隐藏层权值训练"><a href="#隐藏层权值训练" class="headerlink" title="隐藏层权值训练"></a>隐藏层权值训练</h4><p>现在我们要推导出隐藏层的$\frac{\partial{E_d}}{\partial{net_j}}$。</p><p>​    首先，我们需要定义节点$j$的所有直接下游节点的集合$Downstream(j)$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到只能通过影响$Downstream(j)$再影响$E_d$。设$net_k$是节点$j$的下游节点的输入，则$E_d$是$net_k$的函数，而$net_k$是$net_j$的函数。因为$net_k$有多个，我们应用全导数公式，可以做出如下推导：<br>$$<br>\begin{align}<br>\frac{\partial{E_d}}{\partial{net_j}}&amp;=\sum_{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}\frac{\partial{net_k}}{\partial{net_j}}…..(35)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{net_j}}……….(36)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{net_j}}……(37)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_kw_{kj}\frac{\partial{a_j}}{\partial{net_j}}……..(38)\\<br>&amp;=\sum_{k\in Downstream(j)}-\delta_kw_{kj}a_j(1-a_j)………(39)\\<br>&amp;=-a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}…….(40)<br>\end{align}<br>$$<br>因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：<br>$$<br>\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}<br>$$<br>上式就是<strong>式4</strong>。</p><p><strong>——数学公式警报解除——</strong></p><p>​    至此，我们已经推导出了反向传播算法。需要注意的是，我们刚刚推导出的训练规则是根据激活函数是sigmoid函数、平方和误差、全连接网络、随机梯度下降优化算法。如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样。但是无论怎样，训练规则的推导方式都是一样的，应用链式求导法则进行推导即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">for l in range(len(self.weights)):</span><br><span class="line">    # a.append(self.activation(np.dot(a[l], self.weights[l])))</span><br><span class="line"></span><br><span class="line">    if l == len(self.weights) - 1 :</span><br><span class="line">        #添加偏置的输入1</span><br><span class="line">        result = self.activation(np.dot(a[l], self.weights[l]))</span><br><span class="line">        # result = list(self.activation(np.dot(a[l], self.weights[l]))).append(1)</span><br><span class="line">        a.append(np.array(result))</span><br><span class="line">    else:</span><br><span class="line">        result = self.activation(np.dot(a[l], self.weights[l]))</span><br><span class="line">        result = np.concatenate((result, [1]))  # 先将p_变成list形式进行拼接，注意输入为一个tuple</span><br><span class="line">        a.append(np.array(result))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经元&quot;&gt;&lt;a href=&quot;#神经元&quot; class=&quot;headerlink&quot; title=&quot;神经元&quot;&gt;&lt;/a&gt;神经元&lt;/h2&gt;&lt;p&gt;神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是&lt;strong&gt;阶跃函数&lt;/strong&gt;；而当我们说神经元时
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>dl1</title>
    <link href="https://goingcoder.github.io/2018/03/31/dl1/"/>
    <id>https://goingcoder.github.io/2018/03/31/dl1/</id>
    <published>2018-03-31T04:59:39.000Z</published>
    <updated>2018-03-31T14:28:58.270Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性单元"><a href="#线性单元" class="headerlink" title="线性单元"></a>线性单元</h2><p>​    感知器有一个问题，当面对的数据集不是<strong>线性可分</strong>的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个<strong>可导</strong>的<strong>线性函数</strong>来替代感知器的<strong>阶跃函数</strong>，这种感知器就叫做<strong>线性单元</strong>。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。</p><p>为了简单起见，我们可以设置线性单元的激活函数$f$为</p><p>$f(x)=x$</p><p>这样的线性单元如下图所示    </p><p><img src="/2018/03/31/dl1/image1.png" alt="t1"></p><p>感知器</p><p><img src="/2018/03/31/dl1/image2.png" alt="t"></p><p>​    替换激活函数之后，<strong>线性单元</strong>将返回一个<strong>实数值</strong>而不是<strong>0,1分类</strong>。因此线性单元用来解决<strong>回归</strong>问题而不是<strong>分类</strong>问题。</p><h3 id="线性单元的模型"><a href="#线性单元的模型" class="headerlink" title="线性单元的模型"></a>线性单元的模型</h3><p>​    当我们说<strong>模型</strong>时，我们实际上在谈论根据输入$x$预测输出$y$的<strong>算法</strong>。比如，$x$可以是一个人的工作年限，$y$可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如：</p><p>$y=h(x)=w*x+b$</p><p>​    函数$h(x)$叫做<strong>假设</strong>，而$w,b$是它的<strong>参数</strong>。我们假设参数$w=1000$，参数$b=500$，如果一个人的工作年限是5年的话，我们的模型会预测他的月薪为</p><p>$y=h(x)=1000*5+500=5500（元）$</p><p>​    你也许会说，这个模型太不靠谱了。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为<strong>特征</strong>。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他</p><p>$x=(5,IT,百度，T6)$</p><p>​    既然输入$x$变成了一个具备四个特征的向量，相对应的，仅仅一个参数$w$就不够用了，我们应该使用4个参数$w_1,w_2,w_3,w_4$，每个特征对应一个。这样，我们的模型就变成</p><p>$y=h(x)=w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+w_4</em>x_4+b$</p><p>​    其中，$x_1$对应工作年限，$x_2$对应行业，$x_3$对应公司，$x_4$对应职称。</p><p>​    为了书写和计算方便，我们可以$w_0$令等于$b$，同时令$w_0$对应于特征$x_0$。由于$x_0$其实并不存在，我们可以令它的值永远为1。也就是说</p><p>$b=w_0*x_0$ 其中$x_0=1$</p><p>​    这样上面的式子就可以写成</p><p>$y=h(x)=w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+w_4</em>x_4+b$              ………….(1)</p><p>$=w_0<em>x_0+w_1</em>x_1+w_2<em>x_2+w_3</em>x_3+w_4*x_4$                    ………….(2)</p><p>​    我们还可以把上式写成向量的形式</p><p>​    $y=h(x)=w^Tx$</p><p>​    长成这种样子模型就叫做<strong>线性模型</strong>，因为输出$y$就是输入特征$x_1,x_2,x_3,…$的<strong>线性组合</strong>。</p><h3 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h3><p>​    接下来，我们需要关心的是这个模型如何训练，也就是参数$w$取什么值最合适。</p><p>​    机器学习有一类学习方法叫做<strong>监督学习</strong>，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征$x$，也包括对应的输出$y$(也叫做<strong>标记，label</strong>)。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业…)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征)，也看到对应问题的答案(标记)。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。</p><p>​    另外一类学习方法叫做<strong>无监督学习</strong>，这种方法的训练样本中只有而没有。模型可以总结出特征的一些规律，但是无法知道其对应的答案$y$。</p><p>​    很多时候，既有$x$又有的$y$训练样本是很少的，大部分样本都只有$x$。比如在语音到文本(STT)的识别任务中，$x$是语音，$y$是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并<strong>标注</strong>上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用<strong>无监督学习方法</strong>先做一些<strong>聚类</strong>，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。</p><h3 id="线性单元的目标函数"><a href="#线性单元的目标函数" class="headerlink" title="线性单元的目标函数"></a>线性单元的目标函数</h3><p>​    现在，让我们只考虑<strong>监督学习</strong>。</p><p>​    在监督学习下，对于一个样本，我们知道它的特征$x$，以及标记$y$。同时，我们还可以根据模型计算得到输出。注意这里面我们用表示训练样本里面的<strong>标记</strong>，也就是<strong>实际值</strong>；用带上划线的表示模型计算的出来的<strong>预测值</strong>。我们当然希望模型计算出来的和越接近越好。</p><p>​    数学上有很多方法来表示的$\overline y$和$y$的接近程度，比如我们可以用$\overline y$和$y$的差的平方的$1\over2$来表示它们的接近程度</p><p>$e={1\over2}(y-\overline y)^2$</p><p>​    我们把$e$叫做<strong>单个样本</strong>的<strong>误差</strong>。至于为什么前面要乘$1\over 2$，是为了后面计算方便。</p><p>训练数据中会有很多样本，比如$N$个，我们可以用训练数据中<strong>所有样本</strong>的误差的<strong>和</strong>，来表示模型的误差$E$，也就是$E=e^{(1)}+e^{(2)}+e^{(3)}+\dots+e^{(n)}$</p><p>​    上式的$e^{(1)}$表示第一个样本的误差，$e^{(2)}$表示第二个样本的误差……。</p><p>​    我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样</p><p>$E=e^{(1)}+e^{(2)}+e^{(3)}+\dots+e^{(n)}$ …………………(3)</p><p>$\sum_{i=1}^n e^{(i)} $                                             …………………(4)</p><p>$={1\over 2}\sum_{i=1}^n(y^(i)-\overline y^{(i)})^2$         .     …………………(5)</p><p>其中</p><p>$\overline y^{(i)}=h(x^{(i)})$  ……………………..(6)</p><p>$=w^Tx^{(i)}$ ………………………..(7)</p><p>(5)中，$x^{(i)}$表示第$i$个训练样本的特征，$y^{(i)}$表示第$i$个样本的标记，我们也可以用<strong>元组</strong>$(x^{(i)},y^{(i)})$表示第$i$个 <strong>训练样本</strong>。$\overline y^{(i)}$则是模型对第$i$个样本的<strong>预测值</strong>。</p><p>​    我们当然希望对于一个训练数据集来说，误差最小越好，也就是(5)的值越小越好。对于特定的训练数据集来说，$(x^{(i)},y^{(i)})$的值都是已知的，所以(式2)其实是参数$w$的函数。</p><p>$E(w)={1\over 2}\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})^2 $               ……………………..(8)</p><p>$={1\over 2}\sum_{i=1}^n(y^{(i)}-w^Tx^{(i)})^2$ ………………………….(9)</p><p>​    由此可见，模型的训练，实际上就是求取到合适的，使(5)取得最小值。这在数学上称作<strong>优化问题</strong>，而就是我们优化的目标，称之为<strong>目标函数</strong>。</p><h3 id="梯度下降优化算法"><a href="#梯度下降优化算法" class="headerlink" title="梯度下降优化算法"></a>梯度下降优化算法</h3><p>​    我们学过怎样求函数的极值。函数$y=f(x)$的极值点，就是它的导数$f’(x)=0$的那个点。因此我们可以通过解方程$f’(x)=0$，求得函数的极值点$(x_0,y_0)$。</p><p>​    不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点『试』出来。如下图所示：</p><p><img src="/2018/03/31/dl1/image3.png" alt="3"></p><p>​    首先，我们随便选择一个点开始，比如上图的点$x_0$。接下来，每次迭代修改$x$的为，经过数次迭代后最终达到函数最小值点。</p><p>​    你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数$y=f(x)$的<strong>梯度</strong>的<strong>相反方向</strong>来修改$x$。什么是<strong>梯度</strong>呢？梯度<strong>是一个向量，它指向</strong>函数值<strong>上升最快</strong>的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。</p><p>按照上面的讨论，我们就可以写出梯度下降算法的公式</p><p>$x_{new}=x_{old}-\eta\nabla f(x)$</p><p>​    其中，$\nabla$是<strong>梯度算子</strong>，$\nabla f(x)$就是指的梯度。$\eta$是步长，也称作<strong>学习速率</strong>。</p><p>​    对于上一节列出的目标函数(式5)</p><p>$E(w)={1\over2}\sum_{i=1}^n(y^{(i)}-\overline y^{(i)}$</p><p>梯度下降算法可以写成</p><p>$w_{new}=w_{old}+\eta\nabla E(w)$</p><p>我们要来求取$\nabla E(w)$，然后带入上式，就能得到线性单元的参数修改规则。</p><p>关于$\nabla E(w)$的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。在这里，您只需要知道，经过一大串推导，目标函数$E(w)$的梯度是</p><p>$\nabla E(w)=-\sum_{i=1}^{n}(y^{(i)}-\overline y^{(i)})x^{(i)}$</p><p>因此，线性单元的参数修改规则最后是这个样子</p><p>$w_{new}=w_{old}+\eta\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})$</p><p>有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。</p><p>需要说明的是，如果每个样本有M个特征，则上式w，x都是M+1维向量（因为我们加上了一个恒为1的虚拟特征$x_0$,参考前面的内容），而$y$是标量。用高逼格的数学符号表示 ，就是</p><p>$w,x\in R^{(M+1)}$</p><p>$y\in R^1$</p><p>为了让您看明白说的是啥，我吐血写下下面这个解释(写这种公式可累可累了)。因为$w,x$是M+1维<strong>列向量</strong>，所以(式3)可以写成<br>$$<br>\begin{bmatrix}<br>w_0 \\<br>w_1\\<br>w_2\\<br>\dots\\<br>w_m\\<br>\end{bmatrix}=\begin{bmatrix}<br>w_0 \\<br>w_1\\<br>w_2\\<br>\dots\\<br>w_m\\<br>\end{bmatrix}+\eta\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})\begin{bmatrix}<br>1 \\<br>x_1^{(i)}\\<br>x_2^{(i)}\\<br>\dots\\<br>x_m^{(i)}\\<br>\end{bmatrix}<br>$$</p><h4 id="nabla-E-w-的推导"><a href="#nabla-E-w-的推导" class="headerlink" title="$\nabla E(w)$的推导"></a>$\nabla E(w)$的推导</h4><p>​    这一节你尽可以跳过它，并不太会影响到全文的理解。当然如果你非要弄明白每个细节，那恭喜你骚年，机器学习的未来一定是属于你的。</p><p>​    首先，我们先做一个简单的前戏。我们知道函数的梯度的定义就是它相对于各个变量的<strong>偏导数</strong>，所以我们写下下面的式子<br>$$<br>\nabla E(w)={\partial\over\partial w}E(w)                …………….(10)\\<br>={\partial\over \partial w}{1\over2}\sum_{i=1}^n(y^{(i)} - \overline y^{(i)})………….(11)<br>$$<br>​    可接下来怎么办呢？我们知道和的导数等于导数的和，所以我们可以先把求和符号$\sum$里面的导数求出来，然后再把它们加在一起就行了，也就是<br>$$<br>{\partial \over \partial w  }{1\over 2}\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})^2…………(12)\\<br>={1\over 2}\sum_{i=1}^n{\partial\over \partial w}(y^{(i)}-\overline y^{(i)})^2…………(13)<br>$$</p><p>​    现在我们可以不管高大上的$\sum$了，先专心把里面的导数求出来。<br>$$<br>{\partial \over \partial w}(y^{(i)}-\overline y^{(i)})^2………………….(14)\\<br>={\partial \over \partial w}({y^{(i)}}^2-2\overline y^{(i)} +{\overline y^{(i)}}^2)………………….(15)<br>$$<br>​    我们知道，$y$是与$w$无关的常数，而$\overline y=w^Tx$,下面我们根据链式求导法则来求导。<br>$$<br>{\partial E(w)\over \partial w}={\partial E(\overline y)\over \partial\overline y}{\partial\overline y\over \partial w}<br>$$</p><p>我们分别计算上式等号右边的两个偏导数<br>$$<br>{\partial E(w)\over \partial \overline y}={\partial \over\partial \overline y}({y^{(i)}}^2-2\overline y^{(i)}y^{(i)}+{\overline y^{(i)}}^2)…………(16)\\<br>=-2y^{(i)}+2\overline y ^{(i)}……………….(17)<br>$$</p><p>$$<br>{\partial \overline y \over \partial w}={\partial \over\partial w}w^Tx ……………………..(18)\\<br>=x……………………(19)<br>$$<br>代入，我们求得$\sum$里面的偏导数是<br>$$<br>{\partial \over \partial w}(y^{(i)}-\overline y^{(i)})^2………………………..(20)\\<br>=2(-y^{(i)}+\overline y^{(i)})x……………………….(21)<br>$$<br>最后代入$\nabla E(w)$,求得<br>$$<br>\nabla E(w)={1\over2}\sum_{i=1}^n{\partial\over \partial w}(y^{(i)}-\overline y^{(i)})^2…………..(22)\\<br>={1\over 2}\sum_{i=1}^n2(-y^{(i)}+\overline y^{(i)})x…………….(23)\\<br>=-\sum_{i=1}^n(y^{(i)}-\overline y^{(i)})x……………….(25)<br>$$</p><h3 id="随机梯度下降算法-Stochastic-Gradient-Descent-SGD"><a href="#随机梯度下降算法-Stochastic-Gradient-Descent-SGD" class="headerlink" title="随机梯度下降算法(Stochastic Gradient Descent, SGD)"></a>随机梯度下降算法(Stochastic Gradient Descent, SGD)</h3><p>​    如果我们根据上面式子来训练模型，那么我们每次更新的迭代，要遍历训练数据中<strong>所有的样本</strong>进行计算，我们称这种算法叫做<strong>批梯度下降(Batch Gradient Descent)</strong>。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新并不一定按照减少的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别</p><p><img src="/2018/03/31/dl1/image4.png" alt="4"></p><p>​    如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。</p><p>​    最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>事实上，一个机器学习算法其实只有两部分</p><ul><li><p><strong>模型</strong> 从输入特征预测输入的那个函数</p></li><li><p><strong>目标函数</strong> 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的<strong>最优值</strong>。很多时候我们只能获得目标函数的<strong>局部最小(最大)值</strong>，因此也只能得到模型参数的<strong>局部最优值</strong>。</p><p>因此，如果你想最简洁的介绍一个算法，列出这两个函数就行了。</p><p>接下来，你会用<strong>优化算法</strong>去求取目标函数的最小(最大)值。<strong>[随机]梯度{下降|上升}</strong>算法就是一个<strong>优化算法</strong>。针对同一个<strong>目标函数</strong>，不同的<strong>优化算法</strong>会推导出不同的<strong>训练规则</strong>。我们后面还会讲其它的优化算法。</p><p>其实在机器学习中，算法往往并不是关键，真正的关键之处在于选取特征。选取特征需要我们人类对问题的深刻理解，经验、以及思考。而<strong>神经网络</strong>算法的一个优势，就在于它能够自动学习到应该提取什么特征，从而使算法不再那么依赖人类，而这也是神经网络之所以吸引人的一个方面。</p><p>现在，经过漫长的烧脑，你已经具备了学习<strong>神经网络</strong>的必备知识。下一篇文章，我们将介绍本系列文章的主角：<strong>神经网络</strong>，以及用来训练神经网络的大名鼎鼎的算法：<strong>反向传播</strong>算法。至于现在，我们应该暂时忘记一切，尽情奖励自己一下吧。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性单元&quot;&gt;&lt;a href=&quot;#线性单元&quot; class=&quot;headerlink&quot; title=&quot;线性单元&quot;&gt;&lt;/a&gt;线性单元&lt;/h2&gt;&lt;p&gt;​    感知器有一个问题，当面对的数据集不是&lt;strong&gt;线性可分&lt;/strong&gt;的时候，『感知器规则』可能无法收敛，
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="https://goingcoder.github.io/2018/03/12/lr/"/>
    <id>https://goingcoder.github.io/2018/03/12/lr/</id>
    <published>2018-03-12T12:54:18.000Z</published>
    <updated>2018-03-13T18:09:59.946Z</updated>
    
    <content type="html"><![CDATA[<h3 id="逻辑斯谛分布"><a href="#逻辑斯谛分布" class="headerlink" title="逻辑斯谛分布"></a>逻辑斯谛分布</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍<strong>逻辑斯谛分布</strong>，该分布的定义是</p><p>设$X$是连续随机变量,$X$服从逻辑斯谛分布是指$X$ 服从如下分布函数和密度函数:</p><p>$F(x)=P(X\le x)={1\over 1+e^{-(x-\mu)}/\gamma}$ …………..(1)</p><p>$f(x)=F^,(X\le x)={e^{-(x-\mu)/\gamma}\over \gamma(1+e^{-(x-\mu)/\gamma})}$ …………(2)</p><p>式中，$\mu$为位置参数 ，$\gamma&gt;0$ 为形状参数。</p><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;逻辑斯蒂分布的密度函数$f(x)$ 和分布函数$F(x)$的图形如下图所示。</p><p><img src="/2018/03/12/lr/lr.jpg" alt="图片"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布函数属于逻辑斯蒂函数，其图形是一条$S$形曲线(sigmoid curve).该曲线以点$(\mu,{1 \over 2})$为中心对称，即满足</p><p>$F(-x+\mu)-{1\over 2}=-F(x+\mu)+{1\over 2}$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$ 的值越小，曲线在中心附近增长越快。</p><h3 id="二项逻辑斯蒂回归模型"><a href="#二项逻辑斯蒂回归模型" class="headerlink" title="二项逻辑斯蒂回归模型"></a>二项逻辑斯蒂回归模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二项逻辑回归模型(binomial logistic regression model)是一种分类模型，由条件概率分布$P(Y|X)$ 表示，形式为参数化的逻辑斯蒂分布。这里，随机变量$X$ 取值为实数，随机变量$Y$取值为1或0。我们通过监督学习的方法来估计模型参数。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一种由条件概率表示的模型，其条件概率模型如下：</p><p>$P(Y=1|x)={exp(w \cdot x +b) \over 1+exp(w \cdot x +b)}$   ………….(3)</p><p>$P(Y=0|x)={1 \over 1+exp(w \cdot x +b)}$   ………….(4)</p><p><img src="/2018/03/12/lr/lr2.png" alt="图片"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图 逻辑回归网络</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里，$x\in R^n$ 是输入，$Y \in{0,1}$是输出，$w\in R^n$和$b\in R$是参数，$w$称为权重向量，$b$称为偏置，$w\cdot x$为$w$和$x$的內积。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于给定的输入实例$x$，按照(3)和(4)可以求得$P(Y=1|x)$和$P(Y=0|x)$。逻辑斯蒂回归比较两个条件概率值得大小，将实例$x$分到概率值较大的那一类。 有时为了方便，将权值向量和输入向量加以扩充，仍记作$w$，$x$，即$w=(w^{(1)},w^{(2)},\dots,w^{n},b) ^T$，$x=(x^{(1)},x^{(2)},\dots,x^{(n)},1)^T$ 。这时，逻辑斯蒂回归模型如下：</p><p>$P(Y=1|x)={exp(w \cdot x) \over 1+exp(w \cdot x)}$ ………..(5)</p><p>$P(Y=0|x)={1 \over 1+exp(w \cdot x)}$ ………..(6)</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实分子分母同时除以指数部分，就变成sigmoid函数了。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在考查逻辑斯蒂回归模型的特点。一个时间的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率是$p\over{1-p}$，该事件的对数几率(log odds)或者logit函数是</p><p>$logit(p)=log{p\over{1-p}}$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对逻辑斯蒂回归而言，由式（5）与（6）得</p><p>$log{P(Y=1|x)\over {1-P(Y=1|x)}}=w \cdot x$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就是说，在逻辑斯蒂回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。或者说输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型。反过来讲，如果知道权值向量，给定输入$x$，就能求出$Y=1$的概率：</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意 </strong>，这里$x\in R^{n+1},w\in R^{n+1}$。通过逻辑斯蒂回归模型定义式（5）可以将线性函数$w\cdot x$转换为概率：</p><p>$P(Y=1|x)={exp(w\cdot x)\over{1+exp(w\cdot x)}}$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近无穷，概率值就越接近0。这样的模型就是逻辑斯蒂回归模型。</p><h3 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据给定的数据集，把参数$w$给求出来了。要找参数$w$，首先就是得把代价函数给定义出来，也就是目标函数。首先想到的是类型线性回归的做法，利用误差平方来当代价函数。</p><p>$J(w)=\sum_{i}{1\over 2}(\phi(z^{i})-y^i)^2$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中 $z^i=w\cdot x$ ，$i$表示第$i$个样本点，$y^i$表示第$i$个样本的真实值，$\phi(z^{i})$表示第$i$个样本的预测值 。</p><p>$\phi(z^{i}) ={1\over 1+e^{-z^i}}$ ，这是一个非凸函数，这就意味着代价函数有着许多的局部最小值，不利于我们的求解。</p><p><img src="/2018/03/12/lr/lr3.png" alt="3"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;逻辑斯蒂回归模型学习中，对于给定的训练数据集$T={(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)}$,其中，$x_i\in R^n,y_i\in{0,1}$可以应用极大似然估计法估计模型参数，从而得到逻辑斯蒂回归模型。</p><p>设：</p><p>$P(Y=1|x)=\pi(x)$</p><p>$P(Y=0|x)=1-\pi(x)$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于每一个观察到的样本$(x_i,y_i)$ 出现的概率是：</p><p>​    $P(x_i,y_i)=P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}=\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解释一下，当$y=1$的时候，后面的那一项没有了，那就只剩下$x$属于1类的概率，当$y=0$的时候，第一项没有了，那就只剩下后面那个$x$属于0的概率。不管$y$是0还是1，上面得到的都是$(x,y)$出现的概率 。那我们的整个样本集，也就是$n$个独立的样本出现的似然函数为（因为每个样本都是独立的，所以$n$的样本出现的概率就是他们各自出现的概率相乘）：</p><p>似然函数为$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$    </p><p>对数似然函数为</p><p>$L(w)=\sum_{i=1}^N[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]$</p><p>$=\sum_{i=1}^N \left[ y_ilog{\pi(x_i)\over{1-\pi(x_i)}}+log(1-\pi(x_i)) \right ]$</p><p>$=\sum_{i=1}^N[y_i(w\cdot x_i)-log(1+exp(w\cdot x_i))]$</p><p>对$L(w)$ 求极大值，得到$w$的估计值。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;书上没有写出完整的参数估计算法，但给出了其对数似然函数，经过简单的证明可以得出该函数可以得出该函数是单调上。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以使用$L(w)$作为目标函数，求似然函数的最大值，加个负号，就可以变成最小值，与我们平常的机器学习中的损失函数对应起来。下面对公式进行推导。</p><p>$L(w)=\sum_{i=1}^N[y_i(w\cdot x_i)-log(1+exp(w\cdot x_i))]$</p><p>$-L(w)=\sum_{i=1}^N[(log(1+exp(w\cdot x_i))-y_i(w\cdot x_i)]$</p><p>${\partial (-L(w))\over w}=\sum_{i=1}^N\left({ exp(w\cdot x_i)\cdot x_i \over 1+exp(w\cdot x_i)} - y_i\cdot x_i\right)$</p><p>${\partial (-L(w))\over w}=\sum_{i=1}^N\left (\left({ exp(w\cdot x_i) \over 1+exp(w\cdot x_i)} - y_i\right) \cdot x_i\right)$</p><p>${\partial  (-L(w)) \over w }=\sum_{i=1}^N\left (\left({1 \over 1+exp-(w\cdot x_i)} - y_i\right) \cdot x_i\right)$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们令导数为0，会发现无法解析求解。没办法，只能借助高大上的迭代来搞定了。这里选用了经典的梯度下降算法。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每次选择一个误分类点，用上述梯度对$w$进行更新即可，注意由于梯度中包含指数操作，所以需要一个很小的学习率。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样，问题就变成以对数似然函数为目标函数的最优化问题。逻辑斯蒂回归学习中通常采用的方法是梯度下降法及拟牛顿法。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设$w$的极大似然估计值是$\hat w$ ，那么学到的逻辑斯蒂回归模型为</p><p>$P(Y=1|x)={exp(\hat w\cdot x ) \over 1+exp(\hat w \cdot x)}$</p><p>$P(Y=0|x)={1 \over 1+exp(\hat w \cdot x)}$</p><h3 id="多项逻辑回归"><a href="#多项逻辑回归" class="headerlink" title="多项逻辑回归"></a>多项逻辑回归</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面介绍的逻辑斯蒂回归模型是二项分类模型，用于二类分类，用于二类分类。可以将其推广为多项逻辑斯蒂回归(multi-nominal logistic regression model)，用于多类分类。假设离散型随机变量$Y$的取值集合是${1,2,\dots,K}$,那么多项逻辑回归模型是        </p><p>$P(Y=k|x)={exp(w_k \cdot x)}\over 1+{\sum_{k=1}^{K-1}exp(w_k\cdot x)}$ ，$k=1,2,\dots,K-1$    ………………..(7)</p><p>$P(Y=K|x)={1 \over \sum_{i=1}^{K-1}exp(w_k\cdot x)},k=1,2,\dots,K-1$ ………………..(8)</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里，$x\in R^{n+1},w_k\in R^{n+1}$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二项逻辑斯蒂回归的参数法也可以推广到多项逻辑斯蒂回归。</p><p>参考：《统计学习方法》，李航</p><p><a href="http://www.hankcs.com/ml/the-logistic-regression-and-the-maximum-entropy-model.html" target="_blank" rel="noopener">http://www.hankcs.com/ml/the-logistic-regression-and-the-maximum-entropy-model.html</a></p><p><a href="http://blog.csdn.net/wds2006sdo/article/details/53084871" target="_blank" rel="noopener">http://blog.csdn.net/wds2006sdo/article/details/53084871</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: '逻辑回归',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;逻辑斯谛分布&quot;&gt;&lt;a href=&quot;#逻辑斯谛分布&quot; class=&quot;headerlink&quot; title=&quot;逻辑斯谛分布&quot;&gt;&lt;/a&gt;逻辑斯谛分布&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;首先介绍&lt;strong&gt;逻辑斯谛分布&lt;/strong&gt;，该分布的定义是&lt;/p&gt;
&lt;p&gt;设$X$是连续随机变量,$X$服从逻辑斯谛分布是指$X$ 服从如下分布函数和密度函数:&lt;/p&gt;
&lt;p&gt;$F(x)=P(X\le x)={1\over 1+e^{-(x-\mu)}/\gamma}$ …………..(1)&lt;/p&gt;
&lt;p&gt;$f(x)=F^,(X\le x)={e^{-(x-\mu)/\gamma}\over \gamma(1+e^{-(x-\mu)/\gamma})}$ …………(2)&lt;/p&gt;
&lt;p&gt;式中，$\mu$为位置参数 ，$\gamma&amp;gt;0$ 为形状参数。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://goingcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://goingcoder.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计法MLE</title>
    <link href="https://goingcoder.github.io/2018/03/10/mle/"/>
    <id>https://goingcoder.github.io/2018/03/10/mle/</id>
    <published>2018-03-10T15:39:39.000Z</published>
    <updated>2018-03-10T18:16:15.596Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法（$Method  of  Maximum  Likelihood  Estimation –MLE$）</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法最早由高斯（C.F.Gauss）提出。后来为费歇在1912年的文章中重新提出，并且证明了这个方法的一些性质。极大似然估计这一名称也是费歇（R.A.Fisher）给的。这是一种目前仍然得到广泛应用的方法。它是建立在极大似然原理的基础上的一个统计方法。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 在数理统计学中，似然函数是一种关于统计模型中的<strong>参数</strong>的<strong>函数</strong>，表示模型参数中的似然性。 似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。<strong>“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性</strong>，但是在<strong>统计学</strong>中，“似然性”和“或然性”或“概率”又有明确的区分。<strong>概率</strong>用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而<strong>似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。</strong> 有人说，概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法的依据就是：<strong>概率最大的事件最可能发生</strong></p><a id="more"></a><p>&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计法最早由高斯（C.F.Gauss）提出。后来为费歇在1912年的文章中重新提出，并且证明了这个方法的一些性质。极大似然估计这一名称也是费歇（R.A.Fisher）给的。这是一种目前仍然得到广泛应用的方法。它是建立在极大似然原理的基础上的一个统计方法。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 看论文的时候读到这样一句话：</p><p>​    <em>Assuming  the  training  instances  are  independently  sampled,the  likelihood  function  of  parameters   $\theta={w,\alpha,\beta}$ given  the  observations  D  can  be  factored  as</em><br>$$<br>Pr[D|\theta]=\prod_{i=1}^NPr[y_i^1,\dots,y_i^R|x_i;\theta]<br>$$<br>&nbsp; &nbsp; &nbsp; &nbsp; 原来只关注公式，所以一带而过。再重新看这个公式前的描述，细思极恐。</p><p> <em>the <strong>likelihood function</strong> of the parameters θ = {w,α,β} <strong>given the observations D</strong> can be factored as..</em></p><p><strong>两个疑问</strong>：</p><ul><li>likelihood function_为什么会写成条件概率的形式？</li><li><strong>given</strong>的明明是D，为什么到后面的公式里，却变成了$given  \theta$呢？</li></ul><p>&nbsp; &nbsp; &nbsp; &nbsp; <strong>常说的概率是指给定参数后，预测即将发生的事件的可能性</strong>。拿硬币这个例子来说，我们已知一枚均匀硬币的正反面概率分别是0.5，要预测抛两次硬币，硬币都朝上的概率：</p><p>$H$代表$Head$，表示头朝上</p><p>$p(HH | p_H = 0.5) = 0.5*0.5 = 0.25.$</p><p> &nbsp;   这种写法其实有点误导，后面的这个其实是作为参数存在的，而不是一个随机变量，因此不能算作是条件概率，更靠谱的写法应该是 $p(HH;p=0.5)。$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;而似然概率正好与这个过程相反，<strong>我们关注的量</strong>不再是事件的发生概率，而是已<strong>知发生了某些事件，我们希望知道参数应该是多少。</strong></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们已经抛了两次硬币，并且知道了结果是两次头朝上，这时候，我希望知道这枚硬币抛出去正面朝上的概率为0.5的概率是多少？正面朝上的概率为0.8的概率是多少？</p><p>如果我们希望知道正面朝上概率为0.5的概率，这个东西就叫做似<strong>然函数</strong>，可以说成是对某一个参数的猜想$（p=0.5）$的概率，这样表示成(条件)概率就是</p><p>$L(p_H=0.5|HH) =P(HH|p_H=0.5) = $（另一种写法）$P(HH;p_H=0.5).$</p><p>为什么可以写成这样？我觉得可以这样来想：</p><p>$L(\theta|x)=f(x|\theta)$</p><p>这里$\theta$是未知参数，它属于参数空间。</p><p>&nbsp;$f(x|\theta)$是一个密度函数，特别地，它表示给定$\theta$ 下关于联合概率样本值$x$ 的联合密度函数。前者是关于$\theta$的函数，后者是关于$x$的函数。所以这里的等号理解为函数值形式的相等，而不是两个函数本身是同一函数。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>似然函数本身也是一种概率</strong>，我们可以把$L(pH=0.5|HH)$写成$P(pH=0.5|HH)$; 而根据贝叶斯公式，$P(pH=0.5|HH) = {P(pH=0.5,HH)\over P(HH)}$；既然$HH$是已经发生的事件，理所当然$P(HH) = 1$,所以：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 在<strong>统计学</strong>中，我们关心的是在<strong>已知一系列投掷的结果时</strong>，关于硬币投掷时正面朝上的可能性的信息。<br>我们可以建立一个统计模型：假设硬币投出时会有$p_H$的概率正面朝上，而有$1-p_H$的概率反面朝上。<br>这时，条件概率可以改写成似然函数：</p><p>$L(p_H|HH)=P(HH|p_H=0.5)=0.25$</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 也就是说，对于取定的似然函数，在观测到两次投掷都是正面朝上时，$p_H=0.5$ 的<strong>似然性</strong>是0.25。如果考虑$p_H=0.6$，那么似然函数的值也会改变。</p><p>$L(p_H|HH)=P(HH|p_H=0.6)=0.36$ 注意到似然函数的值也变大了。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 这说明，如果参数$p_H$的取值变成0.6的话，结果观测到连续两次正面朝上的概率要比假设时$p_H=0.5$更大。也就是说，参数$p_H$取成0.6要比取成0.5更有说服力，更为“合理”。总之，<strong>似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大。</strong>对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。</p><p> &nbsp; &nbsp; &nbsp; 在这个例子中，似然函数实际上等于：</p><p>$L(\theta|HH)=P(HH|p_H=\theta)=\theta^2$ 其中$0 \le p_H\le 1$</p><p>如果取$p_H=1$ ，那么似然函数达到最大值1.也就是说，当连续观测到两次证明朝上时，假设硬币投掷正面朝上的概率为1是最合理的。</p><p>类似地，如果观测到三次投掷硬币，头两次正面朝上，第三次反面朝上，那么似然函数将会是</p><p>$L(\theta|HHT)=P(HHT|p_H=\theta)=\theta^2(1-\theta)$ ,其中$T$表示反面朝上，$0\le p_H \le 1$</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 这时候，似然函数的最大值将会在$p_H={2\over3}$的时候取到。也就是说，当观测到三次投掷中前两次正面朝上时，估计硬币投掷时正面朝上的概率$p_H={2\over 3}$是合理的。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 若一试验中有$n$个可能结果$A_1，A_2，…，A_n$,现在做一试验，若事件$A_i$发生了，则认为事件$A_i$在这$n$个可能结果中出现的概率最大。</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 一次试验就出现的事件（应该）有较大的概率</p><p> &nbsp; &nbsp; &nbsp; <strong>极大似然估计</strong>就是在一次抽样中，若得到观测值$x_1,…,x_n$则选取$\hat\theta(x_1,…,x_n)$作为$\theta$的估计值。使得当$\theta=\hat\theta(x_1,…,x_n)$样本出现的概率最大。</p><p><strong>极大似然概率，就是在已知观测的数据的前提下，找到使得似然概率最大的参数值。</strong></p><ol><li><p><strong>若总体$X$为离散型</strong></p><p>设分布律$P{X=k}=p(x;\theta)$,$\theta$为待估参数，$\theta\in\Theta$ ,$X_1,X_2,…,X_n$是来自总体$X$的样本，若$x_1,x_2,…,x_n$为相对于$X_1,X_2,…,X_n$的样本值，<br>$L(\theta)=L(x_1,x_2,…,x_n;\theta)\prod p(x_i;\theta),\theta \in \Theta$</p></li></ol><p>$L(\theta)$称为样本似然函数</p><p>若$L(x_1,x_2,…,x_n;\hat\theta)  =  max_{\theta\in\Theta}  L(x_1,x_2,…,x_n;\theta)$  </p><p>$ \hat(x_1,x_2,\dots,x_n)$,参数$\theta$的极大似然估计值。</p><ol><li><strong>设总体$X$为连续型</strong></li></ol><p>&nbsp; &nbsp; &nbsp; &nbsp; 设概率密度为$f(x;\theta),$$\theta$为待估参数， $\theta\in \Theta$,$X_1,x_2, \dots ,X_n$是 来自总体$X$的样本，若$x_1,x_2,\dots,x_n$为相应于$X_1,X_2,\dots,X_n$的样本值，</p><p>$$<br>L(\theta)=L(x_1,x_2,…,x_n;\theta)\prod f(x_i;\theta)<br>$$<br>$\hat \theta(x_1,x_2,\dots,x_n)$,参数$\theta$的极大似然估计值。</p><p>极大似然法求估计值的步骤：（一般情况下）</p><p>1）构造似然函数$L(\theta):$<br>$$<br>L(\theta)=\prod p(x_i;\theta)(离散型)<br>$$</p><p>$$<br>L(\theta)=\prod f(x_i;\theta)(离散型)<br>$$</p><p>2)取对数：$ln  L(\theta)$;</p><p>3)令 $d ln L  \over d\theta$=$0$;</p><p>4)解似然方程得到$\theta$ 的极大似然估计值$\hat\theta$</p><p><strong>说明</strong>：若似然方程（组）无解，或似然函数不可导，此法失效，改用其他方法。</p><p><strong>例1</strong>： 设$X$服从 参数$\lambda(\lambda&gt;0)$的泊松分布，$x_1,x_2,\dots,x_n$是来自于$X$的一个样本值,求$\lambda$的极大似然估计值</p><p><strong>解</strong>：因为$X$的分布律为</p><p>$P{X=x}$=${\lambda^x \over x! }e^{-\lambda}$, $(x=0,1,2,\dots,n)$</p><p>所以$\lambda$的似然函数为<br>$$<br>L(\lambda)=\prod_{i=1}^{n} ({\lambda ^{x_i} \over x_i!  }{e^{-\lambda}})=e^{-n\lambda}{\lambda^{\sum_{i=1}^nx_i}\over \prod_{i=1}^{n}(x_i!)}<br>$$</p><p>$$<br>ln  L(\lambda)  = -n\lambda+(\sum_{i=1}^{n}x_i)ln\lambda-\sum_{i=1}^{n}{(x_i!)},<br>$$</p><p>令${d\over{d \lambda}}lnL(\lambda)  = -n+{\sum_{i=1}^nx_i \over \lambda} = 0$</p><p>解得$\lambda$的极大似然估计值为$\hat\lambda={1\over n}\sum_{i=1}^nx_i=\overline x$</p><p>这有估计值与矩估计值是相同的。</p><p><strong>例2</strong>  设总体$X \sim N(\mu,\sigma^2)$,$\mu,\delta^2$为未知参数， $x_1,x_2,\dots,x_n$是来自$X$的一个样本值，求$\mu,\sigma^2$的极大似然估计值。</p><p><strong>解</strong>：$X$的概率密度为$f(x;\mu,\sigma^2)={1 \over \sqrt{2\pi}\sigma  }e^{-{(x-\mu)^2 \over 2\sigma^2}}$,</p><p>似然函数为<br>$$<br>L(\mu,\sigma ^2)= \prod_{i=1}^n{1 \over \sqrt{2\pi}}e{(x_i-\mu)^2 \over 2\sigma^2},<br>$$</p><p>$$<br>lnL(\mu,\sigma ^2)=-{n\over 2}ln(2\pi)-{n\over 2}ln\sigma^2-{1\over 2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2,<br>$$</p><p>令<br>$$</p><p>f(n) =<br>\begin{cases}<br>{\partial \over \partial\mu}lnL(\mu,\sigma^2)=0   \\<br>{\partial \over \partial\sigma^2}lnL(\mu,\sigma^2)=0   \\<br>\end{cases}</p><p>$$</p><p>$$<br>\begin{cases}<br>{1\over \sigma^2} [\sum_{i=1}^nx_i-n\mu]=0,\dots(1) \\<br>-{n\over 2\sigma^2}+{1\over (\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2=0,\dots,(2)<br>\end{cases}<br>$$</p><p>故$\mu$和$\sigma^2$的 极大似然估计值为</p><p>&nbsp;$\hat\mu={1\over n}\sum_{i=1}^n\overline x$,$   \,\hat\sigma^2={1\over n}\sum_{i=1}^n(x_i-\overline x )$</p><p>这一估计值与矩估计值是相同的。</p><p><strong>例3</strong> 设总体$X$服从$[0,\theta]$上的均匀分布，$\theta&gt;0$未知，$x_1,x_2,\dots,x_n$是来自总体$X$的样本值，求出$\theta$的极大似然估计值。</p><p><strong>解</strong>：记$x_{(h)}=max(x_1,x_2,\dots,x_n)$,</p><p>$X$的概率密度为$f(x;\theta)=\begin{cases} {1\over \theta}, 0\le x \le \theta \\0,其他 \end{cases}$</p><p>所以似然函数为$L(\theta)=\begin{cases} {1\over \theta^n},x_{(h)} \le \theta \\0,其他 \end{cases}$</p><p>对于满足$x_{h}\le\theta$的任意$\theta$有</p><p>$$<br>L(\theta)={1\over\theta^n\le{1\over(x_{(h)})^n}}<br>$$</p><p>即似然函数$L(\theta)$在$\theta=x_h$时取得极大值，$\theta$的极大似然估计值为$\hat\theta=x_{(h)}=max_{1\le i\le n}x_i$</p><p>这一估计值与矩估计是不相同的。</p><p>###矩法估计值与极大似然估计值的比较</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">矩法估计法</th><th style="text-align:center">极大似然估计法</th></tr></thead><tbody><tr><td style="text-align:center">依 据</td><td style="text-align:center">大数定律</td><td style="text-align:center">极大似然思想</td></tr><tr><td style="text-align:center">运 算</td><td style="text-align:center">较简单（可能会有信息量损失）</td><td style="text-align:center">较复杂</td></tr><tr><td style="text-align:center">精 度</td><td style="text-align:center">一般较低</td><td style="text-align:center">一般较高</td></tr></tbody></table><p>注意：</p><p>1）矩法估计值与极大似然估计值不一定相同</p><p>2）不是所以极大似然估计法都需要建立似然方程求解。</p><p>​    最大似然估计是似然函数最初也是最自然的应用。上文已经提到，似然函数取得最大值表示相应的参数能够使得统计模型最为合理。从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是概率密度函数或概率质量函数），整理之后求最大值。实际应用中一般会取<strong>似然函数的对数作为求最大值的函数</strong>，这样求出的最大值和直接求最大值得到的结果是相同的。<strong>似然函数的最大值不一定唯一，也不一定存在</strong>。与矩法估计比较，最大似然估计的精确度较高，信息损失较少，但计算量较大。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就不难理解，在$data  mining$领域，许多求参数的方法最终都归结为最大化似然概率的问题。回到这个硬币的例子上来，在观测到$HH$的情况下，$pH = 1$$是最合理的（却未必符合真实情况，因为数据量太少的缘故）。</p><p>参考：</p><ol><li>极大似然估计法的原理和方法PPT</li></ol><ol><li><a href="https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0</a></li><li><a href="http://www.cnblogs.com/zhsuiy/p/4822020.html" target="_blank" rel="noopener">http://www.cnblogs.com/zhsuiy/p/4822020.html</a></li></ol><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title:  '极大似然估计法MLE',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client_id: &apos;de24b44123b8efbb4747&apos;,</span><br><span class="line">client_secret: &apos;785f51974278cde45a927a91a8438ef35eab9dd0&apos;,</span><br></pre></td></tr></table></figure>  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 极大似然估计法（$Method  of  Maximum  Likelihood  Estimation –MLE$）&lt;/p&gt;
&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 极大似然估计法最早由高斯（C.F.Gauss）提出。后来为费歇在1912年的文章中重新提出，并且证明了这个方法的一些性质。极大似然估计这一名称也是费歇（R.A.Fisher）给的。这是一种目前仍然得到广泛应用的方法。它是建立在极大似然原理的基础上的一个统计方法。&lt;/p&gt;
&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 在数理统计学中，似然函数是一种关于统计模型中的&lt;strong&gt;参数&lt;/strong&gt;的&lt;strong&gt;函数&lt;/strong&gt;，表示模型参数中的似然性。 似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。&lt;strong&gt;“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性&lt;/strong&gt;，但是在&lt;strong&gt;统计学&lt;/strong&gt;中，“似然性”和“或然性”或“概率”又有明确的区分。&lt;strong&gt;概率&lt;/strong&gt;用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而&lt;strong&gt;似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。&lt;/strong&gt; 有人说，概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。&lt;/p&gt;
&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 极大似然估计法的依据就是：&lt;strong&gt;概率最大的事件最可能发生&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://goingcoder.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://goingcoder.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib-1学习</title>
    <link href="https://goingcoder.github.io/2018/02/02/matplotlib1/"/>
    <id>https://goingcoder.github.io/2018/02/02/matplotlib1/</id>
    <published>2018-02-02T14:13:22.000Z</published>
    <updated>2018-02-02T14:50:19.988Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">print(plt.plot([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>第1句，这是导入Matplotlib接口的主要子模块pyplot绘图的首选格式。这是最好的做法，也是为了避免对全局名称空间的污染，强烈鼓励永远不要像这样导入接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> &lt;module&gt; <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><p>第2句，此代码行是实际的绘图命令。我们只指定了一个值列表，这些值表示要绘制的点的垂直坐标。matplotlib将使用一个隐式的水平值列表，从0（前值）到n-1（n为列表中的条目数），纵轴表示Y轴，横坐标表示X轴。</p><p>第3句，这条命令实际上是打开包含绘图图像的窗口。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = range(<span class="number">6</span>)</span><br><span class="line">plt.plot(x, [xi**<span class="number">2</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="1-多线图"><a href="#1-多线图" class="headerlink" title="1.多线图"></a>1.多线图</h2><p>如果需要在一个图中画出多条线，我们只需要在show之前多次plot就可以了。Matplotlib会给每条线自动选择不同颜色。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = range(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">plt.plot(x, [xi*<span class="number">1.5</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.plot(x, [xi*<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.plot(x, [xi/<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>也可以写成下列形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, [xi*<span class="number">1.5</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x], x, [xi*<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x], x,[xi/<span class="number">3.0</span> <span class="keyword">for</span> xi <span class="keyword">in</span> x])</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, x*<span class="number">1.5</span>, x, x*<span class="number">3.0</span>, x, x/<span class="number">3.0</span>)</span><br></pre></td></tr></table></figure><h2 id="2、网格"><a href="#2、网格" class="headerlink" title="2、网格"></a>2、网格</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.grid(<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="3、坐标轴"><a href="#3、坐标轴" class="headerlink" title="3、坐标轴"></a>3、坐标轴</h2><p>你可能已经注意到，matplotlib为了精确地包含绘制的数据集自动设置图的界限。有时我们想自己设置坐标轴的界限。我们可以这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.py</span><br><span class="line">plot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> npx = np.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">plt.plot(x, x*<span class="number">1.5</span>, x, x*<span class="number">3.0</span>, x, x/<span class="number">3.0</span>)</span><br><span class="line">print(plt.axis()) <span class="comment"># shows the current axis limits values，</span></span><br><span class="line"><span class="comment"># (0.85, 4.15, -0.25000000000000006, 12.583333333333334)</span></span><br><span class="line"><span class="comment"># plt.axis([0, 5, -1, 13]) # set new axes limits</span></span><br><span class="line"><span class="comment"># [0, 5, -1, 13]</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>如果axis()函数没有参数，它的返回值是实际的界限，有两种方式给这个函数传，一是通过四个值得列表[xmin, xmax, ymin, ymax] ，二是键值对的方式。</p><p>我们也可以只限制某个轴的最大值或者最小值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.axis(xmin=NNN, ymax=NNN)</span><br></pre></td></tr></table></figure><p>或者通过xlim()和ylim()函数只限制某一个坐标轴的界限。</p><h2 id="4、给坐标增加一个标签"><a href="#4、给坐标增加一个标签" class="headerlink" title="4、给坐标增加一个标签"></a>4、给坐标增加一个标签</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">plt.xlabel(<span class="string">'This is the X axis'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'This is the Y axis'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="5、增加一个标题"><a href="#5、增加一个标题" class="headerlink" title="5、增加一个标题"></a>5、增加一个标题</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">'Simple plot'</span>)</span><br></pre></td></tr></table></figure><h2 id="6、增加图例"><a href="#6、增加图例" class="headerlink" title="6、增加图例"></a>6、增加图例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, x*<span class="number">1.5</span>, label=<span class="string">'Normal'</span>)</span><br><span class="line">plt.plot(x, x*<span class="number">3.0</span>, label=<span class="string">'Fast'</span>)</span><br><span class="line">plt.plot(x, x/<span class="number">3.0</span>, label=<span class="string">'Slow'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><h2 id="7、保存图到文件"><a href="#7、保存图到文件" class="headerlink" title="7、保存图到文件"></a>7、保存图到文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.savefig(<span class="string">'plot123.png'</span>)</span><br><span class="line">plt.savefig(<span class="string">'plot123.png'</span>, dpi=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><h2 id="8、Plot-支持可选的第三个参数"><a href="#8、Plot-支持可选的第三个参数" class="headerlink" title="8、Plot()支持可选的第三个参数"></a>8、Plot()支持可选的第三个参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X, Y, <span class="string">'&lt;format&gt;'</span>, ...)</span><br></pre></td></tr></table></figure><p>plot方法的关键字参数color(或c)用来设置线的颜色。</p><p>控制颜色</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(y, <span class="string">'y'</span>)</span><br><span class="line">plt.plot(y+<span class="number">1</span>, <span class="string">'m'</span>)</span><br><span class="line">plt.plot(y+<span class="number">2</span>, <span class="string">'c'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>1）颜色的缩写</p><table><thead><tr><th style="text-align:center"><strong>Color abbreviation </strong></th><th style="text-align:center"><strong>Color Name </strong></th></tr></thead><tbody><tr><td style="text-align:center">b</td><td style="text-align:center">blue</td></tr><tr><td style="text-align:center">c</td><td style="text-align:center">cyan</td></tr><tr><td style="text-align:center">g</td><td style="text-align:center">green</td></tr><tr><td style="text-align:center">k</td><td style="text-align:center">black</td></tr><tr><td style="text-align:center">m</td><td style="text-align:center">magenta</td></tr><tr><td style="text-align:center">r</td><td style="text-align:center">red</td></tr><tr><td style="text-align:center">w</td><td style="text-align:center">white</td></tr><tr><td style="text-align:center">y</td><td style="text-align:center">yellow 2</td></tr></tbody></table><p>2）十六进制字符 #FF00FF</p><p>3）(r, g, b) 或 (r, g, b, a)，其中 r g b a 取均为[0, 1]之间</p><p>4）[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色</p><p>也可写成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x1, y1, fmt1, x2, y2, fmt2, ...)</span><br><span class="line">plt.plot(y, <span class="string">'y'</span>, y+<span class="number">1</span>, <span class="string">'m'</span>, y+<span class="number">2</span>, <span class="string">'c'</span>)</span><br></pre></td></tr></table></figure><h2 id="9、控制线条样式"><a href="#9、控制线条样式" class="headerlink" title="9、控制线条样式"></a>9、控制线条样式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(y, <span class="string">'--'</span>, y+<span class="number">1</span>, <span class="string">'-.'</span>, y+<span class="number">2</span>, <span class="string">':'</span>)</span><br></pre></td></tr></table></figure><p>-  solid</p><p>–  dashed</p><p>-.  dashdot</p><p>:    dotted</p><p>‘’, ‘ ‘,    None</p><h2 id="10、控制标记样式"><a href="#10、控制标记样式" class="headerlink" title="10、控制标记样式"></a>10、控制标记样式</h2><table><thead><tr><th style="text-align:center"><strong>Marker abbreviation </strong></th><th style="text-align:center"><strong>Marker style </strong></th></tr></thead><tbody><tr><td style="text-align:center">.</td><td style="text-align:center">Point marker</td></tr><tr><td style="text-align:center">,</td><td style="text-align:center">Pixel marker</td></tr><tr><td style="text-align:center">o</td><td style="text-align:center">Circle marker</td></tr><tr><td style="text-align:center">v</td><td style="text-align:center">Triangle down marker</td></tr><tr><td style="text-align:center">^</td><td style="text-align:center">Triangle up marker</td></tr><tr><td style="text-align:center">&lt;</td><td style="text-align:center">Triangle left marker</td></tr><tr><td style="text-align:center">&gt;</td><td style="text-align:center">Triangle right marker</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">Tripod down marker</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Tripod up marker</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">Tripod left marker</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">Tripod right marker</td></tr><tr><td style="text-align:center">s</td><td style="text-align:center">Square marker</td></tr><tr><td style="text-align:center">p</td><td style="text-align:center">Pentagon marker</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">Star marker</td></tr><tr><td style="text-align:center">h</td><td style="text-align:center">Hexagon marker</td></tr><tr><td style="text-align:center">H</td><td style="text-align:center">Rotated hexagon marker</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">Plus marker</td></tr><tr><td style="text-align:center">x</td><td style="text-align:center">Cross (x) marker</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">Diamond marker</td></tr><tr><td style="text-align:center">d</td><td style="text-align:center">Thin diamond marker</td></tr><tr><td style="text-align:center">\</td><td style="text-align:center"></td><td>Vertical line (vline symbol) marker</td></tr><tr><td style="text-align:center">_</td><td style="text-align:center">Horizontal line (hline symbol) marker</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y = np.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.3</span>)</span><br><span class="line">plt.plot(y, <span class="string">'cx--'</span>, y+<span class="number">1</span>, <span class="string">'mo:'</span>, y+<span class="number">2</span>, <span class="string">'kp-.'</span>);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>传递关键字参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y = np.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.3</span>)</span><br><span class="line">plt.plot(y, color=<span class="string">'blue'</span>, linestyle=<span class="string">'dashdot'</span>, linewidth=<span class="number">4</span>,</span><br><span class="line">marker=<span class="string">'o'</span>, markerfacecolor=<span class="string">'red'</span>, markeredgecolor=<span class="string">'black'</span>,</span><br><span class="line">markeredgewidth=<span class="number">3</span>, markersize=<span class="number">12</span>);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>如果想画两条一样颜色的线，可以这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x1, y1, x2, y2, color=<span class="string">'green'</span>)</span><br></pre></td></tr></table></figure><h2 id="11、X轴和Y轴的刻度"><a href="#11、X轴和Y轴的刻度" class="headerlink" title="11、X轴和Y轴的刻度"></a>11、X轴和Y轴的刻度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locs, labels = plt.xticks()</span><br></pre></td></tr></table></figure><p>不加任何参数的画，tick函数返回当前位置和标签，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(plt.xticks())</span><br><span class="line"><span class="comment">#(array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]), &lt;a list of 6 Text xticklabel objects&gt;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line">x = [<span class="number">5</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>] </span><br><span class="line">plt.plot(x); </span><br><span class="line">plt.xticks(range(len(x)), [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>]); </span><br><span class="line">plt.yticks(range(<span class="number">1</span>, <span class="number">8</span>, <span class="number">2</span>)); </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>使用字符来代表x的刻度</p><h2 id="12、中文字体不显示的问题"><a href="#12、中文字体不显示的问题" class="headerlink" title="12、中文字体不显示的问题"></a>12、中文字体不显示的问题</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>] <span class="comment"># 指定默认字体</span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span> <span class="comment"># 解决保存图像是负号'-'显示为方块的问题</span></span><br></pre></td></tr></table></figure><p>参考：Matplotlib for python development</p><p>python中matplotlib的颜色及线条控制：<a href="http://blog.csdn.net/qq_26376175/article/details/67637151" target="_blank" rel="noopener">http://blog.csdn.net/qq_26376175/article/details/67637151</a></p><p><div id="container"></div></p><p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({  id: 'window.location.pathname', // 可选。默认为 location.href  title: 'matplotlib-1学习',  owner: 'goingcoder',  repo: 'goingcoder.github.io',  oauth: {      client_id: 'de24b44123b8efbb4747',      client_secret: '785f51974278cde45a927a91a8438ef35eab9dd0',  },})gitment.render('container')</script>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(plt.plot([&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;plt.show()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;第1句，这是导入Matplotlib接口的主要子模块pyplot绘图的首选格式。这是最好的做法，也是为了避免对全局名称空间的污染，强烈鼓励永远不要像这样导入接口：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; &amp;lt;module&amp;gt; &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; *&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;第2句，此代码行是实际的绘图命令。我们只指定了一个值列表，这些值表示要绘制的点的垂直坐标。matplotlib将使用一个隐式的水平值列表，从0（前值）到n-1（n为列表中的条目数），纵轴表示Y轴，横坐标表示X轴。&lt;/p&gt;
&lt;p&gt;第3句，这条命令实际上是打开包含绘图图像的窗口。&lt;/p&gt;
    
    </summary>
    
      <category term="matplot" scheme="https://goingcoder.github.io/categories/matplot/"/>
    
    
      <category term="matplot" scheme="https://goingcoder.github.io/tags/matplot/"/>
    
  </entry>
  
</feed>
